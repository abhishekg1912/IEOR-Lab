{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab2_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 1. }$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that we implemented the gradient descent algorithm to solve $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. The main ingredients in the gradient descent iterations are the descent direction $\\mathbf{p}^k$ which is set to $-\\nabla f(\\mathbf{x}^k)$, and the step length $\\eta^k$ which is found by solving an optimization problem (or sometimes taken as a constant value over all iterations). We used the following procedure in the previous lab:\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\nabla f (\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for particular cases of quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \n",
        "\n",
        "To find the step length, we will use the following property: \n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \n",
        "\\begin{align}\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \n",
        "\\end{align}\n",
        "\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\alpha\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2) = (x_1-8)^2 + (x_2 + 12)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (x[1]+12)**2 + (-8+x[0])**2\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([2*(x[0]-8), 2*(x[1]+12)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  step_length=(np.dot(gradf.T,gradf)) / (np.matmul(np.matmul(gradf,2*A),gradf)) \n",
        "  \n",
        "  return step_length"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=rho\n",
        "  y=gamma\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x)-y*alpha*np.dot((gradf.T),gradf):\n",
        "    alpha=p*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1, 0],[0,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.3"
      ],
      "metadata": {
        "id": "FH0xJdWS9hKU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40305936-dbb3-49f1-f7e5-cfffa90eebda"
      },
      "source": [
        "\n",
        "my_start_x = np.array([1,1])\n",
        "my_tol= 1e-5\n",
        "\n",
        "\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(f'Value of optimizer for constant step length= {x_opt} and minimum value is= {evalf(x_opt)} ')\n",
        "\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls} and minimum value is= {evalf(x_opt_bls)}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for constant step length= [  7.99999775 -11.99999582] and minimum value is= 2.2517218946096954e-11 \n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[  8. -12.] and minimum value is= 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.4"
      ],
      "metadata": {
        "id": "WnKQX4Lm9zmL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cced1bf3-fed7-4b1d-d629-d94f7ada5c62"
      },
      "source": [
        "my_start_x = np.array([25,25])\n",
        "my_tol= 1e-12\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(f'Value of optimizer for exact step length= {x_opt}, minimum value is= {evalf(x_opt)} and number of iterations= {iterations} ')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for exact step length= [  8. -12.], minimum value is= 0.0 and number of iterations= 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([25,25])\n",
        "my_tol= 1e-12\n",
        "x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls}, minimum value is= {evalf(x_opt_bls)} and number of iterations are= {iterations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd8bGPRy-64T",
        "outputId": "a364280e-78df-472e-9573-8b527e11c05b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[  8. -12.], minimum value is= 0.0 and number of iterations are= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both methods no of iterations taken by method is equal and equal to 1"
      ],
      "metadata": {
        "id": "wuQknofi_Wvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.5"
      ],
      "metadata": {
        "id": "MDHjpwzk_gjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([25,25])\n",
        "my_tol=10**(-10)\n",
        "list_of_alpha=[1,0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01]\n",
        "no_of_iterations=[]\n",
        "list_of_opt=[]\n",
        "list_of_minimum_value=[]\n",
        "for a in list_of_alpha:\n",
        "  print(f\"\\nFor alpha = {a}\")\n",
        "  x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, a, 0.5,0.5)\n",
        "  print(f\"Value of optimizer is= {x_opt_bls}, minimum value is= {evalf(x_opt_bls)} and number of iterations are= {iterations}\")\n",
        "  no_of_iterations.append(iterations)\n",
        "  list_of_opt.append(x_opt_bls)\n",
        "  list_of_minimum_value.append(evalf(x_opt_bls))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmEw1n2y_gAW",
        "outputId": "bdf65c34-af98-41db-a8f5-80cfd40b2e35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For alpha = 1\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 0.0 and number of iterations are= 1\n",
            "\n",
            "For alpha = 0.9\n",
            "Params for Backtracking LS: alpha start: 0.9 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.6579714975258972e-21 and number of iterations are= 12\n",
            "\n",
            "For alpha = 0.75\n",
            "Params for Backtracking LS: alpha start: 0.75 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.3714654556129199e-21 and number of iterations are= 20\n",
            "\n",
            "For alpha = 0.6\n",
            "Params for Backtracking LS: alpha start: 0.6 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 2.2038291998576117e-21 and number of iterations are= 30\n",
            "\n",
            "For alpha = 0.5\n",
            "Params for Backtracking LS: alpha start: 0.5 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 0.0 and number of iterations are= 1\n",
            "\n",
            "For alpha = 0.4\n",
            "Params for Backtracking LS: alpha start: 0.4 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.1393259623274523e-22 and number of iterations are= 18\n",
            "\n",
            "For alpha = 0.25\n",
            "Params for Backtracking LS: alpha start: 0.25 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.3714654556129199e-21 and number of iterations are= 40\n",
            "\n",
            "For alpha = 0.1\n",
            "Params for Backtracking LS: alpha start: 0.1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 2.3972320602008796e-21 and number of iterations are= 123\n",
            "\n",
            "For alpha = 0.01\n",
            "Params for Backtracking LS: alpha start: 0.01 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 2.4523367712209537e-21 and number of iterations are= 1358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(list_of_alpha,no_of_iterations)\n",
        "plt.ylabel('Iterations')\n",
        "plt.xlabel('Alpha')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "DKzmiA5HAyA4",
        "outputId": "4b524c99-c627-4ec4-bec5-d591c8d4da91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we decrease the value of alpha from 1 to 0.5 the number of iterations increases,but at alpha= 1 and 0.5 the number of iteration is 1 and after this if we decrease the value of alpha from 0.5 to 0.01 the number of iterations increases. for alpha equal to 1 and 0.5, the number of iteration is 1 it means that these two values are the most suitable values.\n",
        "for each value of the alpha the minimum value of the function is approaching to zero.\n",
        "\n",
        "As we have calculated above the exact line search method is taking 1 iteration to terminate and for alpha= 0.5 and 1 the backtracking algorithm is taking 1 iteration to terminate,and for other values of alpha backtracking algorithm is taking more iterations. hence we can conclude that the exact line search is faster algorithm but it can't be used for general purposes."
      ],
      "metadata": {
        "id": "KFh4iwuoRttp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.6"
      ],
      "metadata": {
        "id": "uFN6GNqxBdrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([25,25])\n",
        "my_tol=10**(-10)\n",
        "list_of_rho=[0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "no_of_iterations=[]\n",
        "list_of_opt=[]\n",
        "list_of_minimum_value=[]\n",
        "for r in list_of_rho:\n",
        "  print(f\"\\nFor rho = {r}\")\n",
        "  x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, r,0.5)\n",
        "  print(f\"Value of optimizer is= {x_opt_bls}, minimum value is= {evalf(x_opt_bls)} and number of iterations are= {iterations}\")\n",
        "  no_of_iterations.append(iterations)\n",
        "  list_of_opt.append(x_opt_bls)\n",
        "  list_of_minimum_value.append(evalf(x_opt_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6N852kvBgS_",
        "outputId": "282467e9-61e6-4a60-e3ab-40a3fd9d11f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For rho = 0.9\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.9  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 4.960536411900771e-22 and number of iterations are= 9\n",
            "\n",
            "For rho = 0.75\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.75  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.0819468296335504e-21 and number of iterations are= 15\n",
            "\n",
            "For rho = 0.6\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.6  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 7.844395544174143e-22 and number of iterations are= 22\n",
            "\n",
            "For rho = 0.5\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 0.0 and number of iterations are= 1\n",
            "\n",
            "For rho = 0.4\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.4  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.1393259623274523e-22 and number of iterations are= 18\n",
            "\n",
            "For rho = 0.25\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.25  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 1.3714654556129199e-21 and number of iterations are= 40\n",
            "\n",
            "For rho = 0.1\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.1  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 2.3972320602008796e-21 and number of iterations are= 123\n",
            "\n",
            "For rho = 0.01\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.01  gamma: 0.5\n",
            "Value of optimizer is= [  8. -12.], minimum value is= 2.4523367712209537e-21 and number of iterations are= 1358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list_of_rho,no_of_iterations)\n",
        "plt.ylabel('Iterations')\n",
        "plt.xlabel('rho')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "0HvOgG2BCKig",
        "outputId": "5e3eaa4b-b76b-4d95-b1d2-beb45ff935ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we decrease the value of rho from 0.9 to 0.5 the number of iterations increases but at rho=0.5 the number of iterations is 1 and after this if we decrease the value of rho from 0.5 to 0.01 the number of iterations increases. At rho= 0.5 the number of iteration is 1 it means that it is the more suitable value for this method. and for each value of rho the minimum value of the function is approaching to zero.\n",
        "\n",
        "As we have calculated above the exact line search method is taking 1 iteration to terminate and for rho=0.5 the backtracking algorithm is also taking 1 iteration to terminate,and for other values of rho backtracking algorithm is taking more iterations. hence we can conclude that the exact line search is faster algorithm but it can't be used for general purposes."
      ],
      "metadata": {
        "id": "OecqrHSaR9xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MzPjGepNTGKK"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}