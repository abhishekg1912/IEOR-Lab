{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab2_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE-2"
      ],
      "metadata": {
        "id": "hMA68TUhfHhF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trQpmoqnCvCG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x)==3\n",
        "  return (((x[0]-2)**2)/8 + ((x[1]-4)**2)/64 + ((x[2]-8)**2)/512)"
      ],
      "metadata": {
        "id": "WuPUSWxHFQ4K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x)==3\n",
        "  return np.array([(x[0]-2)/4 ,(x[1]-4)/32 ,(x[2]-8)/256])"
      ],
      "metadata": {
        "id": "MdgvzZqNHXYD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalf(np.array([1,1,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j1Mh402H3q2",
        "outputId": "560f7912-d644-4191-a291-f811a716a39d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.361328125"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 3 and  A.shape[1] == 3 #allow only a 2x2 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  step_length=(np.dot(gradf.T,gradf)) / (np.matmul(np.matmul(gradf,2*A),gradf)) \n",
        "  \n",
        "  return step_length"
      ],
      "metadata": {
        "id": "MXApNt_nH8ki"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=rho\n",
        "  y=gamma\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x)-y*alpha*np.dot((gradf.T),gradf):\n",
        "    alpha=p*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "qviDuCJgJCa-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "metadata": {
        "id": "OqTtV8l0JFKT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 3 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0, 0],[0, 1/64, 0],[0, 0, 1/512]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n"
      ],
      "metadata": {
        "id": "7BrR5waZJFnY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.2"
      ],
      "metadata": {
        "id": "0q4Z6WK_KSQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1,1,1])\n",
        "my_tol=10**(-5)\n",
        "\n",
        "\n",
        "\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(f'Value of optimizer for constant step length= {x_opt} and minimum value is= {evalf(x_opt)} ')\n",
        "\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls} and minimum value is= {evalf(x_opt_bls)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmkkrNLAJKG6",
        "outputId": "723b6bfd-e59a-4ce7-95a0-1627b4eaff90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for constant step length= [2.         4.         7.99744099] and minimum value is= 1.2790080180472962e-08 \n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[2.         4.         7.99744063] and minimum value is= 1.2793697352715871e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.3"
      ],
      "metadata": {
        "id": "AA-a6mEvL5Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1/64,1/8,1])\n",
        "my_tol=10**(-10)\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(f'Value of optimizer for exact step length= {x_opt}, minimum value is= {evalf(x_opt)}\\n number of iterations are= {iterations} ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ2PzO3OL6Z5",
        "outputId": "e8648b68-9483-4ff3-d1e9-7bfb73e101f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for exact step length= [2.         4.         7.99999998], minimum value is= 9.150071377581033e-19\n",
            " number of iterations are= 269 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([1/64,1/8,1])\n",
        "my_tol= 1e-10\n",
        "x_opt_bls,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls}, minimum value is= {evalf(x_opt_bls)} \\n number of iterations are= {iterations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMQkd-RrR3-k",
        "outputId": "3b6fd1f4-10c0-4031-9b9b-7b2f40b371f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[2.         4.         7.99999997], minimum value is= 1.2748574165464873e-18 \n",
            " number of iterations are= 4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. of iterations to terminate Backtracking search (4964) is higher than the number of iterations to terminate Exact line search(269). By this we can conclude that in this case exact line search is a faster algorithm."
      ],
      "metadata": {
        "id": "ORhQg6p6fYbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.4"
      ],
      "metadata": {
        "id": "bC7Cm830S49I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x)==4\n",
        "  return (((x[0]-2)**2)/8 + ((x[1]-4)**2)/64 + ((x[2]-8)**2)/512+ ((x[3]-16)**2)/4096)"
      ],
      "metadata": {
        "id": "zpUb8HngS6Ag"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x)==4\n",
        "  return np.array([(x[0]-2)/4 ,(x[1]-4)/32 ,(x[2]-8)/256,(x[3]-16)/2048])"
      ],
      "metadata": {
        "id": "FtXir_2-TnL3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 4 and  A.shape[1] == 4 #allow only a 2x2 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  step_length=(np.dot(gradf.T,gradf)) / (np.matmul(np.matmul(gradf,2*A),gradf)) \n",
        "  \n",
        "  return step_length"
      ],
      "metadata": {
        "id": "UImIkK1YT876"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 4\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=rho\n",
        "  y=gamma\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x)-y*alpha*np.dot((gradf.T),gradf):\n",
        "    alpha=p*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "Lyeuw-7uULQx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "metadata": {
        "id": "AhuVL1AmUSNe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1, 0],[0,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n",
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 4 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0, 0 ,0],[0,1/64, 0 ,0],[0,0,1/512,0],[0 ,0 ,0,1/4096 ]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n",
        "\n"
      ],
      "metadata": {
        "id": "1CWqasbRUSv8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1/512,1/64,1/8,1])\n",
        "my_tol=10**(-10)\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(f'Value of optimizer for exact step length= {x_opt}, minimum value is= {evalf(x_opt)}\\n number of iterations are= {iterations} ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XixqlSbQUqnx",
        "outputId": "47f6134b-4aeb-4e32-e0a1-6d18e7b29b3e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for exact step length= [ 2.          4.          8.         15.99999981], minimum value is= 8.8565993523583e-18\n",
            " number of iterations are= 2013 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1/512,1/64,1/8,1])\n",
        "my_tol=10**(-10)\n",
        "x_opt,iterations = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,1,0.5,0.5)\n",
        "print(f'Value of optimizer for Backtracking step length= {x_opt}, minimum value is= {evalf(x_opt)}\\n number of iterations are= {iterations} ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7prbFsIekSZ",
        "outputId": "34faf355-2ec0-4ec2-a0af-b85c925483ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking step length= [ 2.         4.         8.        15.9999998], minimum value is= 1.0237544252113035e-17\n",
            " number of iterations are= 37079 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. of iterations to terminate Backtracking search (37079) is higher than the number of iterations to terminate Exact line search(2013). By this we can conclude that in this case exact line search is a faster algorithm."
      ],
      "metadata": {
        "id": "hI6Ws3DmgKMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.5"
      ],
      "metadata": {
        "id": "vkzHz2oTggyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#for N>4:\n",
        "As we see above that the exact line search algorithm take 269,2013 iterations to terminate for N=3 and 4 respectively and as we increase the value of N by 1 the number of iteration increase  almost 7.5 times. hence  the number of iterations to terminate for N>4 will be very high. \n",
        "\n",
        "As we see above that the Backtracking line search algorithm take 4964 and 37079 iterations to terminate for N=3 and 4 respectively and as we increase the value of N by 1 the number of iteration increase  almost 7.5 times. hence  the number of iterations to terminate for N>4 will be very high. \n",
        "\n",
        "And the exact line search algorithm is taking less iteration than Backtracking line search, so exact line search algorithm is more suitable for N>4"
      ],
      "metadata": {
        "id": "4KLkT3RkgknL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5q25_QLdgTbF"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}