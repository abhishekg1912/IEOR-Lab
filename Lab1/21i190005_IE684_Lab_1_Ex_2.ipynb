{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab 1_Ex 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.1"
      ],
      "metadata": {
        "id": "ygiWuF4Pt9GY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "#Answer:\n",
        "\n",
        "$f(x)$ can be written as $f(x)=x_1^2+x_2^2+200x_1-50x_2+10625$\n",
        "\n",
        "If we take $ A=\\begin{bmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1\n",
        "\\end{bmatrix} \\\\ b=\\begin{bmatrix}\n",
        "100\\\\\n",
        "-25\n",
        "\\end{bmatrix} \\\\ c=10625 \\\\ \\text{and} \\  x=\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Then $f(x)$ can be written in the form $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c.$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "#Que.2\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "#Answer:\n",
        "As we show in the above question that the given $f(x)$ can be written of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$.\n",
        "\n",
        "now $$x-α∇f(x)= \\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix}-\\alpha\\begin{bmatrix}\n",
        "2(x_1+100)\\\\\n",
        "2(x_2-25)\n",
        "\\end{bmatrix}\\\\\n",
        "⇒x-α∇f(x)=\\begin{bmatrix}\n",
        "x_1(1-2\\alpha)-200\\alpha\\\\\n",
        "x_2(1-2\\alpha)+50\\alpha\n",
        "\\end{bmatrix}\\\\\n",
        "$$\n",
        "Now $f(\\begin{bmatrix}\n",
        "x_1(1-2\\alpha)-200\\alpha\\\\\n",
        "x_2(1-2\\alpha)+50\\alpha\n",
        "\\end{bmatrix})= (x_1(1-2α)-200\\alpha+100)^2+(x_2(1-2α)+50\\alpha-25)^2$\n",
        "\n",
        "To find the minimum, we differentiate it with respect to $α$ and put it equal to zero.\n",
        "\n",
        "Diffferentiating $ f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ with respect to $α$\n",
        "\n",
        "$\\frac{d(f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))}{dα}=-2(x_1(1-2α)-200α+100)(2x_1+200)+2(x_2(1-2α)+50α-25)(-2x_2+50)\\\\ $\n",
        "\n",
        "$\\frac{d(f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))}{dα}$ vanishes at $α=0.5$\n",
        "\n",
        "Hence,$f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})$ will be minimum for $α=0.5$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(g,x):\n",
        "  s_l=1\n",
        "  a=10**(-4)\n",
        "  r=0.5\n",
        "  while evalf(x-s_l*g) > (evalf(x)-(a*s_l*np.linalg.multi_dot([g,g]))):\n",
        "    s_l=r*s_l\n",
        "  return s_l"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(g_x,x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d536026-0945-457c-e84b-a0fcd60c6d39"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-100.,   25.]), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.4"
      ],
      "metadata": {
        "id": "RV3RajH1rPbV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66100cbd-260f-4ce7-c058-6950e402c526"
      },
      "source": [
        "list_of_tolerance=[10**(-p) for p in range(1,11)]\n",
        "no_of_iterations=[]\n",
        "for t in list_of_tolerance:\n",
        "  opt_x,iterations=find_minimizer(my_start_x,t)\n",
        "  no_of_iterations.append(iterations)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(list_of_tolerance,no_of_iterations)\n",
        "plt.ylabel('Iterations')\n",
        "plt.xlabel('Tolerance')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "6-P49wUnjxVz",
        "outputId": "5858356a-2e7c-4a10-fb26-f6812d28ddd1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,number of iterations doesn't change with tolerance but in exercise 1(in case of fixed step length) number of iterations decreases as tolerance increases."
      ],
      "metadata": {
        "id": "IaX7s1iqkZdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xb_Ej4iQkDv6"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}