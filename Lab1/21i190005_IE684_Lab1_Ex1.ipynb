{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab1_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Welcome to IE 684 (Spring 2021-22)}$  \n",
        "\n",
        "$\\large\\textbf{Lab 1 Exercise 1. }$\n",
        "\n",
        "We will start with a procedure which helps to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. \n",
        "\n",
        "We will use the following gradient descent type algorithm: \n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$, Steplength $\\eta$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu_eWNWHTg64"
      },
      "source": [
        "#check whether you can pass arbitrary arguments to evalf \n",
        "#my_x = [1,3] #Note: my_x is a list of 2 elements, but not a numpy array\n",
        "#print('f(my_x) is:',evalf(my_x))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhgbIivdTshs",
        "outputId": "fe3a1739-516e-4a73-cb1a-3baba3443242"
      },
      "source": [
        "# First we will create a numpy array of size 2\n",
        "my_x = np.array([1,2])\n",
        "print('type of my_x',type(my_x), 'length of my_x:',len(my_x)) #verify if my_x is indeed a numpy array of size 2\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of my_x <class 'numpy.ndarray'> length of my_x: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Iky9aOT78d"
      },
      "source": [
        "#now call evalf (x) with my_x as argument and check if it works\n",
        "#print(evalf(my_x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, step_length):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  assert type(step_length) is float and step_length>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "\n",
        "  #we can manage a list to store the function values, might be useful for plotting \n",
        "  fvals = [evalf(x)]\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #append the current function value to the list containing function values\n",
        "    fvals.append(evalf(x))\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x, fvals,k \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-kHCkbwe-M4",
        "outputId": "9d906402-fa02-4d97-9847-a824c479928c"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_steplength = 0.1\n",
        "my_tol= 1e-3 #10^{-3} or 0.001\n",
        "opt_x, fvals_ret,total_iteration = find_minimizer(my_start_x, my_tol, my_steplength)\n",
        "print('Optimizer:',opt_x)\n",
        "print(f\"total iterations= {total_iteration}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "Optimizer: [-99.99958844  24.99994388]\n",
            "total iterations= 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "27e7513d-31d3-43c7-f5c5-e0b7c82ed2c6"
      },
      "source": [
        "#we will plot the function values and check the behavior\n",
        "import matplotlib.pyplot as plt #package useful for plotting\n",
        "plt.plot(fvals_ret)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('f(x)')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.2"
      ],
      "metadata": {
        "id": "Lgj-xs7MigKz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6_yk3_CaBYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a9a323-e057-475c-d867-e93147dbbf4f"
      },
      "source": [
        "print(f\"The minimizer is {opt_x} and the minimum function value is {evalf(opt_x)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimizer is [-99.99958844  24.99994388] and the minimum function value is 1.7253033717248775e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.3"
      ],
      "metadata": {
        "id": "GglVjn76k0j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tolerance=[]\n",
        "for p in range(1,11):\n",
        "  list_of_tolerance.append(10**(-p))"
      ],
      "metadata": {
        "id": "33ZDkQLpM0kU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tolerance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlFQeNUMnbo1",
        "outputId": "7f20ff69-ff68-4f67-de8b-c146f24a8ebb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Iterations=[]\n",
        "for t in list_of_tolerance:\n",
        "  my_start_x = np.array([10,10])\n",
        "  my_steplength = 0.1\n",
        "  my_tol= t\n",
        "  print(f\"For tolerance = {t}\\n\")\n",
        "  opt_x, fvals_ret,iterations = find_minimizer(my_start_x, my_tol, my_steplength)\n",
        "  print('\\nOptimizer:',opt_x,)\n",
        "  print(f\"for tolerance value= {t}, the minimum value of function is {evalf(opt_x)} and number of iterations are= {iterations}\")\n",
        "  Iterations.append(iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8GbNxy1lWO1",
        "outputId": "dff1ef22-6a9c-408d-8b5b-47356f6b1d36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For tolerance = 0.1\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "\n",
            "Optimizer: [-99.9553787   24.99391528]\n",
            "for tolerance value= 0.1, the minimum value of function is 0.0020280843668995063 and number of iterations are= 35\n",
            "For tolerance = 0.01\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "\n",
            "Optimizer: [-99.99520882  24.99934666]\n",
            "for tolerance value= 0.01, the minimum value of function is 2.3382220797474075e-05 and number of iterations are= 45\n",
            "For tolerance = 0.001\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "\n",
            "Optimizer: [-99.99958844  24.99994388]\n",
            "for tolerance value= 0.001, the minimum value of function is 1.7253033717248775e-07 and number of iterations are= 56\n",
            "For tolerance = 0.0001\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "\n",
            "Optimizer: [-99.99995581  24.99999397]\n",
            "for tolerance value= 0.0001, the minimum value of function is 1.989139359193299e-09 and number of iterations are= 66\n",
            "For tolerance = 1e-05\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "\n",
            "Optimizer: [-99.99999526  24.99999935]\n",
            "for tolerance value= 1e-05, the minimum value of function is 2.293321544605827e-11 and number of iterations are= 76\n",
            "For tolerance = 1e-06\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "iter: 77  x: [-99.9999962   24.99999948]  f(x): 1.4677257905583636e-11  grad at x: [ 7.59192099e-06 -1.03526195e-06]  gradient norm: 7.662181910026317e-06\n",
            "iter: 78  x: [-99.99999696  24.99999959]  f(x): 9.393445025637944e-12  grad at x: [ 6.07353678e-06 -8.28209558e-07]  gradient norm: 6.1297455169486255e-06\n",
            "iter: 79  x: [-99.99999757  24.99999967]  f(x): 6.011804787847472e-12  grad at x: [ 4.85882941e-06 -6.62567643e-07]  gradient norm: 4.9037964019104515e-06\n",
            "iter: 80  x: [-99.99999806  24.99999973]  f(x): 3.847555086694406e-12  grad at x: [ 3.88706354e-06 -5.30054116e-07]  gradient norm: 3.923037132984804e-06\n",
            "iter: 81  x: [-99.99999845  24.99999979]  f(x): 2.4624352554844198e-12  grad at x: [ 3.10965083e-06 -4.24043293e-07]  gradient norm: 3.1384297063878423e-06\n",
            "iter: 82  x: [-99.99999876  24.99999983]  f(x): 1.5759585771690021e-12  grad at x: [ 2.48772068e-06 -3.39234631e-07]  gradient norm: 2.510743775990694e-06\n",
            "iter: 83  x: [-99.999999    24.99999986]  f(x): 1.0086134893881614e-12  grad at x: [ 1.99017654e-06 -2.71387705e-07]  gradient norm: 2.0085950207925555e-06\n",
            "iter: 84  x: [-99.9999992   24.99999989]  f(x): 6.455126421044324e-13  grad at x: [ 1.59214125e-06 -2.17110163e-07]  gradient norm: 1.6068760277064716e-06\n",
            "iter: 85  x: [-99.99999936  24.99999991]  f(x): 4.1312809082342387e-13  grad at x: [ 1.27371300e-06 -1.73688129e-07]  gradient norm: 1.2855008219731698e-06\n",
            "iter: 86  x: [-99.99999949  24.99999993]  f(x): 2.644019840178977e-13  grad at x: [ 1.01897041e-06 -1.38950504e-07]  gradient norm: 1.0284006690349782e-06\n",
            "iter: 87  x: [-99.99999959  24.99999994]  f(x): 1.6921726745458407e-13  grad at x: [ 8.15176321e-07 -1.11160404e-07]  gradient norm: 8.227205295957652e-07\n",
            "\n",
            "Optimizer: [-99.99999959  24.99999994]\n",
            "for tolerance value= 1e-06, the minimum value of function is 1.6921726745458407e-13 and number of iterations are= 87\n",
            "For tolerance = 1e-07\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "iter: 77  x: [-99.9999962   24.99999948]  f(x): 1.4677257905583636e-11  grad at x: [ 7.59192099e-06 -1.03526195e-06]  gradient norm: 7.662181910026317e-06\n",
            "iter: 78  x: [-99.99999696  24.99999959]  f(x): 9.393445025637944e-12  grad at x: [ 6.07353678e-06 -8.28209558e-07]  gradient norm: 6.1297455169486255e-06\n",
            "iter: 79  x: [-99.99999757  24.99999967]  f(x): 6.011804787847472e-12  grad at x: [ 4.85882941e-06 -6.62567643e-07]  gradient norm: 4.9037964019104515e-06\n",
            "iter: 80  x: [-99.99999806  24.99999973]  f(x): 3.847555086694406e-12  grad at x: [ 3.88706354e-06 -5.30054116e-07]  gradient norm: 3.923037132984804e-06\n",
            "iter: 81  x: [-99.99999845  24.99999979]  f(x): 2.4624352554844198e-12  grad at x: [ 3.10965083e-06 -4.24043293e-07]  gradient norm: 3.1384297063878423e-06\n",
            "iter: 82  x: [-99.99999876  24.99999983]  f(x): 1.5759585771690021e-12  grad at x: [ 2.48772068e-06 -3.39234631e-07]  gradient norm: 2.510743775990694e-06\n",
            "iter: 83  x: [-99.999999    24.99999986]  f(x): 1.0086134893881614e-12  grad at x: [ 1.99017654e-06 -2.71387705e-07]  gradient norm: 2.0085950207925555e-06\n",
            "iter: 84  x: [-99.9999992   24.99999989]  f(x): 6.455126421044324e-13  grad at x: [ 1.59214125e-06 -2.17110163e-07]  gradient norm: 1.6068760277064716e-06\n",
            "iter: 85  x: [-99.99999936  24.99999991]  f(x): 4.1312809082342387e-13  grad at x: [ 1.27371300e-06 -1.73688129e-07]  gradient norm: 1.2855008219731698e-06\n",
            "iter: 86  x: [-99.99999949  24.99999993]  f(x): 2.644019840178977e-13  grad at x: [ 1.01897041e-06 -1.38950504e-07]  gradient norm: 1.0284006690349782e-06\n",
            "iter: 87  x: [-99.99999959  24.99999994]  f(x): 1.6921726745458407e-13  grad at x: [ 8.15176321e-07 -1.11160404e-07]  gradient norm: 8.227205295957652e-07\n",
            "iter: 88  x: [-99.99999967  24.99999996]  f(x): 1.0829905494111393e-13  grad at x: [ 6.52141068e-07 -8.89283243e-08]  gradient norm: 6.581764351330543e-07\n",
            "iter: 89  x: [-99.99999974  24.99999996]  f(x): 6.931139362896591e-14  grad at x: [ 5.21712849e-07 -7.11426580e-08]  gradient norm: 5.265411422822187e-07\n",
            "iter: 90  x: [-99.99999979  24.99999997]  f(x): 4.435929069586059e-14  grad at x: [ 4.17370273e-07 -5.69141250e-08]  gradient norm: 4.212329080015501e-07\n",
            "iter: 91  x: [-99.99999983  24.99999998]  f(x): 2.8389947026692853e-14  grad at x: [ 3.33896224e-07 -4.55313014e-08]  gradient norm: 3.369863322254649e-07\n",
            "iter: 92  x: [-99.99999987  24.99999998]  f(x): 1.8169565363772865e-14  grad at x: [ 2.67116974e-07 -3.64250425e-08]  gradient norm: 2.695890603401619e-07\n",
            "iter: 93  x: [-99.99999989  24.99999999]  f(x): 1.1628520597402065e-14  grad at x: [ 2.13693568e-07 -2.91400326e-08]  gradient norm: 2.1567123681568726e-07\n",
            "iter: 94  x: [-99.99999991  24.99999999]  f(x): 7.44225269645441e-15  grad at x: [ 1.70954848e-07 -2.33120261e-08]  gradient norm: 1.725369838203324e-07\n",
            "iter: 95  x: [-99.99999993  24.99999999]  f(x): 4.7630413502758614e-15  grad at x: [ 1.36763873e-07 -1.86496223e-08]  gradient norm: 1.3802958161605594e-07\n",
            "iter: 96  x: [-99.99999995  24.99999999]  f(x): 3.0483467857426878e-15  grad at x: [ 1.09411104e-07 -1.49196993e-08]  gradient norm: 1.104236711170696e-07\n",
            "iter: 97  x: [-99.99999996  24.99999999]  f(x): 1.950942200128245e-15  grad at x: [ 8.75288890e-08 -1.19357608e-08]  gradient norm: 8.833894271788054e-08\n",
            "\n",
            "Optimizer: [-99.99999996  24.99999999]\n",
            "for tolerance value= 1e-07, the minimum value of function is 1.950942200128245e-15 and number of iterations are= 97\n",
            "For tolerance = 1e-08\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "iter: 77  x: [-99.9999962   24.99999948]  f(x): 1.4677257905583636e-11  grad at x: [ 7.59192099e-06 -1.03526195e-06]  gradient norm: 7.662181910026317e-06\n",
            "iter: 78  x: [-99.99999696  24.99999959]  f(x): 9.393445025637944e-12  grad at x: [ 6.07353678e-06 -8.28209558e-07]  gradient norm: 6.1297455169486255e-06\n",
            "iter: 79  x: [-99.99999757  24.99999967]  f(x): 6.011804787847472e-12  grad at x: [ 4.85882941e-06 -6.62567643e-07]  gradient norm: 4.9037964019104515e-06\n",
            "iter: 80  x: [-99.99999806  24.99999973]  f(x): 3.847555086694406e-12  grad at x: [ 3.88706354e-06 -5.30054116e-07]  gradient norm: 3.923037132984804e-06\n",
            "iter: 81  x: [-99.99999845  24.99999979]  f(x): 2.4624352554844198e-12  grad at x: [ 3.10965083e-06 -4.24043293e-07]  gradient norm: 3.1384297063878423e-06\n",
            "iter: 82  x: [-99.99999876  24.99999983]  f(x): 1.5759585771690021e-12  grad at x: [ 2.48772068e-06 -3.39234631e-07]  gradient norm: 2.510743775990694e-06\n",
            "iter: 83  x: [-99.999999    24.99999986]  f(x): 1.0086134893881614e-12  grad at x: [ 1.99017654e-06 -2.71387705e-07]  gradient norm: 2.0085950207925555e-06\n",
            "iter: 84  x: [-99.9999992   24.99999989]  f(x): 6.455126421044324e-13  grad at x: [ 1.59214125e-06 -2.17110163e-07]  gradient norm: 1.6068760277064716e-06\n",
            "iter: 85  x: [-99.99999936  24.99999991]  f(x): 4.1312809082342387e-13  grad at x: [ 1.27371300e-06 -1.73688129e-07]  gradient norm: 1.2855008219731698e-06\n",
            "iter: 86  x: [-99.99999949  24.99999993]  f(x): 2.644019840178977e-13  grad at x: [ 1.01897041e-06 -1.38950504e-07]  gradient norm: 1.0284006690349782e-06\n",
            "iter: 87  x: [-99.99999959  24.99999994]  f(x): 1.6921726745458407e-13  grad at x: [ 8.15176321e-07 -1.11160404e-07]  gradient norm: 8.227205295957652e-07\n",
            "iter: 88  x: [-99.99999967  24.99999996]  f(x): 1.0829905494111393e-13  grad at x: [ 6.52141068e-07 -8.89283243e-08]  gradient norm: 6.581764351330543e-07\n",
            "iter: 89  x: [-99.99999974  24.99999996]  f(x): 6.931139362896591e-14  grad at x: [ 5.21712849e-07 -7.11426580e-08]  gradient norm: 5.265411422822187e-07\n",
            "iter: 90  x: [-99.99999979  24.99999997]  f(x): 4.435929069586059e-14  grad at x: [ 4.17370273e-07 -5.69141250e-08]  gradient norm: 4.212329080015501e-07\n",
            "iter: 91  x: [-99.99999983  24.99999998]  f(x): 2.8389947026692853e-14  grad at x: [ 3.33896224e-07 -4.55313014e-08]  gradient norm: 3.369863322254649e-07\n",
            "iter: 92  x: [-99.99999987  24.99999998]  f(x): 1.8169565363772865e-14  grad at x: [ 2.67116974e-07 -3.64250425e-08]  gradient norm: 2.695890603401619e-07\n",
            "iter: 93  x: [-99.99999989  24.99999999]  f(x): 1.1628520597402065e-14  grad at x: [ 2.13693568e-07 -2.91400326e-08]  gradient norm: 2.1567123681568726e-07\n",
            "iter: 94  x: [-99.99999991  24.99999999]  f(x): 7.44225269645441e-15  grad at x: [ 1.70954848e-07 -2.33120261e-08]  gradient norm: 1.725369838203324e-07\n",
            "iter: 95  x: [-99.99999993  24.99999999]  f(x): 4.7630413502758614e-15  grad at x: [ 1.36763873e-07 -1.86496223e-08]  gradient norm: 1.3802958161605594e-07\n",
            "iter: 96  x: [-99.99999995  24.99999999]  f(x): 3.0483467857426878e-15  grad at x: [ 1.09411104e-07 -1.49196993e-08]  gradient norm: 1.104236711170696e-07\n",
            "iter: 97  x: [-99.99999996  24.99999999]  f(x): 1.950942200128245e-15  grad at x: [ 8.75288890e-08 -1.19357608e-08]  gradient norm: 8.833894271788054e-08\n",
            "iter: 98  x: [-99.99999996  25.        ]  f(x): 1.2486028022797387e-15  grad at x: [ 7.00231055e-08 -9.54860724e-09]  gradient norm: 7.067114835007957e-08\n",
            "iter: 99  x: [-99.99999997  25.        ]  f(x): 7.991056451004438e-16  grad at x: [ 5.60184787e-08 -7.63888863e-09]  gradient norm: 5.653691343186127e-08\n",
            "iter: 100  x: [-99.99999998  25.        ]  f(x): 5.114276041798751e-16  grad at x: [ 4.48147830e-08 -6.11110806e-09]  gradient norm: 4.522953036147402e-08\n",
            "iter: 101  x: [-99.99999998  25.        ]  f(x): 3.273136736226476e-16  grad at x: [ 3.58518264e-08 -4.88888929e-09]  gradient norm: 3.618362467319423e-08\n",
            "iter: 102  x: [-99.99999999  25.        ]  f(x): 2.0948066960088715e-16  grad at x: [ 2.86814554e-08 -3.91111143e-09]  gradient norm: 2.8946894106338052e-08\n",
            "iter: 103  x: [-99.99999999  25.        ]  f(x): 1.3406756333049646e-16  grad at x: [ 2.29451587e-08 -3.12888915e-09]  gradient norm: 2.315750965285313e-08\n",
            "iter: 104  x: [-99.99999999  25.        ]  f(x): 8.580329625991355e-17  grad at x: [ 1.83561326e-08 -2.50311416e-09]  gradient norm: 1.8526013738515206e-08\n",
            "iter: 105  x: [-99.99999999  25.        ]  f(x): 5.491407071505271e-17  grad at x: [ 1.46849004e-08 -2.00249417e-09]  gradient norm: 1.4820805742610988e-08\n",
            "iter: 106  x: [-99.99999999  25.        ]  f(x): 3.5144973006330675e-17  grad at x: [ 1.17479146e-08 -1.60199676e-09]  gradient norm: 1.1856639153880104e-08\n",
            "iter: 107  x: [-100.   25.]  f(x): 2.249273021138356e-17  grad at x: [ 9.39832034e-09 -1.28159883e-09]  gradient norm: 9.485300250679166e-09\n",
            "\n",
            "Optimizer: [-100.   25.]\n",
            "for tolerance value= 1e-08, the minimum value of function is 2.249273021138356e-17 and number of iterations are= 107\n",
            "For tolerance = 1e-09\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "iter: 77  x: [-99.9999962   24.99999948]  f(x): 1.4677257905583636e-11  grad at x: [ 7.59192099e-06 -1.03526195e-06]  gradient norm: 7.662181910026317e-06\n",
            "iter: 78  x: [-99.99999696  24.99999959]  f(x): 9.393445025637944e-12  grad at x: [ 6.07353678e-06 -8.28209558e-07]  gradient norm: 6.1297455169486255e-06\n",
            "iter: 79  x: [-99.99999757  24.99999967]  f(x): 6.011804787847472e-12  grad at x: [ 4.85882941e-06 -6.62567643e-07]  gradient norm: 4.9037964019104515e-06\n",
            "iter: 80  x: [-99.99999806  24.99999973]  f(x): 3.847555086694406e-12  grad at x: [ 3.88706354e-06 -5.30054116e-07]  gradient norm: 3.923037132984804e-06\n",
            "iter: 81  x: [-99.99999845  24.99999979]  f(x): 2.4624352554844198e-12  grad at x: [ 3.10965083e-06 -4.24043293e-07]  gradient norm: 3.1384297063878423e-06\n",
            "iter: 82  x: [-99.99999876  24.99999983]  f(x): 1.5759585771690021e-12  grad at x: [ 2.48772068e-06 -3.39234631e-07]  gradient norm: 2.510743775990694e-06\n",
            "iter: 83  x: [-99.999999    24.99999986]  f(x): 1.0086134893881614e-12  grad at x: [ 1.99017654e-06 -2.71387705e-07]  gradient norm: 2.0085950207925555e-06\n",
            "iter: 84  x: [-99.9999992   24.99999989]  f(x): 6.455126421044324e-13  grad at x: [ 1.59214125e-06 -2.17110163e-07]  gradient norm: 1.6068760277064716e-06\n",
            "iter: 85  x: [-99.99999936  24.99999991]  f(x): 4.1312809082342387e-13  grad at x: [ 1.27371300e-06 -1.73688129e-07]  gradient norm: 1.2855008219731698e-06\n",
            "iter: 86  x: [-99.99999949  24.99999993]  f(x): 2.644019840178977e-13  grad at x: [ 1.01897041e-06 -1.38950504e-07]  gradient norm: 1.0284006690349782e-06\n",
            "iter: 87  x: [-99.99999959  24.99999994]  f(x): 1.6921726745458407e-13  grad at x: [ 8.15176321e-07 -1.11160404e-07]  gradient norm: 8.227205295957652e-07\n",
            "iter: 88  x: [-99.99999967  24.99999996]  f(x): 1.0829905494111393e-13  grad at x: [ 6.52141068e-07 -8.89283243e-08]  gradient norm: 6.581764351330543e-07\n",
            "iter: 89  x: [-99.99999974  24.99999996]  f(x): 6.931139362896591e-14  grad at x: [ 5.21712849e-07 -7.11426580e-08]  gradient norm: 5.265411422822187e-07\n",
            "iter: 90  x: [-99.99999979  24.99999997]  f(x): 4.435929069586059e-14  grad at x: [ 4.17370273e-07 -5.69141250e-08]  gradient norm: 4.212329080015501e-07\n",
            "iter: 91  x: [-99.99999983  24.99999998]  f(x): 2.8389947026692853e-14  grad at x: [ 3.33896224e-07 -4.55313014e-08]  gradient norm: 3.369863322254649e-07\n",
            "iter: 92  x: [-99.99999987  24.99999998]  f(x): 1.8169565363772865e-14  grad at x: [ 2.67116974e-07 -3.64250425e-08]  gradient norm: 2.695890603401619e-07\n",
            "iter: 93  x: [-99.99999989  24.99999999]  f(x): 1.1628520597402065e-14  grad at x: [ 2.13693568e-07 -2.91400326e-08]  gradient norm: 2.1567123681568726e-07\n",
            "iter: 94  x: [-99.99999991  24.99999999]  f(x): 7.44225269645441e-15  grad at x: [ 1.70954848e-07 -2.33120261e-08]  gradient norm: 1.725369838203324e-07\n",
            "iter: 95  x: [-99.99999993  24.99999999]  f(x): 4.7630413502758614e-15  grad at x: [ 1.36763873e-07 -1.86496223e-08]  gradient norm: 1.3802958161605594e-07\n",
            "iter: 96  x: [-99.99999995  24.99999999]  f(x): 3.0483467857426878e-15  grad at x: [ 1.09411104e-07 -1.49196993e-08]  gradient norm: 1.104236711170696e-07\n",
            "iter: 97  x: [-99.99999996  24.99999999]  f(x): 1.950942200128245e-15  grad at x: [ 8.75288890e-08 -1.19357608e-08]  gradient norm: 8.833894271788054e-08\n",
            "iter: 98  x: [-99.99999996  25.        ]  f(x): 1.2486028022797387e-15  grad at x: [ 7.00231055e-08 -9.54860724e-09]  gradient norm: 7.067114835007957e-08\n",
            "iter: 99  x: [-99.99999997  25.        ]  f(x): 7.991056451004438e-16  grad at x: [ 5.60184787e-08 -7.63888863e-09]  gradient norm: 5.653691343186127e-08\n",
            "iter: 100  x: [-99.99999998  25.        ]  f(x): 5.114276041798751e-16  grad at x: [ 4.48147830e-08 -6.11110806e-09]  gradient norm: 4.522953036147402e-08\n",
            "iter: 101  x: [-99.99999998  25.        ]  f(x): 3.273136736226476e-16  grad at x: [ 3.58518264e-08 -4.88888929e-09]  gradient norm: 3.618362467319423e-08\n",
            "iter: 102  x: [-99.99999999  25.        ]  f(x): 2.0948066960088715e-16  grad at x: [ 2.86814554e-08 -3.91111143e-09]  gradient norm: 2.8946894106338052e-08\n",
            "iter: 103  x: [-99.99999999  25.        ]  f(x): 1.3406756333049646e-16  grad at x: [ 2.29451587e-08 -3.12888915e-09]  gradient norm: 2.315750965285313e-08\n",
            "iter: 104  x: [-99.99999999  25.        ]  f(x): 8.580329625991355e-17  grad at x: [ 1.83561326e-08 -2.50311416e-09]  gradient norm: 1.8526013738515206e-08\n",
            "iter: 105  x: [-99.99999999  25.        ]  f(x): 5.491407071505271e-17  grad at x: [ 1.46849004e-08 -2.00249417e-09]  gradient norm: 1.4820805742610988e-08\n",
            "iter: 106  x: [-99.99999999  25.        ]  f(x): 3.5144973006330675e-17  grad at x: [ 1.17479146e-08 -1.60199676e-09]  gradient norm: 1.1856639153880104e-08\n",
            "iter: 107  x: [-100.   25.]  f(x): 2.249273021138356e-17  grad at x: [ 9.39832034e-09 -1.28159883e-09]  gradient norm: 9.485300250679166e-09\n",
            "iter: 108  x: [-100.   25.]  f(x): 1.4395325237483076e-17  grad at x: [ 7.51865059e-09 -1.02527764e-09]  gradient norm: 7.588234376317874e-09\n",
            "iter: 109  x: [-100.   25.]  f(x): 9.212991056565054e-18  grad at x: [ 6.01491479e-09 -8.20222112e-10]  gradient norm: 6.070581868837633e-09\n",
            "iter: 110  x: [-100.   25.]  f(x): 5.8963284187853376e-18  grad at x: [ 4.81193752e-09 -6.56179111e-10]  gradient norm: 4.856471319295662e-09\n",
            "iter: 111  x: [-100.   25.]  f(x): 3.77364981502848e-18  grad at x: [ 3.84955001e-09 -5.24941868e-10]  gradient norm: 3.885176863427703e-09\n",
            "iter: 112  x: [-100.   25.]  f(x): 2.415126830368555e-18  grad at x: [ 3.07963433e-09 -4.19952073e-10]  gradient norm: 3.1081356665168623e-09\n",
            "iter: 113  x: [-100.   25.]  f(x): 1.5456806940076629e-18  grad at x: [ 2.46370746e-09 -3.35958816e-10]  gradient norm: 2.4865081491985205e-09\n",
            "iter: 114  x: [-100.   25.]  f(x): 9.892304242937514e-19  grad at x: [ 1.97096028e-09 -2.68769895e-10]  gradient norm: 1.9892012711575985e-09\n",
            "iter: 115  x: [-100.   25.]  f(x): 6.331165872484927e-19  grad at x: [ 1.57677960e-09 -2.15017337e-10]  gradient norm: 1.5913724733681838e-09\n",
            "iter: 116  x: [-100.   25.]  f(x): 4.051875677316165e-19  grad at x: [ 1.26141231e-09 -1.72015291e-10]  gradient norm: 1.2730869062740635e-09\n",
            "iter: 117  x: [-100.   25.]  f(x): 2.5932568184073297e-19  grad at x: [ 1.00914122e-09 -1.37610812e-10]  gradient norm: 1.0184805974405855e-09\n",
            "iter: 118  x: [-100.   25.]  f(x): 1.6597088735501767e-19  grad at x: [ 8.07318656e-10 -1.10091491e-10]  gradient norm: 8.147904941885801e-10\n",
            "\n",
            "Optimizer: [-100.   25.]\n",
            "for tolerance value= 1e-09, the minimum value of function is 1.6597088735501767e-19 and number of iterations are= 118\n",
            "For tolerance = 1e-10\n",
            "\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "iter: 77  x: [-99.9999962   24.99999948]  f(x): 1.4677257905583636e-11  grad at x: [ 7.59192099e-06 -1.03526195e-06]  gradient norm: 7.662181910026317e-06\n",
            "iter: 78  x: [-99.99999696  24.99999959]  f(x): 9.393445025637944e-12  grad at x: [ 6.07353678e-06 -8.28209558e-07]  gradient norm: 6.1297455169486255e-06\n",
            "iter: 79  x: [-99.99999757  24.99999967]  f(x): 6.011804787847472e-12  grad at x: [ 4.85882941e-06 -6.62567643e-07]  gradient norm: 4.9037964019104515e-06\n",
            "iter: 80  x: [-99.99999806  24.99999973]  f(x): 3.847555086694406e-12  grad at x: [ 3.88706354e-06 -5.30054116e-07]  gradient norm: 3.923037132984804e-06\n",
            "iter: 81  x: [-99.99999845  24.99999979]  f(x): 2.4624352554844198e-12  grad at x: [ 3.10965083e-06 -4.24043293e-07]  gradient norm: 3.1384297063878423e-06\n",
            "iter: 82  x: [-99.99999876  24.99999983]  f(x): 1.5759585771690021e-12  grad at x: [ 2.48772068e-06 -3.39234631e-07]  gradient norm: 2.510743775990694e-06\n",
            "iter: 83  x: [-99.999999    24.99999986]  f(x): 1.0086134893881614e-12  grad at x: [ 1.99017654e-06 -2.71387705e-07]  gradient norm: 2.0085950207925555e-06\n",
            "iter: 84  x: [-99.9999992   24.99999989]  f(x): 6.455126421044324e-13  grad at x: [ 1.59214125e-06 -2.17110163e-07]  gradient norm: 1.6068760277064716e-06\n",
            "iter: 85  x: [-99.99999936  24.99999991]  f(x): 4.1312809082342387e-13  grad at x: [ 1.27371300e-06 -1.73688129e-07]  gradient norm: 1.2855008219731698e-06\n",
            "iter: 86  x: [-99.99999949  24.99999993]  f(x): 2.644019840178977e-13  grad at x: [ 1.01897041e-06 -1.38950504e-07]  gradient norm: 1.0284006690349782e-06\n",
            "iter: 87  x: [-99.99999959  24.99999994]  f(x): 1.6921726745458407e-13  grad at x: [ 8.15176321e-07 -1.11160404e-07]  gradient norm: 8.227205295957652e-07\n",
            "iter: 88  x: [-99.99999967  24.99999996]  f(x): 1.0829905494111393e-13  grad at x: [ 6.52141068e-07 -8.89283243e-08]  gradient norm: 6.581764351330543e-07\n",
            "iter: 89  x: [-99.99999974  24.99999996]  f(x): 6.931139362896591e-14  grad at x: [ 5.21712849e-07 -7.11426580e-08]  gradient norm: 5.265411422822187e-07\n",
            "iter: 90  x: [-99.99999979  24.99999997]  f(x): 4.435929069586059e-14  grad at x: [ 4.17370273e-07 -5.69141250e-08]  gradient norm: 4.212329080015501e-07\n",
            "iter: 91  x: [-99.99999983  24.99999998]  f(x): 2.8389947026692853e-14  grad at x: [ 3.33896224e-07 -4.55313014e-08]  gradient norm: 3.369863322254649e-07\n",
            "iter: 92  x: [-99.99999987  24.99999998]  f(x): 1.8169565363772865e-14  grad at x: [ 2.67116974e-07 -3.64250425e-08]  gradient norm: 2.695890603401619e-07\n",
            "iter: 93  x: [-99.99999989  24.99999999]  f(x): 1.1628520597402065e-14  grad at x: [ 2.13693568e-07 -2.91400326e-08]  gradient norm: 2.1567123681568726e-07\n",
            "iter: 94  x: [-99.99999991  24.99999999]  f(x): 7.44225269645441e-15  grad at x: [ 1.70954848e-07 -2.33120261e-08]  gradient norm: 1.725369838203324e-07\n",
            "iter: 95  x: [-99.99999993  24.99999999]  f(x): 4.7630413502758614e-15  grad at x: [ 1.36763873e-07 -1.86496223e-08]  gradient norm: 1.3802958161605594e-07\n",
            "iter: 96  x: [-99.99999995  24.99999999]  f(x): 3.0483467857426878e-15  grad at x: [ 1.09411104e-07 -1.49196993e-08]  gradient norm: 1.104236711170696e-07\n",
            "iter: 97  x: [-99.99999996  24.99999999]  f(x): 1.950942200128245e-15  grad at x: [ 8.75288890e-08 -1.19357608e-08]  gradient norm: 8.833894271788054e-08\n",
            "iter: 98  x: [-99.99999996  25.        ]  f(x): 1.2486028022797387e-15  grad at x: [ 7.00231055e-08 -9.54860724e-09]  gradient norm: 7.067114835007957e-08\n",
            "iter: 99  x: [-99.99999997  25.        ]  f(x): 7.991056451004438e-16  grad at x: [ 5.60184787e-08 -7.63888863e-09]  gradient norm: 5.653691343186127e-08\n",
            "iter: 100  x: [-99.99999998  25.        ]  f(x): 5.114276041798751e-16  grad at x: [ 4.48147830e-08 -6.11110806e-09]  gradient norm: 4.522953036147402e-08\n",
            "iter: 101  x: [-99.99999998  25.        ]  f(x): 3.273136736226476e-16  grad at x: [ 3.58518264e-08 -4.88888929e-09]  gradient norm: 3.618362467319423e-08\n",
            "iter: 102  x: [-99.99999999  25.        ]  f(x): 2.0948066960088715e-16  grad at x: [ 2.86814554e-08 -3.91111143e-09]  gradient norm: 2.8946894106338052e-08\n",
            "iter: 103  x: [-99.99999999  25.        ]  f(x): 1.3406756333049646e-16  grad at x: [ 2.29451587e-08 -3.12888915e-09]  gradient norm: 2.315750965285313e-08\n",
            "iter: 104  x: [-99.99999999  25.        ]  f(x): 8.580329625991355e-17  grad at x: [ 1.83561326e-08 -2.50311416e-09]  gradient norm: 1.8526013738515206e-08\n",
            "iter: 105  x: [-99.99999999  25.        ]  f(x): 5.491407071505271e-17  grad at x: [ 1.46849004e-08 -2.00249417e-09]  gradient norm: 1.4820805742610988e-08\n",
            "iter: 106  x: [-99.99999999  25.        ]  f(x): 3.5144973006330675e-17  grad at x: [ 1.17479146e-08 -1.60199676e-09]  gradient norm: 1.1856639153880104e-08\n",
            "iter: 107  x: [-100.   25.]  f(x): 2.249273021138356e-17  grad at x: [ 9.39832034e-09 -1.28159883e-09]  gradient norm: 9.485300250679166e-09\n",
            "iter: 108  x: [-100.   25.]  f(x): 1.4395325237483076e-17  grad at x: [ 7.51865059e-09 -1.02527764e-09]  gradient norm: 7.588234376317874e-09\n",
            "iter: 109  x: [-100.   25.]  f(x): 9.212991056565054e-18  grad at x: [ 6.01491479e-09 -8.20222112e-10]  gradient norm: 6.070581868837633e-09\n",
            "iter: 110  x: [-100.   25.]  f(x): 5.8963284187853376e-18  grad at x: [ 4.81193752e-09 -6.56179111e-10]  gradient norm: 4.856471319295662e-09\n",
            "iter: 111  x: [-100.   25.]  f(x): 3.77364981502848e-18  grad at x: [ 3.84955001e-09 -5.24941868e-10]  gradient norm: 3.885176863427703e-09\n",
            "iter: 112  x: [-100.   25.]  f(x): 2.415126830368555e-18  grad at x: [ 3.07963433e-09 -4.19952073e-10]  gradient norm: 3.1081356665168623e-09\n",
            "iter: 113  x: [-100.   25.]  f(x): 1.5456806940076629e-18  grad at x: [ 2.46370746e-09 -3.35958816e-10]  gradient norm: 2.4865081491985205e-09\n",
            "iter: 114  x: [-100.   25.]  f(x): 9.892304242937514e-19  grad at x: [ 1.97096028e-09 -2.68769895e-10]  gradient norm: 1.9892012711575985e-09\n",
            "iter: 115  x: [-100.   25.]  f(x): 6.331165872484927e-19  grad at x: [ 1.57677960e-09 -2.15017337e-10]  gradient norm: 1.5913724733681838e-09\n",
            "iter: 116  x: [-100.   25.]  f(x): 4.051875677316165e-19  grad at x: [ 1.26141231e-09 -1.72015291e-10]  gradient norm: 1.2730869062740635e-09\n",
            "iter: 117  x: [-100.   25.]  f(x): 2.5932568184073297e-19  grad at x: [ 1.00914122e-09 -1.37610812e-10]  gradient norm: 1.0184805974405855e-09\n",
            "iter: 118  x: [-100.   25.]  f(x): 1.6597088735501767e-19  grad at x: [ 8.07318656e-10 -1.10091491e-10]  gradient norm: 8.147904941885801e-10\n",
            "iter: 119  x: [-100.   25.]  f(x): 1.0622130532794855e-19  grad at x: [ 6.45854925e-10 -8.80717721e-11]  gradient norm: 6.518322033405485e-10\n",
            "iter: 120  x: [-100.   25.]  f(x): 6.798016691388378e-20  grad at x: [ 5.16678256e-10 -7.04574177e-11]  gradient norm: 5.214601304563323e-10\n",
            "iter: 121  x: [-100.   25.]  f(x): 4.3506172093533196e-20  grad at x: [ 4.13336920e-10 -5.63673552e-11]  gradient norm: 4.171626641660694e-10\n",
            "iter: 122  x: [-100.   25.]  f(x): 2.784200645323375e-20  grad at x: [ 3.30658168e-10 -4.50910420e-11]  gradient norm: 3.337184828758141e-10\n",
            "iter: 123  x: [-100.   25.]  f(x): 1.7818157940313658e-20  grad at x: [ 2.64520850e-10 -3.60742547e-11]  gradient norm: 2.6696934610785306e-10\n",
            "iter: 124  x: [-100.   25.]  f(x): 1.1404865029367773e-20  grad at x: [ 2.11628048e-10 -2.88622459e-11]  gradient norm: 2.135871253551372e-10\n",
            "iter: 125  x: [-100.   25.]  f(x): 7.299627627939442e-21  grad at x: [ 1.69308123e-10 -2.30926389e-11]  gradient norm: 1.7087571656545515e-10\n",
            "iter: 126  x: [-100.   25.]  f(x): 4.6725316383975134e-21  grad at x: [ 1.35457867e-10 -1.84741111e-11]  gradient norm: 1.3671183764981748e-10\n",
            "iter: 127  x: [-100.   25.]  f(x): 2.9907282521835495e-21  grad at x: [ 1.08371978e-10 -1.47792889e-11]  gradient norm: 1.0937510232559418e-10\n",
            "iter: 128  x: [-100.   25.]  f(x): 1.9135732950098918e-21  grad at x: [ 8.66862138e-11 -1.18234311e-11]  gradient norm: 8.748881745708744e-11\n",
            "\n",
            "Optimizer: [-100.   25.]\n",
            "for tolerance value= 1e-10, the minimum value of function is 1.9135732950098918e-21 and number of iterations are= 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list_of_tolerance,Iterations)\n",
        "plt.ylabel('Iterations')\n",
        "plt.xlabel('Tolerance')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "MmibF7EfB0-1",
        "outputId": "db02cd8d-e8d2-418b-a122-15329db60fde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we observed that as we decrease the value of tolerance the number of iterations increses.and with the decrease in the tolerance value the value of optimizer approaches to $[-100,25]$ and the optimal value tends to zero."
      ],
      "metadata": {
        "id": "2KLuI-je75EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.4"
      ],
      "metadata": {
        "id": "zTrpc3lot5Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_step_length=[0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]"
      ],
      "metadata": {
        "id": "d4Lx1ZFem8pO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Iterations=[]\n",
        "for n in list_of_step_length:\n",
        "  my_start_x = np.array([10,10])\n",
        "  my_steplength = n\n",
        "  my_tol= 1e-5 #1\n",
        "  print(f\"For step length = {n}\")\n",
        "  opt_x, fvals_ret,iterations= find_minimizer(my_start_x, my_tol, my_steplength)\n",
        "  print('Optimizer:',opt_x,)\n",
        "  print(f\"for step length = {n}, the minimum value of function is {evalf(opt_x)} and number of iterations are= {iterations}\\n\")\n",
        "  Iterations.append(iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gQOQplluUjH",
        "outputId": "4fd0043f-d6de-48d6-d430-3d79dd154444"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iter: 4623  x: [-99.98948317  24.99856589]  f(x): 0.00011266049912664543  grad at x: [ 0.02103367 -0.00286823]  gradient norm: 0.021228330045167984\n",
            "iter: 4624  x: [-99.9895042   24.99856875]  f(x): 0.00011221030777216574  grad at x: [ 0.0209916  -0.00286249]  gradient norm: 0.021185873385080517\n",
            "iter: 4625  x: [-99.98952519  24.99857162]  f(x): 0.00011176191538238311  grad at x: [ 0.02094962 -0.00285677]  gradient norm: 0.021143501638317443\n",
            "iter: 4626  x: [-99.98954614  24.99857447]  f(x): 0.00011131531476846489  grad at x: [ 0.02090772 -0.00285105]  gradient norm: 0.021101214635036048\n",
            "iter: 4627  x: [-99.98956705  24.99857732]  f(x): 0.00011087049877065489  grad at x: [ 0.0208659  -0.00284535]  gradient norm: 0.021059012205766434\n",
            "iter: 4628  x: [-99.98958791  24.99858017]  f(x): 0.0001104274602575703  grad at x: [ 0.02082417 -0.00283966]  gradient norm: 0.02101689418135518\n",
            "iter: 4629  x: [-99.98960874  24.99858301]  f(x): 0.00010998619212638219  grad at x: [ 0.02078252 -0.00283398]  gradient norm: 0.020974860392992577\n",
            "iter: 4630  x: [-99.98962952  24.99858584]  f(x): 0.00010954668730271813  grad at x: [ 0.02074096 -0.00282831]  gradient norm: 0.02093291067221356\n",
            "iter: 4631  x: [-99.98965026  24.99858867]  f(x): 0.00010910893874024087  grad at x: [ 0.02069948 -0.00282266]  gradient norm: 0.020891044850867645\n",
            "iter: 4632  x: [-99.98967096  24.99859149]  f(x): 0.00010867293942116197  grad at x: [ 0.02065808 -0.00281701]  gradient norm: 0.0208492627611781\n",
            "iter: 4633  x: [-99.98969162  24.99859431]  f(x): 0.00010823868235522342  grad at x: [ 0.02061676 -0.00281138]  gradient norm: 0.020807564235654632\n",
            "iter: 4634  x: [-99.98971224  24.99859712]  f(x): 0.00010780616058049477  grad at x: [ 0.02057553 -0.00280575]  gradient norm: 0.020765949107179743\n",
            "iter: 4635  x: [-99.98973281  24.99859993]  f(x): 0.000107375367162671  grad at x: [ 0.02053438 -0.00280014]  gradient norm: 0.020724417208951473\n",
            "iter: 4636  x: [-99.98975335  24.99860273]  f(x): 0.00010694629519556254  grad at x: [ 0.02049331 -0.00279454]  gradient norm: 0.020682968374540686\n",
            "iter: 4637  x: [-99.98977384  24.99860552]  f(x): 0.00010651893780010549  grad at x: [ 0.02045232 -0.00278895]  gradient norm: 0.020641602437805596\n",
            "iter: 4638  x: [-99.98979429  24.99860831]  f(x): 0.00010609328812455312  grad at x: [ 0.02041142 -0.00278338]  gradient norm: 0.02060031923291997\n",
            "iter: 4639  x: [-99.9898147  24.9986111]  f(x): 0.00010566933934525274  grad at x: [ 0.02037059 -0.00277781]  gradient norm: 0.020559118594458543\n",
            "iter: 4640  x: [-99.98983507  24.99861387]  f(x): 0.00010524708466508164  grad at x: [ 0.02032985 -0.00277225]  gradient norm: 0.02051800035725525\n",
            "iter: 4641  x: [-99.9898554   24.99861665]  f(x): 0.00010482651731479396  grad at x: [ 0.02028919 -0.00276671]  gradient norm: 0.02047696435654406\n",
            "iter: 4642  x: [-99.98987569  24.99861941]  f(x): 0.0001044076305514827  grad at x: [ 0.02024862 -0.00276117]  gradient norm: 0.020436010427819094\n",
            "iter: 4643  x: [-99.98989594  24.99862217]  f(x): 0.00010399041765990183  grad at x: [ 0.02020812 -0.00275565]  gradient norm: 0.020395138406973545\n",
            "iter: 4644  x: [-99.98991615  24.99862493]  f(x): 0.00010357487195095448  grad at x: [ 0.0201677  -0.00275014]  gradient norm: 0.02035434813016172\n",
            "iter: 4645  x: [-99.98993632  24.99862768]  f(x): 0.00010316098676270427  grad at x: [ 0.02012737 -0.00274464]  gradient norm: 0.020313639433907876\n",
            "iter: 4646  x: [-99.98995644  24.99863042]  f(x): 0.00010274875545973885  grad at x: [ 0.02008711 -0.00273915]  gradient norm: 0.02027301215505371\n",
            "iter: 4647  x: [-99.98997653  24.99863316]  f(x): 0.00010233817143304203  grad at x: [ 0.02004694 -0.00273367]  gradient norm: 0.020232466130755494\n",
            "iter: 4648  x: [-99.98999658  24.9986359 ]  f(x): 0.00010192922809992467  grad at x: [ 0.02000684 -0.00272821]  gradient norm: 0.020192001198486957\n",
            "iter: 4649  x: [-99.99001658  24.99863863]  f(x): 0.00010152191890445525  grad at x: [ 0.01996683 -0.00272275]  gradient norm: 0.020151617196091758\n",
            "iter: 4650  x: [-99.99003655  24.99864135]  f(x): 0.00010111623731653632  grad at x: [ 0.0199269 -0.0027173]  gradient norm: 0.020111313961701887\n",
            "iter: 4651  x: [-99.99005648  24.99864407]  f(x): 0.00010071217683209246  grad at x: [ 0.01988704 -0.00271187]  gradient norm: 0.02007109133376583\n",
            "iter: 4652  x: [-99.99007637  24.99864678]  f(x): 0.00010030973097351813  grad at x: [ 0.01984727 -0.00270645]  gradient norm: 0.020030949151102963\n",
            "iter: 4653  x: [-99.99009621  24.99864948]  f(x): 9.990889328846877e-05  grad at x: [ 0.01980757 -0.00270103]  gradient norm: 0.019990887252792834\n",
            "iter: 4654  x: [-99.99011602  24.99865218]  f(x): 9.950965735088404e-05  grad at x: [ 0.01976796 -0.00269563]  gradient norm: 0.01995090547828685\n",
            "iter: 4655  x: [-99.99013579  24.99865488]  f(x): 9.911201676003564e-05  grad at x: [ 0.01972842 -0.00269024]  gradient norm: 0.019911003667322815\n",
            "iter: 4656  x: [-99.99015552  24.99865757]  f(x): 9.871596514101318e-05  grad at x: [ 0.01968897 -0.00268486]  gradient norm: 0.0198711816599832\n",
            "iter: 4657  x: [-99.99017521  24.99866026]  f(x): 9.832149614433717e-05  grad at x: [ 0.01964959 -0.00267949]  gradient norm: 0.019831439296666007\n",
            "iter: 4658  x: [-99.99019486  24.99866293]  f(x): 9.792860344587351e-05  grad at x: [ 0.01961029 -0.00267413]  gradient norm: 0.01979177641808572\n",
            "iter: 4659  x: [-99.99021447  24.99866561]  f(x): 9.75372807464321e-05  grad at x: [ 0.01957107 -0.00266878]  gradient norm: 0.01975219286524229\n",
            "iter: 4660  x: [-99.99023404  24.99866828]  f(x): 9.71475217725459e-05  grad at x: [ 0.01953193 -0.00266344]  gradient norm: 0.019712688479509424\n",
            "iter: 4661  x: [-99.99025357  24.99867094]  f(x): 9.675932027550238e-05  grad at x: [ 0.01949286 -0.00265812]  gradient norm: 0.019673263102546298\n",
            "iter: 4662  x: [-99.99027306  24.9986736 ]  f(x): 9.63726700315668e-05  grad at x: [ 0.01945388 -0.0026528 ]  gradient norm: 0.019633916576329524\n",
            "iter: 4663  x: [-99.99029252  24.99867625]  f(x): 9.598756484214569e-05  grad at x: [ 0.01941497 -0.0026475 ]  gradient norm: 0.019594648743179417\n",
            "iter: 4664  x: [-99.99031193  24.9986789 ]  f(x): 9.560399853312141e-05  grad at x: [ 0.01937614 -0.0026422 ]  gradient norm: 0.019555459445701747\n",
            "iter: 4665  x: [-99.99033131  24.99868154]  f(x): 9.522196495507459e-05  grad at x: [ 0.01933739 -0.00263692]  gradient norm: 0.019516348526819723\n",
            "iter: 4666  x: [-99.99035064  24.99868418]  f(x): 9.484145798315397e-05  grad at x: [ 0.01929871 -0.00263164]  gradient norm: 0.019477315829770175\n",
            "iter: 4667  x: [-99.99036994  24.99868681]  f(x): 9.44624715170217e-05  grad at x: [ 0.01926011 -0.00262638]  gradient norm: 0.019438361198107386\n",
            "iter: 4668  x: [-99.9903892   24.99868944]  f(x): 9.408499948073333e-05  grad at x: [ 0.01922159 -0.00262113]  gradient norm: 0.019399484475700206\n",
            "iter: 4669  x: [-99.99040842  24.99869206]  f(x): 9.370903582292813e-05  grad at x: [ 0.01918315 -0.00261588]  gradient norm: 0.019360685506761184\n",
            "iter: 4670  x: [-99.99042761  24.99869467]  f(x): 9.333457451565614e-05  grad at x: [ 0.01914478 -0.00261065]  gradient norm: 0.019321964135734872\n",
            "iter: 4671  x: [-99.99044675  24.99869728]  f(x): 9.296160955588733e-05  grad at x: [ 0.0191065  -0.00260543]  gradient norm: 0.01928332020746296\n",
            "iter: 4672  x: [-99.99046586  24.99869989]  f(x): 9.25901349641135e-05  grad at x: [ 0.01906828 -0.00260022]  gradient norm: 0.019244753567049227\n",
            "iter: 4673  x: [-99.99048493  24.99870249]  f(x): 9.222014478476716e-05  grad at x: [ 0.01903015 -0.00259502]  gradient norm: 0.019206264059912034\n",
            "iter: 4674  x: [-99.99050396  24.99870509]  f(x): 9.18516330861222e-05  grad at x: [ 0.01899209 -0.00258983]  gradient norm: 0.019167851531783336\n",
            "iter: 4675  x: [-99.99052295  24.99870767]  f(x): 9.148459396025023e-05  grad at x: [ 0.0189541  -0.00258465]  gradient norm: 0.019129515828713514\n",
            "iter: 4676  x: [-99.9905419   24.99871026]  f(x): 9.111902152287585e-05  grad at x: [ 0.01891619 -0.00257948]  gradient norm: 0.0190912567970656\n",
            "iter: 4677  x: [-99.99056082  24.99871284]  f(x): 9.075490991279688e-05  grad at x: [ 0.01887836 -0.00257432]  gradient norm: 0.019053074283463745\n",
            "iter: 4678  x: [-99.9905797   24.99871541]  f(x): 9.039225329284422e-05  grad at x: [ 0.0188406  -0.00256917]  gradient norm: 0.01901496813490301\n",
            "iter: 4679  x: [-99.99059854  24.99871798]  f(x): 9.003104584871914e-05  grad at x: [ 0.01880292 -0.00256403]  gradient norm: 0.018976938198636696\n",
            "iter: 4680  x: [-99.99061734  24.99872055]  f(x): 8.967128178946198e-05  grad at x: [ 0.01876532 -0.00255891]  gradient norm: 0.0189389843222346\n",
            "iter: 4681  x: [-99.99063611  24.99872311]  f(x): 8.93129553473458e-05  grad at x: [ 0.01872779 -0.00255379]  gradient norm: 0.018901106353581085\n",
            "iter: 4682  x: [-99.99065483  24.99872566]  f(x): 8.895606077780696e-05  grad at x: [ 0.01869033 -0.00254868]  gradient norm: 0.018863304140877012\n",
            "iter: 4683  x: [-99.99067352  24.99872821]  f(x): 8.860059235906532e-05  grad at x: [ 0.01865295 -0.00254358]  gradient norm: 0.018825577532608696\n",
            "iter: 4684  x: [-99.99069218  24.99873075]  f(x): 8.824654439206707e-05  grad at x: [ 0.01861564 -0.0025385 ]  gradient norm: 0.018787926377550775\n",
            "iter: 4685  x: [-99.99071079  24.99873329]  f(x): 8.78939112006553e-05  grad at x: [ 0.01857841 -0.00253342]  gradient norm: 0.01875035052479343\n",
            "iter: 4686  x: [-99.99072937  24.99873582]  f(x): 8.754268713147461e-05  grad at x: [ 0.01854126 -0.00252835]  gradient norm: 0.018712849823741398\n",
            "iter: 4687  x: [-99.99074791  24.99873835]  f(x): 8.719286655362187e-05  grad at x: [ 0.01850417 -0.0025233 ]  gradient norm: 0.018675424124085842\n",
            "iter: 4688  x: [-99.99076642  24.99874088]  f(x): 8.684444385885218e-05  grad at x: [ 0.01846717 -0.00251825]  gradient norm: 0.018638073275835373\n",
            "iter: 4689  x: [-99.99078488  24.99874339]  f(x): 8.64974134611865e-05  grad at x: [ 0.01843023 -0.00251321]  gradient norm: 0.018600797129283086\n",
            "iter: 4690  x: [-99.99080331  24.99874591]  f(x): 8.615176979687361e-05  grad at x: [ 0.01839337 -0.00250819]  gradient norm: 0.01856359553501138\n",
            "iter: 4691  x: [-99.99082171  24.99874841]  f(x): 8.580750732479363e-05  grad at x: [ 0.01835658 -0.00250317]  gradient norm: 0.018526468343944415\n",
            "iter: 4692  x: [-99.99084006  24.99875092]  f(x): 8.54646205256162e-05  grad at x: [ 0.01831987 -0.00249816]  gradient norm: 0.018489415407266525\n",
            "iter: 4693  x: [-99.99085838  24.99875342]  f(x): 8.512310390196326e-05  grad at x: [ 0.01828323 -0.00249317]  gradient norm: 0.01845243657644846\n",
            "iter: 4694  x: [-99.99087667  24.99875591]  f(x): 8.478295197884592e-05  grad at x: [ 0.01824666 -0.00248818]  gradient norm: 0.0184155317033037\n",
            "iter: 4695  x: [-99.99089491  24.9987584 ]  f(x): 8.444415930281931e-05  grad at x: [ 0.01821017 -0.00248321]  gradient norm: 0.018378700639905893\n",
            "iter: 4696  x: [-99.99091312  24.99876088]  f(x): 8.410672044213572e-05  grad at x: [ 0.01817375 -0.00247824]  gradient norm: 0.01834194323861414\n",
            "iter: 4697  x: [-99.9909313   24.99876336]  f(x): 8.377062998721453e-05  grad at x: [ 0.0181374  -0.00247328]  gradient norm: 0.01830525935213315\n",
            "iter: 4698  x: [-99.99094944  24.99876583]  f(x): 8.343588254974076e-05  grad at x: [ 0.01810113 -0.00246834]  gradient norm: 0.01826864883342397\n",
            "iter: 4699  x: [-99.99096754  24.9987683 ]  f(x): 8.310247276313596e-05  grad at x: [ 0.01806493 -0.0024634 ]  gradient norm: 0.01823211153576414\n",
            "iter: 4700  x: [-99.9909856   24.99877076]  f(x): 8.277039528195435e-05  grad at x: [ 0.0180288  -0.00245847]  gradient norm: 0.0181956473126904\n",
            "iter: 4701  x: [-99.99100363  24.99877322]  f(x): 8.243964478229955e-05  grad at x: [ 0.01799274 -0.00245356]  gradient norm: 0.018159256018053113\n",
            "iter: 4702  x: [-99.99102162  24.99877568]  f(x): 8.211021596176868e-05  grad at x: [ 0.01795675 -0.00244865]  gradient norm: 0.018122937506019125\n",
            "iter: 4703  x: [-99.99103958  24.99877812]  f(x): 8.178210353884378e-05  grad at x: [ 0.01792084 -0.00244375]  gradient norm: 0.01808669163101354\n",
            "iter: 4704  x: [-99.9910575   24.99878057]  f(x): 8.145530225306964e-05  grad at x: [ 0.017885   -0.00243886]  gradient norm: 0.018050518247747862\n",
            "iter: 4705  x: [-99.99107539  24.99878301]  f(x): 8.112980686523736e-05  grad at x: [ 0.01784923 -0.00243399]  gradient norm: 0.018014417211249144\n",
            "iter: 4706  x: [-99.99109324  24.99878544]  f(x): 8.080561215705016e-05  grad at x: [ 0.01781353 -0.00242912]  gradient norm: 0.017978388376831796\n",
            "iter: 4707  x: [-99.99111105  24.99878787]  f(x): 8.048271293079261e-05  grad at x: [ 0.0177779  -0.00242426]  gradient norm: 0.01794243160006944\n",
            "iter: 4708  x: [-99.99112883  24.99879029]  f(x): 8.01611040099858e-05  grad at x: [ 0.01774235 -0.00241941]  gradient norm: 0.01790654673687652\n",
            "iter: 4709  x: [-99.99114657  24.99879271]  f(x): 7.984078023834108e-05  grad at x: [ 0.01770686 -0.00241457]  gradient norm: 0.01787073364340044\n",
            "iter: 4710  x: [-99.99116428  24.99879513]  f(x): 7.952173648040681e-05  grad at x: [ 0.01767145 -0.00240974]  gradient norm: 0.017834992176102217\n",
            "iter: 4711  x: [-99.99118195  24.99879754]  f(x): 7.920396762150576e-05  grad at x: [ 0.01763611 -0.00240492]  gradient norm: 0.0177993221917584\n",
            "iter: 4712  x: [-99.99119958  24.99879994]  f(x): 7.888746856690542e-05  grad at x: [ 0.01760083 -0.00240011]  gradient norm: 0.017763723547376594\n",
            "iter: 4713  x: [-99.99121718  24.99880234]  f(x): 7.85722342424866e-05  grad at x: [ 0.01756563 -0.00239531]  gradient norm: 0.017728196100278967\n",
            "iter: 4714  x: [-99.99123475  24.99880474]  f(x): 7.825825959441557e-05  grad at x: [ 0.0175305  -0.00239052]  gradient norm: 0.01769273970807411\n",
            "iter: 4715  x: [-99.99125228  24.99880713]  f(x): 7.794553958906786e-05  grad at x: [ 0.01749544 -0.00238574]  gradient norm: 0.017657354228657007\n",
            "iter: 4716  x: [-99.99126978  24.99880951]  f(x): 7.763406921296947e-05  grad at x: [ 0.01746045 -0.00238097]  gradient norm: 0.017622039520210988\n",
            "iter: 4717  x: [-99.99128724  24.9988119 ]  f(x): 7.73238434724481e-05  grad at x: [ 0.01742553 -0.00237621]  gradient norm: 0.017586795441176667\n",
            "iter: 4718  x: [-99.99130466  24.99881427]  f(x): 7.701485739382441e-05  grad at x: [ 0.01739068 -0.00237146]  gradient norm: 0.017551621850282032\n",
            "iter: 4719  x: [-99.99132205  24.99881664]  f(x): 7.670710602357511e-05  grad at x: [ 0.0173559  -0.00236671]  gradient norm: 0.01751651860656964\n",
            "iter: 4720  x: [-99.99133941  24.99881901]  f(x): 7.640058442801785e-05  grad at x: [ 0.01732118 -0.00236198]  gradient norm: 0.017481485569369425\n",
            "iter: 4721  x: [-99.99135673  24.99882137]  f(x): 7.609528769273694e-05  grad at x: [ 0.01728654 -0.00235726]  gradient norm: 0.017446522598241396\n",
            "iter: 4722  x: [-99.99137402  24.99882373]  f(x): 7.579121092301262e-05  grad at x: [ 0.01725197 -0.00235254]  gradient norm: 0.017411629553032954\n",
            "iter: 4723  x: [-99.99139127  24.99882608]  f(x): 7.548834924421957e-05  grad at x: [ 0.01721746 -0.00234784]  gradient norm: 0.01737680629393325\n",
            "iter: 4724  x: [-99.99140849  24.99882843]  f(x): 7.518669780053561e-05  grad at x: [ 0.01718303 -0.00234314]  gradient norm: 0.017342052681333386\n",
            "iter: 4725  x: [-99.99142567  24.99883077]  f(x): 7.488625175611895e-05  grad at x: [ 0.01714866 -0.00233845]  gradient norm: 0.017307368575970056\n",
            "iter: 4726  x: [-99.99144282  24.99883311]  f(x): 7.458700629400703e-05  grad at x: [ 0.01711437 -0.00233378]  gradient norm: 0.01727275383880718\n",
            "iter: 4727  x: [-99.99145993  24.99883545]  f(x): 7.428895661682649e-05  grad at x: [ 0.01708014 -0.00232911]  gradient norm: 0.01723820833112612\n",
            "iter: 4728  x: [-99.99147701  24.99883777]  f(x): 7.399209794619994e-05  grad at x: [ 0.01704598 -0.00232445]  gradient norm: 0.017203731914465527\n",
            "iter: 4729  x: [-99.99149406  24.9988401 ]  f(x): 7.369642552292605e-05  grad at x: [ 0.01701188 -0.0023198 ]  gradient norm: 0.017169324450650475\n",
            "iter: 4730  x: [-99.99151107  24.99884242]  f(x): 7.340193460643277e-05  grad at x: [ 0.01697786 -0.00231516]  gradient norm: 0.017134985801737074\n",
            "iter: 4731  x: [-99.99152805  24.99884473]  f(x): 7.310862047565666e-05  grad at x: [ 0.01694391 -0.00231053]  gradient norm: 0.017100715830123212\n",
            "iter: 4732  x: [-99.99154499  24.99884704]  f(x): 7.28164784282699e-05  grad at x: [ 0.01691002 -0.00230591]  gradient norm: 0.01706651439846695\n",
            "iter: 4733  x: [-99.9915619   24.99884935]  f(x): 7.252550378058648e-05  grad at x: [ 0.0168762 -0.0023013]  gradient norm: 0.017032381369683627\n",
            "iter: 4734  x: [-99.99157878  24.99885165]  f(x): 7.223569186750944e-05  grad at x: [ 0.01684244 -0.0022967 ]  gradient norm: 0.01699831660694781\n",
            "iter: 4735  x: [-99.99159562  24.99885395]  f(x): 7.194703804268466e-05  grad at x: [ 0.01680876 -0.0022921 ]  gradient norm: 0.01696431997371951\n",
            "iter: 4736  x: [-99.99161243  24.99885624]  f(x): 7.165953767869276e-05  grad at x: [ 0.01677514 -0.00228752]  gradient norm: 0.01693039133377522\n",
            "iter: 4737  x: [-99.9916292   24.99885853]  f(x): 7.137318616623772e-05  grad at x: [ 0.01674159 -0.00228294]  gradient norm: 0.016896530551120573\n",
            "iter: 4738  x: [-99.99164595  24.99886081]  f(x): 7.108797891432687e-05  grad at x: [ 0.01670811 -0.00227838]  gradient norm: 0.01686273749001945\n",
            "iter: 4739  x: [-99.99166265  24.99886309]  f(x): 7.080391135067688e-05  grad at x: [ 0.01667469 -0.00227382]  gradient norm: 0.016829012015050306\n",
            "iter: 4740  x: [-99.99167933  24.99886536]  f(x): 7.052097892094e-05  grad at x: [ 0.01664134 -0.00226927]  gradient norm: 0.016795353991022638\n",
            "iter: 4741  x: [-99.99169597  24.99886763]  f(x): 7.023917708910293e-05  grad at x: [ 0.01660806 -0.00226474]  gradient norm: 0.01676176328303236\n",
            "iter: 4742  x: [-99.99171258  24.9988699 ]  f(x): 6.995850133741721e-05  grad at x: [ 0.01657484 -0.00226021]  gradient norm: 0.01672823975646179\n",
            "iter: 4743  x: [-99.99172915  24.99887216]  f(x): 6.967894716608694e-05  grad at x: [ 0.01654169 -0.00225569]  gradient norm: 0.01669478327695055\n",
            "iter: 4744  x: [-99.99174569  24.99887441]  f(x): 6.940051009322599e-05  grad at x: [ 0.01650861 -0.00225117]  gradient norm: 0.016661393710398417\n",
            "iter: 4745  x: [-99.9917622   24.99887666]  f(x): 6.912318565500162e-05  grad at x: [ 0.01647559 -0.00224667]  gradient norm: 0.01662807092299063\n",
            "iter: 4746  x: [-99.99177868  24.99887891]  f(x): 6.884696940511442e-05  grad at x: [ 0.01644264 -0.00224218]  gradient norm: 0.016594814781143465\n",
            "iter: 4747  x: [-99.99179512  24.99888115]  f(x): 6.857185691541833e-05  grad at x: [ 0.01640976 -0.00223769]  gradient norm: 0.016561625151586826\n",
            "iter: 4748  x: [-99.99181153  24.99888339]  f(x): 6.829784377515984e-05  grad at x: [ 0.01637694 -0.00223322]  gradient norm: 0.01652850190128069\n",
            "iter: 4749  x: [-99.99182791  24.99888562]  f(x): 6.802492559138718e-05  grad at x: [ 0.01634418 -0.00222875]  gradient norm: 0.016495444897472414\n",
            "iter: 4750  x: [-99.99184425  24.99888785]  f(x): 6.77530979886349e-05  grad at x: [ 0.0163115  -0.00222429]  gradient norm: 0.016462454007666646\n",
            "iter: 4751  x: [-99.99186056  24.99889008]  f(x): 6.748235660911378e-05  grad at x: [ 0.01627887 -0.00221985]  gradient norm: 0.01642952909965636\n",
            "iter: 4752  x: [-99.99187684  24.9988923 ]  f(x): 6.721269711214223e-05  grad at x: [ 0.01624632 -0.00221541]  gradient norm: 0.01639667004146174\n",
            "iter: 4753  x: [-99.99189309  24.99889451]  f(x): 6.694411517459213e-05  grad at x: [ 0.01621382 -0.00221098]  gradient norm: 0.016363876701392263\n",
            "iter: 4754  x: [-99.9919093   24.99889672]  f(x): 6.66766064903226e-05  grad at x: [ 0.0161814  -0.00220655]  gradient norm: 0.016331148947985576\n",
            "iter: 4755  x: [-99.99192548  24.99889893]  f(x): 6.641016677082222e-05  grad at x: [ 0.01614903 -0.00220214]  gradient norm: 0.016298486650093893\n",
            "iter: 4756  x: [-99.99194163  24.99890113]  f(x): 6.614479174446895e-05  grad at x: [ 0.01611673 -0.00219774]  gradient norm: 0.016265889676801443\n",
            "iter: 4757  x: [-99.99195775  24.99890333]  f(x): 6.58804771566657e-05  grad at x: [ 0.0160845  -0.00219334]  gradient norm: 0.016233357897448782\n",
            "iter: 4758  x: [-99.99197383  24.99890552]  f(x): 6.561721877002826e-05  grad at x: [ 0.01605233 -0.00218895]  gradient norm: 0.016200891181663835\n",
            "iter: 4759  x: [-99.99198989  24.99890771]  f(x): 6.53550123638487e-05  grad at x: [ 0.01602023 -0.00218458]  gradient norm: 0.01616848939930366\n",
            "iter: 4760  x: [-99.99200591  24.9989099 ]  f(x): 6.509385373450431e-05  grad at x: [ 0.01598819 -0.00218021]  gradient norm: 0.01613615242051268\n",
            "iter: 4761  x: [-99.99202189  24.99891208]  f(x): 6.483373869492327e-05  grad at x: [ 0.01595621 -0.00217585]  gradient norm: 0.016103880115664458\n",
            "iter: 4762  x: [-99.99203785  24.99891425]  f(x): 6.457466307498445e-05  grad at x: [ 0.0159243 -0.0021715]  gradient norm: 0.016071672355418953\n",
            "iter: 4763  x: [-99.99205378  24.99891642]  f(x): 6.431662272123443e-05  grad at x: [ 0.01589245 -0.00216715]  gradient norm: 0.016039529010695348\n",
            "iter: 4764  x: [-99.99206967  24.99891859]  f(x): 6.405961349680954e-05  grad at x: [ 0.01586066 -0.00216282]  gradient norm: 0.016007449952670106\n",
            "iter: 4765  x: [-99.99208553  24.99892075]  f(x): 6.380363128138158e-05  grad at x: [ 0.01582894 -0.00215849]  gradient norm: 0.01597543505277795\n",
            "iter: 4766  x: [-99.99210136  24.99892291]  f(x): 6.354867197087878e-05  grad at x: [ 0.01579729 -0.00215418]  gradient norm: 0.015943484182684636\n",
            "iter: 4767  x: [-99.99211715  24.99892507]  f(x): 6.329473147762746e-05  grad at x: [ 0.01576569 -0.00214987]  gradient norm: 0.015911597214312265\n",
            "iter: 4768  x: [-99.99213292  24.99892722]  f(x): 6.304180573053699e-05  grad at x: [ 0.01573416 -0.00214557]  gradient norm: 0.015879774019870306\n",
            "iter: 4769  x: [-99.99214865  24.99892936]  f(x): 6.278989067479772e-05  grad at x: [ 0.01570269 -0.00214128]  gradient norm: 0.015848014471825513\n",
            "iter: 4770  x: [-99.99216436  24.9989315 ]  f(x): 6.253898227161244e-05  grad at x: [ 0.01567129 -0.00213699]  gradient norm: 0.015816318442875694\n",
            "iter: 4771  x: [-99.99218003  24.99893364]  f(x): 6.228907649834456e-05  grad at x: [ 0.01563994 -0.00213272]  gradient norm: 0.01578468580597594\n",
            "iter: 4772  x: [-99.99219567  24.99893577]  f(x): 6.204016934869446e-05  grad at x: [ 0.01560866 -0.00212845]  gradient norm: 0.01575311643436872\n",
            "iter: 4773  x: [-99.99221128  24.9989379 ]  f(x): 6.179225683194979e-05  grad at x: [ 0.01557745 -0.0021242 ]  gradient norm: 0.01572161020149651\n",
            "iter: 4774  x: [-99.99222685  24.99894003]  f(x): 6.154533497361508e-05  grad at x: [ 0.01554629 -0.00211995]  gradient norm: 0.015690166981089153\n",
            "iter: 4775  x: [-99.9922424   24.99894215]  f(x): 6.129939981512137e-05  grad at x: [ 0.0155152  -0.00211571]  gradient norm: 0.01565878664713475\n",
            "iter: 4776  x: [-99.99225792  24.99894426]  f(x): 6.105444741353857e-05  grad at x: [ 0.01548417 -0.00211148]  gradient norm: 0.015627469073850514\n",
            "iter: 4777  x: [-99.9922734   24.99894637]  f(x): 6.0810473841745154e-05  grad at x: [ 0.0154532  -0.00210725]  gradient norm: 0.01559621413571193\n",
            "iter: 4778  x: [-99.99228885  24.99894848]  f(x): 6.0567475188368585e-05  grad at x: [ 0.01542229 -0.00210304]  gradient norm: 0.015565021707452718\n",
            "iter: 4779  x: [-99.99230428  24.99895058]  f(x): 6.032544755750715e-05  grad at x: [ 0.01539145 -0.00209883]  gradient norm: 0.015533891664036691\n",
            "iter: 4780  x: [-99.99231967  24.99895268]  f(x): 6.00843870691022e-05  grad at x: [ 0.01536067 -0.00209464]  gradient norm: 0.015502823880713114\n",
            "iter: 4781  x: [-99.99233503  24.99895478]  f(x): 5.9844289858456165e-05  grad at x: [ 0.01532994 -0.00209045]  gradient norm: 0.0154718182329623\n",
            "iter: 4782  x: [-99.99235036  24.99895687]  f(x): 5.960515207615363e-05  grad at x: [ 0.01529928 -0.00208627]  gradient norm: 0.015440874596492729\n",
            "iter: 4783  x: [-99.99236566  24.99895895]  f(x): 5.9366969888461266e-05  grad at x: [ 0.01526869 -0.00208209]  gradient norm: 0.015409992847300257\n",
            "iter: 4784  x: [-99.99238093  24.99896104]  f(x): 5.912973947681932e-05  grad at x: [ 0.01523815 -0.00207793]  gradient norm: 0.015379172861609862\n",
            "iter: 4785  x: [-99.99239616  24.99896311]  f(x): 5.8893457037800394e-05  grad at x: [ 0.01520767 -0.00207377]  gradient norm: 0.01534841451587758\n",
            "iter: 4786  x: [-99.99241137  24.99896519]  f(x): 5.8658118783462815e-05  grad at x: [ 0.01517726 -0.00206963]  gradient norm: 0.015317717686843927\n",
            "iter: 4787  x: [-99.99242655  24.99896726]  f(x): 5.842372094088235e-05  grad at x: [ 0.0151469  -0.00206549]  gradient norm: 0.015287082251480478\n",
            "iter: 4788  x: [-99.9924417   24.99896932]  f(x): 5.8190259752082016e-05  grad at x: [ 0.01511661 -0.00206136]  gradient norm: 0.01525650808698793\n",
            "iter: 4789  x: [-99.99245681  24.99897138]  f(x): 5.795773147419125e-05  grad at x: [ 0.01508638 -0.00205723]  gradient norm: 0.015225995070824271\n",
            "iter: 4790  x: [-99.9924719   24.99897344]  f(x): 5.7726132379189387e-05  grad at x: [ 0.0150562  -0.00205312]  gradient norm: 0.015195543080678543\n",
            "iter: 4791  x: [-99.99248695  24.99897549]  f(x): 5.749545875426317e-05  grad at x: [ 0.01502609 -0.00204901]  gradient norm: 0.015165151994525235\n",
            "iter: 4792  x: [-99.99250198  24.99897754]  f(x): 5.726570690111577e-05  grad at x: [ 0.01499604 -0.00204491]  gradient norm: 0.01513482169054076\n",
            "iter: 4793  x: [-99.99251698  24.99897959]  f(x): 5.703687313633975e-05  grad at x: [ 0.01496605 -0.00204082]  gradient norm: 0.01510455204715979\n",
            "iter: 4794  x: [-99.99253194  24.99898163]  f(x): 5.680895379134609e-05  grad at x: [ 0.01493611 -0.00203674]  gradient norm: 0.015074342943073318\n",
            "iter: 4795  x: [-99.99254688  24.99898367]  f(x): 5.658194521190605e-05  grad at x: [ 0.01490624 -0.00203267]  gradient norm: 0.015044194257175231\n",
            "iter: 4796  x: [-99.99256179  24.9989857 ]  f(x): 5.635584375893046e-05  grad at x: [ 0.01487643 -0.0020286 ]  gradient norm: 0.015014105868673027\n",
            "iter: 4797  x: [-99.99257666  24.99898773]  f(x): 5.6130645807369014e-05  grad at x: [ 0.01484668 -0.00202455]  gradient norm: 0.014984077656948928\n",
            "iter: 4798  x: [-99.99259151  24.99898975]  f(x): 5.5906347746763876e-05  grad at x: [ 0.01481698 -0.0020205 ]  gradient norm: 0.014954109501640528\n",
            "iter: 4799  x: [-99.99260633  24.99899177]  f(x): 5.5682945981230056e-05  grad at x: [ 0.01478735 -0.00201646]  gradient norm: 0.014924201282645589\n",
            "iter: 4800  x: [-99.99262111  24.99899379]  f(x): 5.5460436929161655e-05  grad at x: [ 0.01475777 -0.00201242]  gradient norm: 0.014894352880090046\n",
            "iter: 4801  x: [-99.99263587  24.9989958 ]  f(x): 5.523881702319312e-05  grad at x: [ 0.01472826 -0.0020084 ]  gradient norm: 0.014864564174329919\n",
            "iter: 4802  x: [-99.9926506   24.99899781]  f(x): 5.5018082710355235e-05  grad at x: [ 0.0146988  -0.00200438]  gradient norm: 0.014834835045979478\n",
            "iter: 4803  x: [-99.9926653   24.99899981]  f(x): 5.479823045180472e-05  grad at x: [ 0.0146694  -0.00200037]  gradient norm: 0.014805165375882124\n",
            "iter: 4804  x: [-99.99267997  24.99900181]  f(x): 5.457925672298689e-05  grad at x: [ 0.01464007 -0.00199637]  gradient norm: 0.014775555045139508\n",
            "iter: 4805  x: [-99.99269461  24.99900381]  f(x): 5.4361158013158835e-05  grad at x: [ 0.01461079 -0.00199238]  gradient norm: 0.014746003935054246\n",
            "iter: 4806  x: [-99.99270922  24.9990058 ]  f(x): 5.414393082575379e-05  grad at x: [ 0.01458156 -0.0019884 ]  gradient norm: 0.01471651192718625\n",
            "iter: 4807  x: [-99.9927238   24.99900779]  f(x): 5.392757167814143e-05  grad at x: [ 0.0145524  -0.00198442]  gradient norm: 0.014687078903327431\n",
            "iter: 4808  x: [-99.99273835  24.99900978]  f(x): 5.3712077101747116e-05  grad at x: [ 0.0145233  -0.00198045]  gradient norm: 0.01465770474552508\n",
            "iter: 4809  x: [-99.99275288  24.99901176]  f(x): 5.349744364160742e-05  grad at x: [ 0.01449425 -0.00197649]  gradient norm: 0.014628389336028409\n",
            "iter: 4810  x: [-99.99276737  24.99901373]  f(x): 5.328366785673183e-05  grad at x: [ 0.01446526 -0.00197254]  gradient norm: 0.014599132557344883\n",
            "iter: 4811  x: [-99.99278183  24.9990157 ]  f(x): 5.307074632004234e-05  grad at x: [ 0.01443633 -0.00196859]  gradient norm: 0.014569934292239253\n",
            "iter: 4812  x: [-99.99279627  24.99901767]  f(x): 5.285867561771307e-05  grad at x: [ 0.01440746 -0.00196465]  gradient norm: 0.014540794423650047\n",
            "iter: 4813  x: [-99.99281068  24.99901964]  f(x): 5.2647452349933694e-05  grad at x: [ 0.01437864 -0.00196072]  gradient norm: 0.014511712834801231\n",
            "iter: 4814  x: [-99.99282506  24.9990216 ]  f(x): 5.243707313024284e-05  grad at x: [ 0.01434989 -0.0019568 ]  gradient norm: 0.014482689409117747\n",
            "iter: 4815  x: [-99.99283941  24.99902356]  f(x): 5.2227534586103954e-05  grad at x: [ 0.01432119 -0.00195289]  gradient norm: 0.014453724030311905\n",
            "iter: 4816  x: [-99.99285373  24.99902551]  f(x): 5.201883335781438e-05  grad at x: [ 0.01429254 -0.00194898]  gradient norm: 0.014424816582239703\n",
            "iter: 4817  x: [-99.99286802  24.99902746]  f(x): 5.1810966099684336e-05  grad at x: [ 0.01426396 -0.00194509]  gradient norm: 0.014395966949070749\n",
            "iter: 4818  x: [-99.99288228  24.9990294 ]  f(x): 5.160392947918541e-05  grad at x: [ 0.01423543 -0.0019412 ]  gradient norm: 0.014367175015177536\n",
            "iter: 4819  x: [-99.99289652  24.99903134]  f(x): 5.139772017688874e-05  grad at x: [ 0.01420696 -0.00193731]  gradient norm: 0.014338440665133534\n",
            "iter: 4820  x: [-99.99291073  24.99903328]  f(x): 5.1192334887021726e-05  grad at x: [ 0.01417855 -0.00193344]  gradient norm: 0.014309763783797653\n",
            "iter: 4821  x: [-99.99292491  24.99903521]  f(x): 5.098777031681119e-05  grad at x: [ 0.01415019 -0.00192957]  gradient norm: 0.014281144256229778\n",
            "iter: 4822  x: [-99.99293906  24.99903714]  f(x): 5.078402318664347e-05  grad at x: [ 0.01412189 -0.00192571]  gradient norm: 0.01425258196771988\n",
            "iter: 4823  x: [-99.99295318  24.99903907]  f(x): 5.058109023000145e-05  grad at x: [ 0.01409364 -0.00192186]  gradient norm: 0.0142240768037861\n",
            "iter: 4824  x: [-99.99296727  24.99904099]  f(x): 5.0378968193442795e-05  grad at x: [ 0.01406546 -0.00191802]  gradient norm: 0.014195628650178588\n",
            "iter: 4825  x: [-99.99298134  24.99904291]  f(x): 5.017765383651679e-05  grad at x: [ 0.01403733 -0.00191418]  gradient norm: 0.014167237392874702\n",
            "iter: 4826  x: [-99.99299537  24.99904482]  f(x): 4.997714393174292e-05  grad at x: [ 0.01400925 -0.00191035]  gradient norm: 0.014138902918082848\n",
            "iter: 4827  x: [-99.99300938  24.99904673]  f(x): 4.977743526454173e-05  grad at x: [ 0.01398123 -0.00190653]  gradient norm: 0.014110625112239603\n",
            "iter: 4828  x: [-99.99302336  24.99904864]  f(x): 4.957852463320004e-05  grad at x: [ 0.01395327 -0.00190272]  gradient norm: 0.014082403862011632\n",
            "iter: 4829  x: [-99.99303732  24.99905054]  f(x): 4.938040884881582e-05  grad at x: [ 0.01392536 -0.00189891]  gradient norm: 0.014054239054294732\n",
            "iter: 4830  x: [-99.99305124  24.99905244]  f(x): 4.9183084735052545e-05  grad at x: [ 0.01389751 -0.00189512]  gradient norm: 0.014026130576185656\n",
            "iter: 4831  x: [-99.99306514  24.99905434]  f(x): 4.898654912849423e-05  grad at x: [ 0.01386972 -0.00189133]  gradient norm: 0.013998078315039422\n",
            "iter: 4832  x: [-99.99307901  24.99905623]  f(x): 4.8790798878195386e-05  grad at x: [ 0.01384198 -0.00188754]  gradient norm: 0.01397008215841201\n",
            "iter: 4833  x: [-99.99309285  24.99905812]  f(x): 4.859583084583837e-05  grad at x: [ 0.01381429 -0.00188377]  gradient norm: 0.013942141994089483\n",
            "iter: 4834  x: [-99.99310667  24.99906   ]  f(x): 4.840164190586841e-05  grad at x: [ 0.01378667 -0.00188   ]  gradient norm: 0.013914257710114243\n",
            "iter: 4835  x: [-99.99312045  24.99906188]  f(x): 4.820822894487116e-05  grad at x: [ 0.01375909 -0.00187624]  gradient norm: 0.013886429194702454\n",
            "iter: 4836  x: [-99.99313421  24.99906376]  f(x): 4.8015588861912926e-05  grad at x: [ 0.01373157 -0.00187249]  gradient norm: 0.013858656336299407\n",
            "iter: 4837  x: [-99.99314794  24.99906563]  f(x): 4.7823718568883255e-05  grad at x: [ 0.01370411 -0.00186874]  gradient norm: 0.01383093902363585\n",
            "iter: 4838  x: [-99.99316165  24.9990675 ]  f(x): 4.7632614989472285e-05  grad at x: [ 0.0136767  -0.00186501]  gradient norm: 0.013803277145587172\n",
            "iter: 4839  x: [-99.99317532  24.99906936]  f(x): 4.744227505991907e-05  grad at x: [ 0.01364935 -0.00186128]  gradient norm: 0.013775670591287972\n",
            "iter: 4840  x: [-99.99318897  24.99907122]  f(x): 4.725269572874296e-05  grad at x: [ 0.01362205 -0.00185755]  gradient norm: 0.01374811925010006\n",
            "iter: 4841  x: [-99.9932026   24.99907308]  f(x): 4.7063873956530456e-05  grad at x: [ 0.01359481 -0.00185384]  gradient norm: 0.013720623011588133\n",
            "iter: 4842  x: [-99.99321619  24.99907494]  f(x): 4.687580671625671e-05  grad at x: [ 0.01356762 -0.00185013]  gradient norm: 0.013693181765573216\n",
            "iter: 4843  x: [-99.99322976  24.99907679]  f(x): 4.668849099267345e-05  grad at x: [ 0.01354048 -0.00184643]  gradient norm: 0.013665795402050105\n",
            "iter: 4844  x: [-99.9932433   24.99907863]  f(x): 4.650192378263778e-05  grad at x: [ 0.0135134  -0.00184274]  gradient norm: 0.013638463811241761\n",
            "iter: 4845  x: [-99.99325681  24.99908047]  f(x): 4.631610209527775e-05  grad at x: [ 0.01348637 -0.00183905]  gradient norm: 0.013611186883630354\n",
            "iter: 4846  x: [-99.9932703   24.99908231]  f(x): 4.6131022951344766e-05  grad at x: [ 0.0134594  -0.00183537]  gradient norm: 0.013583964509868946\n",
            "iter: 4847  x: [-99.99328376  24.99908415]  f(x): 4.594668338357992e-05  grad at x: [ 0.01343248 -0.0018317 ]  gradient norm: 0.013556796580841643\n",
            "iter: 4848  x: [-99.99329719  24.99908598]  f(x): 4.5763080436839496e-05  grad at x: [ 0.01340562 -0.00182804]  gradient norm: 0.013529682987688883\n",
            "iter: 4849  x: [-99.9933106   24.99908781]  f(x): 4.558021116749061e-05  grad at x: [ 0.01337881 -0.00182438]  gradient norm: 0.01350262362172487\n",
            "iter: 4850  x: [-99.99332398  24.99908963]  f(x): 4.539807264373645e-05  grad at x: [ 0.01335205 -0.00182073]  gradient norm: 0.013475618374491977\n",
            "iter: 4851  x: [-99.99333733  24.99909145]  f(x): 4.521666194539501e-05  grad at x: [ 0.01332535 -0.00181709]  gradient norm: 0.013448667137734507\n",
            "iter: 4852  x: [-99.99335065  24.99909327]  f(x): 4.503597616422772e-05  grad at x: [ 0.01329869 -0.00181346]  gradient norm: 0.013421769803454046\n",
            "iter: 4853  x: [-99.99336395  24.99909508]  f(x): 4.485601240351579e-05  grad at x: [ 0.0132721  -0.00180983]  gradient norm: 0.01339492626385316\n",
            "iter: 4854  x: [-99.99337722  24.99909689]  f(x): 4.467676777802402e-05  grad at x: [ 0.01324555 -0.00180621]  gradient norm: 0.013368136411336327\n",
            "iter: 4855  x: [-99.99339047  24.9990987 ]  f(x): 4.449823941393923e-05  grad at x: [ 0.01321906 -0.0018026 ]  gradient norm: 0.013341400138507088\n",
            "iter: 4856  x: [-99.99340369  24.9991005 ]  f(x): 4.4320424449228534e-05  grad at x: [ 0.01319262 -0.00179899]  gradient norm: 0.013314717338228182\n",
            "iter: 4857  x: [-99.99341688  24.9991023 ]  f(x): 4.4143320033200096e-05  grad at x: [ 0.01316624 -0.0017954 ]  gradient norm: 0.013288087903562363\n",
            "iter: 4858  x: [-99.99343005  24.9991041 ]  f(x): 4.3966923326287165e-05  grad at x: [ 0.01313991 -0.00179181]  gradient norm: 0.01326151172774615\n",
            "iter: 4859  x: [-99.99344319  24.99910589]  f(x): 4.37912315007408e-05  grad at x: [ 0.01311363 -0.00178822]  gradient norm: 0.013234988704300552\n",
            "iter: 4860  x: [-99.9934563   24.99910768]  f(x): 4.361624173966537e-05  grad at x: [ 0.0130874  -0.00178465]  gradient norm: 0.013208518726892183\n",
            "iter: 4861  x: [-99.99346939  24.99910946]  f(x): 4.344195123771052e-05  grad at x: [ 0.01306122 -0.00178108]  gradient norm: 0.01318210168944399\n",
            "iter: 4862  x: [-99.99348245  24.99911124]  f(x): 4.3268357200482944e-05  grad at x: [ 0.0130351  -0.00177751]  gradient norm: 0.013155737486052683\n",
            "iter: 4863  x: [-99.99349548  24.99911302]  f(x): 4.309545684505525e-05  grad at x: [ 0.01300903 -0.00177396]  gradient norm: 0.013129426011072266\n",
            "iter: 4864  x: [-99.99350849  24.99911479]  f(x): 4.2923247399545805e-05  grad at x: [ 0.01298301 -0.00177041]  gradient norm: 0.013103167159056746\n",
            "iter: 4865  x: [-99.99352148  24.99911656]  f(x): 4.2751726102906274e-05  grad at x: [ 0.01295705 -0.00176687]  gradient norm: 0.0130769608247339\n",
            "iter: 4866  x: [-99.99353443  24.99911833]  f(x): 4.2580890205427516e-05  grad at x: [ 0.01293113 -0.00176334]  gradient norm: 0.013050806903088791\n",
            "iter: 4867  x: [-99.99354736  24.9991201 ]  f(x): 4.241073696813878e-05  grad at x: [ 0.01290527 -0.00175981]  gradient norm: 0.013024705289278338\n",
            "iter: 4868  x: [-99.99356027  24.99912185]  f(x): 4.224126366314752e-05  grad at x: [ 0.01287946 -0.00175629]  gradient norm: 0.012998655878689537\n",
            "iter: 4869  x: [-99.99357315  24.99912361]  f(x): 4.2072467573590855e-05  grad at x: [ 0.0128537  -0.00175278]  gradient norm: 0.012972658566938522\n",
            "iter: 4870  x: [-99.993586    24.99912536]  f(x): 4.1904345993228816e-05  grad at x: [ 0.01282799 -0.00174927]  gradient norm: 0.012946713249814227\n",
            "iter: 4871  x: [-99.99359883  24.99912711]  f(x): 4.173689622658173e-05  grad at x: [ 0.01280234 -0.00174577]  gradient norm: 0.012920819823305599\n",
            "iter: 4872  x: [-99.99361163  24.99912886]  f(x): 4.157011558925956e-05  grad at x: [ 0.01277673 -0.00174228]  gradient norm: 0.01289497818365887\n",
            "iter: 4873  x: [-99.99362441  24.9991306 ]  f(x): 4.140400140738091e-05  grad at x: [ 0.01275118 -0.0017388 ]  gradient norm: 0.012869188227294045\n",
            "iter: 4874  x: [-99.99363716  24.99913234]  f(x): 4.123855101770972e-05  grad at x: [ 0.01272568 -0.00173532]  gradient norm: 0.012843449850832092\n",
            "iter: 4875  x: [-99.99364989  24.99913408]  f(x): 4.107376176778988e-05  grad at x: [ 0.01270023 -0.00173185]  gradient norm: 0.012817762951122146\n",
            "iter: 4876  x: [-99.99366259  24.99913581]  f(x): 4.090963101573006e-05  grad at x: [ 0.01267483 -0.00172839]  gradient norm: 0.012792127425214316\n",
            "iter: 4877  x: [-99.99367526  24.99913754]  f(x): 4.074615613016432e-05  grad at x: [ 0.01264948 -0.00172493]  gradient norm: 0.012766543170359676\n",
            "iter: 4878  x: [-99.99368791  24.99913926]  f(x): 4.0583334490212805e-05  grad at x: [ 0.01262418 -0.00172148]  gradient norm: 0.012741010084010263\n",
            "iter: 4879  x: [-99.99370054  24.99914098]  f(x): 4.0421163485627865e-05  grad at x: [ 0.01259893 -0.00171804]  gradient norm: 0.012715528063848212\n",
            "iter: 4880  x: [-99.99371313  24.9991427 ]  f(x): 4.02596405163775e-05  grad at x: [ 0.01257373 -0.0017146 ]  gradient norm: 0.012690097007726537\n",
            "iter: 4881  x: [-99.99372571  24.99914441]  f(x): 4.009876299280503e-05  grad at x: [ 0.01254858 -0.00171117]  gradient norm: 0.012664716813700183\n",
            "iter: 4882  x: [-99.99373826  24.99914613]  f(x): 3.9938528335940145e-05  grad at x: [ 0.01252349 -0.00170775]  gradient norm: 0.012639387380081385\n",
            "iter: 4883  x: [-99.99375078  24.99914783]  f(x): 3.977893397674631e-05  grad at x: [ 0.01249844 -0.00170433]  gradient norm: 0.012614108605327022\n",
            "iter: 4884  x: [-99.99376328  24.99914954]  f(x): 3.961997735660554e-05  grad at x: [ 0.01247344 -0.00170092]  gradient norm: 0.012588880388121183\n",
            "iter: 4885  x: [-99.99377575  24.99915124]  f(x): 3.946165592712564e-05  grad at x: [ 0.0124485  -0.00169752]  gradient norm: 0.012563702627350846\n",
            "iter: 4886  x: [-99.9937882   24.99915294]  f(x): 3.930396715008378e-05  grad at x: [ 0.0124236  -0.00169413]  gradient norm: 0.012538575222102994\n",
            "iter: 4887  x: [-99.99380062  24.99915463]  f(x): 3.914690849739454e-05  grad at x: [ 0.01239875 -0.00169074]  gradient norm: 0.01251349807166558\n",
            "iter: 4888  x: [-99.99381302  24.99915632]  f(x): 3.899047745107189e-05  grad at x: [ 0.01237395 -0.00168736]  gradient norm: 0.012488471075527523\n",
            "iter: 4889  x: [-99.9938254   24.99915801]  f(x): 3.8834671503185394e-05  grad at x: [ 0.01234921 -0.00168398]  gradient norm: 0.01246349413337775\n",
            "iter: 4890  x: [-99.99383775  24.99915969]  f(x): 3.8679488155834555e-05  grad at x: [ 0.01232451 -0.00168061]  gradient norm: 0.012438567145107117\n",
            "iter: 4891  x: [-99.99385007  24.99916137]  f(x): 3.85249249210992e-05  grad at x: [ 0.01229986 -0.00167725]  gradient norm: 0.01241369001080649\n",
            "iter: 4892  x: [-99.99386237  24.99916305]  f(x): 3.83709793211825e-05  grad at x: [ 0.01227526 -0.0016739 ]  gradient norm: 0.012388862630795855\n",
            "iter: 4893  x: [-99.99387465  24.99916472]  f(x): 3.8217648887849625e-05  grad at x: [ 0.01225071 -0.00167055]  gradient norm: 0.012364084905539856\n",
            "iter: 4894  x: [-99.9938869   24.99916639]  f(x): 3.806493116290931e-05  grad at x: [ 0.01222621 -0.00166721]  gradient norm: 0.012339356735731293\n",
            "iter: 4895  x: [-99.99389912  24.99916806]  f(x): 3.791282369801357e-05  grad at x: [ 0.01220175 -0.00166388]  gradient norm: 0.012314678022264906\n",
            "iter: 4896  x: [-99.99391132  24.99916973]  f(x): 3.7761324054435785e-05  grad at x: [ 0.01217735 -0.00166055]  gradient norm: 0.012290048666207271\n",
            "iter: 4897  x: [-99.9939235   24.99917139]  f(x): 3.761042980355379e-05  grad at x: [ 0.012153   -0.00165723]  gradient norm: 0.012265468568881303\n",
            "iter: 4898  x: [-99.99393565  24.99917304]  f(x): 3.746013852613796e-05  grad at x: [ 0.01212869 -0.00165391]  gradient norm: 0.012240937631756476\n",
            "iter: 4899  x: [-99.99394778  24.9991747 ]  f(x): 3.731044781263265e-05  grad at x: [ 0.01210443 -0.0016506 ]  gradient norm: 0.012216455756500353\n",
            "iter: 4900  x: [-99.99395989  24.99917635]  f(x): 3.71613552631492e-05  grad at x: [ 0.01208022 -0.0016473 ]  gradient norm: 0.012192022844983387\n",
            "iter: 4901  x: [-99.99397197  24.999178  ]  f(x): 3.701285848758324e-05  grad at x: [ 0.01205606 -0.00164401]  gradient norm: 0.0121676387993042\n",
            "iter: 4902  x: [-99.99398402  24.99917964]  f(x): 3.6864955105063836e-05  grad at x: [ 0.01203195 -0.00164072]  gradient norm: 0.012143303521705093\n",
            "iter: 4903  x: [-99.99399606  24.99918128]  f(x): 3.6717642744444515e-05  grad at x: [ 0.01200789 -0.00163744]  gradient norm: 0.012119016914658469\n",
            "iter: 4904  x: [-99.99400806  24.99918292]  f(x): 3.6570919044083516e-05  grad at x: [ 0.01198387 -0.00163416]  gradient norm: 0.012094778880836725\n",
            "iter: 4905  x: [-99.99402005  24.99918455]  f(x): 3.642478165163804e-05  grad at x: [ 0.0119599 -0.0016309]  gradient norm: 0.012070589323084111\n",
            "iter: 4906  x: [-99.99403201  24.99918618]  f(x): 3.627922822421148e-05  grad at x: [ 0.01193598 -0.00162763]  gradient norm: 0.012046448144446806\n",
            "iter: 4907  x: [-99.99404394  24.99918781]  f(x): 3.613425642830039e-05  grad at x: [ 0.01191211 -0.00162438]  gradient norm: 0.012022355248170033\n",
            "iter: 4908  x: [-99.99405586  24.99918943]  f(x): 3.598986393961322e-05  grad at x: [ 0.01188829 -0.00162113]  gradient norm: 0.011998310537673747\n",
            "iter: 4909  x: [-99.99406774  24.99919106]  f(x): 3.5846048443350665e-05  grad at x: [ 0.01186451 -0.00161789]  gradient norm: 0.011974313916605103\n",
            "iter: 4910  x: [-99.99407961  24.99919267]  f(x): 3.570280763384381e-05  grad at x: [ 0.01184078 -0.00161465]  gradient norm: 0.011950365288784074\n",
            "iter: 4911  x: [-99.99409145  24.99919429]  f(x): 3.556013921452063e-05  grad at x: [ 0.0118171  -0.00161142]  gradient norm: 0.011926464558203429\n",
            "iter: 4912  x: [-99.99410327  24.9991959 ]  f(x): 3.54180408982078e-05  grad at x: [ 0.01179347 -0.0016082 ]  gradient norm: 0.011902611629085073\n",
            "iter: 4913  x: [-99.99411506  24.99919751]  f(x): 3.527651040674822e-05  grad at x: [ 0.01176988 -0.00160498]  gradient norm: 0.011878806405821793\n",
            "iter: 4914  x: [-99.99412683  24.99919911]  f(x): 3.513554547115194e-05  grad at x: [ 0.01174634 -0.00160177]  gradient norm: 0.011855048793008308\n",
            "iter: 4915  x: [-99.99413858  24.99920071]  f(x): 3.49951438313835e-05  grad at x: [ 0.01172285 -0.00159857]  gradient norm: 0.011831338695411184\n",
            "iter: 4916  x: [-99.9941503   24.99920231]  f(x): 3.485530323667289e-05  grad at x: [ 0.0116994  -0.00159537]  gradient norm: 0.011807676018027069\n",
            "iter: 4917  x: [-99.994162    24.99920391]  f(x): 3.471602144496464e-05  grad at x: [ 0.011676   -0.00159218]  gradient norm: 0.011784060665995341\n",
            "iter: 4918  x: [-99.99417367  24.9992055 ]  f(x): 3.457729622322939e-05  grad at x: [ 0.01165265 -0.001589  ]  gradient norm: 0.011760492544656349\n",
            "iter: 4919  x: [-99.99418533  24.99920709]  f(x): 3.443912534759489e-05  grad at x: [ 0.01162935 -0.00158582]  gradient norm: 0.011736971559579566\n",
            "iter: 4920  x: [-99.99419696  24.99920868]  f(x): 3.4301506602644894e-05  grad at x: [ 0.01160609 -0.00158265]  gradient norm: 0.01171349761644999\n",
            "iter: 4921  x: [-99.99420856  24.99921026]  f(x): 3.416443778221877e-05  grad at x: [ 0.01158287 -0.00157948]  gradient norm: 0.01169007062120991\n",
            "iter: 4922  x: [-99.99422015  24.99921184]  f(x): 3.4027916688881464e-05  grad at x: [ 0.01155971 -0.00157632]  gradient norm: 0.011666690479974423\n",
            "iter: 4923  x: [-99.99423171  24.99921341]  f(x): 3.389194113372189e-05  grad at x: [ 0.01153659 -0.00157317]  gradient norm: 0.011643357099002313\n",
            "iter: 4924  x: [-99.99424324  24.99921499]  f(x): 3.3756508936976973e-05  grad at x: [ 0.01151352 -0.00157002]  gradient norm: 0.011620070384808686\n",
            "iter: 4925  x: [-99.99425476  24.99921656]  f(x): 3.362161792718961e-05  grad at x: [ 0.01149049 -0.00156688]  gradient norm: 0.011596830244026099\n",
            "iter: 4926  x: [-99.99426625  24.99921812]  f(x): 3.348726594198376e-05  grad at x: [ 0.01146751 -0.00156375]  gradient norm: 0.01157363658354344\n",
            "iter: 4927  x: [-99.99427771  24.99921969]  f(x): 3.335345082720916e-05  grad at x: [ 0.01144457 -0.00156062]  gradient norm: 0.011550489310364157\n",
            "iter: 4928  x: [-99.99428916  24.99922125]  f(x): 3.3220170437746744e-05  grad at x: [ 0.01142168 -0.0015575 ]  gradient norm: 0.011527388331750908\n",
            "iter: 4929  x: [-99.99430058  24.99922281]  f(x): 3.308742263664021e-05  grad at x: [ 0.01139884 -0.00155439]  gradient norm: 0.01150433355508092\n",
            "iter: 4930  x: [-99.99431198  24.99922436]  f(x): 3.2955205295725575e-05  grad at x: [ 0.01137604 -0.00155128]  gradient norm: 0.011481324887960548\n",
            "iter: 4931  x: [-99.99432335  24.99922591]  f(x): 3.2823516295429865e-05  grad at x: [ 0.01135329 -0.00154818]  gradient norm: 0.011458362238196148\n",
            "iter: 4932  x: [-99.99433471  24.99922746]  f(x): 3.26923535242608e-05  grad at x: [ 0.01133058 -0.00154508]  gradient norm: 0.01143544551371057\n",
            "iter: 4933  x: [-99.99434604  24.99922901]  f(x): 3.256171487958241e-05  grad at x: [ 0.01130792 -0.00154199]  gradient norm: 0.011412574622683948\n",
            "iter: 4934  x: [-99.99435735  24.99923055]  f(x): 3.2431598266932266e-05  grad at x: [ 0.01128531 -0.00153891]  gradient norm: 0.0113897494734401\n",
            "iter: 4935  x: [-99.99436863  24.99923209]  f(x): 3.2302001600306927e-05  grad at x: [ 0.01126274 -0.00153583]  gradient norm: 0.011366969974501899\n",
            "iter: 4936  x: [-99.99437989  24.99923362]  f(x): 3.2172922801832015e-05  grad at x: [ 0.01124021 -0.00153276]  gradient norm: 0.011344236034538777\n",
            "iter: 4937  x: [-99.99439113  24.99923515]  f(x): 3.204435980234895e-05  grad at x: [ 0.01121773 -0.00152969]  gradient norm: 0.011321547562475538\n",
            "iter: 4938  x: [-99.99440235  24.99923668]  f(x): 3.191631054059506e-05  grad at x: [ 0.0111953  -0.00152663]  gradient norm: 0.011298904467353472\n",
            "iter: 4939  x: [-99.99441355  24.99923821]  f(x): 3.178877296365268e-05  grad at x: [ 0.0111729  -0.00152358]  gradient norm: 0.011276306658414835\n",
            "iter: 4940  x: [-99.99442472  24.99923973]  f(x): 3.166174502690638e-05  grad at x: [ 0.01115056 -0.00152053]  gradient norm: 0.01125375404510093\n",
            "iter: 4941  x: [-99.99443587  24.99924126]  f(x): 3.1535224693705735e-05  grad at x: [ 0.01112826 -0.00151749]  gradient norm: 0.011231246536997705\n",
            "iter: 4942  x: [-99.994447    24.99924277]  f(x): 3.140920993580488e-05  grad at x: [ 0.011106   -0.00151445]  gradient norm: 0.011208784043919284\n",
            "iter: 4943  x: [-99.99445811  24.99924429]  f(x): 3.128369873286752e-05  grad at x: [ 0.01108379 -0.00151143]  gradient norm: 0.011186366475825386\n",
            "iter: 4944  x: [-99.99446919  24.9992458 ]  f(x): 3.1158689072736834e-05  grad at x: [ 0.01106162 -0.0015084 ]  gradient norm: 0.011163993742874785\n",
            "iter: 4945  x: [-99.99448025  24.99924731]  f(x): 3.103417895126336e-05  grad at x: [ 0.0110395  -0.00150539]  gradient norm: 0.011141665755400017\n",
            "iter: 4946  x: [-99.99449129  24.99924881]  f(x): 3.091016637209724e-05  grad at x: [ 0.01101742 -0.00150238]  gradient norm: 0.01111938242387539\n",
            "iter: 4947  x: [-99.99450231  24.99925031]  f(x): 3.078664934730662e-05  grad at x: [ 0.01099538 -0.00149937]  gradient norm: 0.011097143659033457\n",
            "iter: 4948  x: [-99.9945133   24.99925181]  f(x): 3.0663625896553005e-05  grad at x: [ 0.01097339 -0.00149637]  gradient norm: 0.011074949371722292\n",
            "iter: 4949  x: [-99.99452428  24.99925331]  f(x): 3.0541094047537246e-05  grad at x: [ 0.01095145 -0.00149338]  gradient norm: 0.011052799472990949\n",
            "iter: 4950  x: [-99.99453523  24.9992548 ]  f(x): 3.0419051835647314e-05  grad at x: [ 0.01092954 -0.00149039]  gradient norm: 0.011030693874031192\n",
            "iter: 4951  x: [-99.99454616  24.99925629]  f(x): 3.0297497304566652e-05  grad at x: [ 0.01090769 -0.00148741]  gradient norm: 0.011008632486293047\n",
            "iter: 4952  x: [-99.99455707  24.99925778]  f(x): 3.0176428505296273e-05  grad at x: [ 0.01088587 -0.00148444]  gradient norm: 0.010986615221312937\n",
            "iter: 4953  x: [-99.99456795  24.99925927]  f(x): 3.0055843496918193e-05  grad at x: [ 0.0108641  -0.00148147]  gradient norm: 0.010964641990857374\n",
            "iter: 4954  x: [-99.99457882  24.99926075]  f(x): 2.9935740346234168e-05  grad at x: [ 0.01084237 -0.0014785 ]  gradient norm: 0.010942712706862805\n",
            "iter: 4955  x: [-99.99458966  24.99926223]  f(x): 2.9816117127763174e-05  grad at x: [ 0.01082069 -0.00147555]  gradient norm: 0.01092082728144039\n",
            "iter: 4956  x: [-99.99460048  24.9992637 ]  f(x): 2.9696971923686398e-05  grad at x: [ 0.01079904 -0.0014726 ]  gradient norm: 0.010898985626871227\n",
            "iter: 4957  x: [-99.99461128  24.99926517]  f(x): 2.957830282383959e-05  grad at x: [ 0.01077745 -0.00146965]  gradient norm: 0.010877187655610174\n",
            "iter: 4958  x: [-99.99462205  24.99926664]  f(x): 2.946010792582168e-05  grad at x: [ 0.01075589 -0.00146671]  gradient norm: 0.010855433280311142\n",
            "iter: 4959  x: [-99.99463281  24.99926811]  f(x): 2.9342385334517598e-05  grad at x: [ 0.01073438 -0.00146378]  gradient norm: 0.01083372241374452\n",
            "iter: 4960  x: [-99.99464354  24.99926957]  f(x): 2.922513316268195e-05  grad at x: [ 0.01071291 -0.00146085]  gradient norm: 0.010812054968909832\n",
            "iter: 4961  x: [-99.99465426  24.99927104]  f(x): 2.91083495305934e-05  grad at x: [ 0.01069148 -0.00145793]  gradient norm: 0.010790430858977485\n",
            "iter: 4962  x: [-99.99466495  24.99927249]  f(x): 2.8992032565885318e-05  grad at x: [ 0.0106701  -0.00145501]  gradient norm: 0.010768849997262533\n",
            "iter: 4963  x: [-99.99467562  24.99927395]  f(x): 2.887618040382184e-05  grad at x: [ 0.01064876 -0.0014521 ]  gradient norm: 0.010747312297280999\n",
            "iter: 4964  x: [-99.99468627  24.9992754 ]  f(x): 2.8760791186955997e-05  grad at x: [ 0.01062746 -0.0014492 ]  gradient norm: 0.010725817672691625\n",
            "iter: 4965  x: [-99.9946969   24.99927685]  f(x): 2.864586306541514e-05  grad at x: [ 0.01060621 -0.0014463 ]  gradient norm: 0.01070436603735413\n",
            "iter: 4966  x: [-99.9947075  24.9992783]  f(x): 2.8531394196560504e-05  grad at x: [ 0.010585   -0.00144341]  gradient norm: 0.010682957305270952\n",
            "iter: 4967  x: [-99.99471809  24.99927974]  f(x): 2.8417382745281807e-05  grad at x: [ 0.01056383 -0.00144052]  gradient norm: 0.010661591390647422\n",
            "iter: 4968  x: [-99.99472865  24.99928118]  f(x): 2.8303826883787522e-05  grad at x: [ 0.0105427  -0.00143764]  gradient norm: 0.010640268207857831\n",
            "iter: 4969  x: [-99.99473919  24.99928262]  f(x): 2.8190724791608153e-05  grad at x: [ 0.01052161 -0.00143477]  gradient norm: 0.010618987671451202\n",
            "iter: 4970  x: [-99.99474971  24.99928405]  f(x): 2.807807465539908e-05  grad at x: [ 0.01050057 -0.0014319 ]  gradient norm: 0.01059774969611928\n",
            "iter: 4971  x: [-99.99476022  24.99928548]  f(x): 2.7965874669068802e-05  grad at x: [ 0.01047957 -0.00142903]  gradient norm: 0.010576554196725662\n",
            "iter: 4972  x: [-99.9947707   24.99928691]  f(x): 2.7854123033905426e-05  grad at x: [ 0.01045861 -0.00142617]  gradient norm: 0.010555401088334906\n",
            "iter: 4973  x: [-99.99478115  24.99928834]  f(x): 2.7742817958241205e-05  grad at x: [ 0.01043769 -0.00142332]  gradient norm: 0.010534290286154298\n",
            "iter: 4974  x: [-99.99479159  24.99928976]  f(x): 2.7631957657738246e-05  grad at x: [ 0.01041682 -0.00142048]  gradient norm: 0.010513221705593058\n",
            "iter: 4975  x: [-99.99480201  24.99929118]  f(x): 2.752154035490165e-05  grad at x: [ 0.01039598 -0.00141763]  gradient norm: 0.010492195262174956\n",
            "iter: 4976  x: [-99.9948124  24.9992926]  f(x): 2.7411564279650487e-05  grad at x: [ 0.01037519 -0.0014148 ]  gradient norm: 0.010471210871651948\n",
            "iter: 4977  x: [-99.99482278  24.99929402]  f(x): 2.7302027668851592e-05  grad at x: [ 0.01035444 -0.00141197]  gradient norm: 0.010450268449920622\n",
            "iter: 4978  x: [-99.99483313  24.99929543]  f(x): 2.719292876628936e-05  grad at x: [ 0.01033373 -0.00140915]  gradient norm: 0.010429367913021261\n",
            "iter: 4979  x: [-99.99484347  24.99929684]  f(x): 2.7084265822933693e-05  grad at x: [ 0.01031306 -0.00140633]  gradient norm: 0.010408509177194146\n",
            "iter: 4980  x: [-99.99485378  24.99929824]  f(x): 2.6976037096770785e-05  grad at x: [ 0.01029244 -0.00140351]  gradient norm: 0.010387692158852376\n",
            "iter: 4981  x: [-99.99486407  24.99929965]  f(x): 2.686824085246975e-05  grad at x: [ 0.01027185 -0.00140071]  gradient norm: 0.010366916774522644\n",
            "iter: 4982  x: [-99.99487434  24.99930105]  f(x): 2.67608753619669e-05  grad at x: [ 0.01025131 -0.00139791]  gradient norm: 0.0103461829409627\n",
            "iter: 4983  x: [-99.9948846   24.99930244]  f(x): 2.665393890397546e-05  grad at x: [ 0.01023081 -0.00139511]  gradient norm: 0.010325490575072056\n",
            "iter: 4984  x: [-99.99489483  24.99930384]  f(x): 2.6547429764120923e-05  grad at x: [ 0.01021035 -0.00139232]  gradient norm: 0.010304839593923027\n",
            "iter: 4985  x: [-99.99490504  24.99930523]  f(x): 2.6441346234765183e-05  grad at x: [ 0.01018993 -0.00138954]  gradient norm: 0.01028422991473162\n",
            "iter: 4986  x: [-99.99491523  24.99930662]  f(x): 2.633568661527105e-05  grad at x: [ 0.01016955 -0.00138676]  gradient norm: 0.010263661454913845\n",
            "iter: 4987  x: [-99.9949254   24.99930801]  f(x): 2.6230449211547125e-05  grad at x: [ 0.01014921 -0.00138398]  gradient norm: 0.010243134132002202\n",
            "iter: 4988  x: [-99.99493555  24.99930939]  f(x): 2.612563233645175e-05  grad at x: [ 0.01012891 -0.00138121]  gradient norm: 0.010222647863729192\n",
            "iter: 4989  x: [-99.99494567  24.99931077]  f(x): 2.6021234309617295e-05  grad at x: [ 0.01010865 -0.00137845]  gradient norm: 0.010202202567998206\n",
            "iter: 4990  x: [-99.99495578  24.99931215]  f(x): 2.591725345728609e-05  grad at x: [ 0.01008843 -0.0013757 ]  gradient norm: 0.010181798162856321\n",
            "iter: 4991  x: [-99.99496587  24.99931353]  f(x): 2.581368811243913e-05  grad at x: [ 0.01006826 -0.00137294]  gradient norm: 0.01016143456652438\n",
            "iter: 4992  x: [-99.99497594  24.9993149 ]  f(x): 2.5710536614755926e-05  grad at x: [ 0.01004812 -0.0013702 ]  gradient norm: 0.010141111697394112\n",
            "iter: 4993  x: [-99.99498599  24.99931627]  f(x): 2.5607797310446653e-05  grad at x: [ 0.01002802 -0.00136746]  gradient norm: 0.010120829473999974\n",
            "iter: 4994  x: [-99.99499602  24.99931764]  f(x): 2.5505468552385073e-05  grad at x: [ 0.01000797 -0.00136472]  gradient norm: 0.010100587815050186\n",
            "iter: 4995  x: [-99.99500602  24.999319  ]  f(x): 2.540354870007359e-05  grad at x: [ 0.00998795 -0.00136199]  gradient norm: 0.010080386639424817\n",
            "iter: 4996  x: [-99.99501601  24.99932037]  f(x): 2.5302036119466755e-05  grad at x: [ 0.00996798 -0.00135927]  gradient norm: 0.010060225866145701\n",
            "iter: 4997  x: [-99.99502598  24.99932172]  f(x): 2.5200929183108405e-05  grad at x: [ 0.00994804 -0.00135655]  gradient norm: 0.010040105414408437\n",
            "iter: 4998  x: [-99.99503593  24.99932308]  f(x): 2.5100226270096982e-05  grad at x: [ 0.00992814 -0.00135384]  gradient norm: 0.010020025203580474\n",
            "iter: 4999  x: [-99.99504586  24.99932443]  f(x): 2.4999925765915034e-05  grad at x: [ 0.00990829 -0.00135113]  gradient norm: 0.009999985153171985\n",
            "iter: 5000  x: [-99.99505576  24.99932579]  f(x): 2.490002606255598e-05  grad at x: [ 0.00988847 -0.00134843]  gradient norm: 0.00997998518286595\n",
            "iter: 5001  x: [-99.99506565  24.99932713]  f(x): 2.4800525558354292e-05  grad at x: [ 0.00986869 -0.00134573]  gradient norm: 0.00996002521248903\n",
            "iter: 5002  x: [-99.99507552  24.99932848]  f(x): 2.4701422658237476e-05  grad at x: [ 0.00984896 -0.00134304]  gradient norm: 0.009940105162066944\n",
            "iter: 5003  x: [-99.99508537  24.99932982]  f(x): 2.4602715773290566e-05  grad at x: [ 0.00982926 -0.00134035]  gradient norm: 0.009920224951741883\n",
            "iter: 5004  x: [-99.9950952   24.99933116]  f(x): 2.4504403321008496e-05  grad at x: [ 0.0098096  -0.00133767]  gradient norm: 0.009900384501827895\n",
            "iter: 5005  x: [-99.99510501  24.9993325 ]  f(x): 2.440648372527169e-05  grad at x: [ 0.00978998 -0.001335  ]  gradient norm: 0.00988058373281087\n",
            "iter: 5006  x: [-99.9951148   24.99933384]  f(x): 2.4308955416321746e-05  grad at x: [ 0.0097704  -0.00133233]  gradient norm: 0.00986082256534854\n",
            "iter: 5007  x: [-99.99512457  24.99933517]  f(x): 2.4211816830460065e-05  grad at x: [ 0.00975086 -0.00132966]  gradient norm: 0.009841100920214174\n",
            "iter: 5008  x: [-99.99513432  24.9993365 ]  f(x): 2.4115066410436638e-05  grad at x: [ 0.00973136 -0.001327  ]  gradient norm: 0.009821418718380076\n",
            "iter: 5009  x: [-99.99514405  24.99933783]  f(x): 2.401870260502469e-05  grad at x: [ 0.0097119  -0.00132435]  gradient norm: 0.009801775880936003\n",
            "iter: 5010  x: [-99.99515376  24.99933915]  f(x): 2.39227238693938e-05  grad at x: [ 0.00969247 -0.0013217 ]  gradient norm: 0.009782172329169794\n",
            "iter: 5011  x: [-99.99516346  24.99934047]  f(x): 2.3827128664828507e-05  grad at x: [ 0.00967309 -0.00131906]  gradient norm: 0.009762607984514898\n",
            "iter: 5012  x: [-99.99517313  24.99934179]  f(x): 2.3731915458687057e-05  grad at x: [ 0.00965374 -0.00131642]  gradient norm: 0.009743082768546525\n",
            "iter: 5013  x: [-99.99518278  24.99934311]  f(x): 2.363708272453001e-05  grad at x: [ 0.00963443 -0.00131379]  gradient norm: 0.009723596603012696\n",
            "iter: 5014  x: [-99.99519242  24.99934442]  f(x): 2.3542628941955244e-05  grad at x: [ 0.00961516 -0.00131116]  gradient norm: 0.009704149409805116\n",
            "iter: 5015  x: [-99.99520203  24.99934573]  f(x): 2.3448552596712048e-05  grad at x: [ 0.00959593 -0.00130854]  gradient norm: 0.009684741110987334\n",
            "iter: 5016  x: [-99.99521163  24.99934704]  f(x): 2.3354852180541533e-05  grad at x: [ 0.00957674 -0.00130592]  gradient norm: 0.00966537162876659\n",
            "iter: 5017  x: [-99.99522121  24.99934835]  f(x): 2.3261526191285713e-05  grad at x: [ 0.00955759 -0.00130331]  gradient norm: 0.009646040885521005\n",
            "iter: 5018  x: [-99.99523076  24.99934965]  f(x): 2.316857313260698e-05  grad at x: [ 0.00953847 -0.0013007 ]  gradient norm: 0.009626748803746149\n",
            "iter: 5019  x: [-99.9952403   24.99935095]  f(x): 2.3075991514354763e-05  grad at x: [ 0.0095194 -0.0012981]  gradient norm: 0.009607495306135676\n",
            "iter: 5020  x: [-99.99524982  24.99935225]  f(x): 2.2983779852280294e-05  grad at x: [ 0.00950036 -0.0012955 ]  gradient norm: 0.009588280315526929\n",
            "iter: 5021  x: [-99.99525932  24.99935354]  f(x): 2.2891936668019373e-05  grad at x: [ 0.00948136 -0.00129291]  gradient norm: 0.009569103754901892\n",
            "iter: 5022  x: [-99.9952688   24.99935484]  f(x): 2.2800460489066036e-05  grad at x: [ 0.00946239 -0.00129033]  gradient norm: 0.00954996554738624\n",
            "iter: 5023  x: [-99.99527827  24.99935613]  f(x): 2.2709349849010177e-05  grad at x: [ 0.00944347 -0.00128775]  gradient norm: 0.009530865616303731\n",
            "iter: 5024  x: [-99.99528771  24.99935741]  f(x): 2.2618603287000433e-05  grad at x: [ 0.00942458 -0.00128517]  gradient norm: 0.00951180388506837\n",
            "iter: 5025  x: [-99.99529713  24.9993587 ]  f(x): 2.2528219348232616e-05  grad at x: [ 0.00940573 -0.0012826 ]  gradient norm: 0.009492780277291288\n",
            "iter: 5026  x: [-99.99530654  24.99935998]  f(x): 2.2438196583681622e-05  grad at x: [ 0.00938692 -0.00128003]  gradient norm: 0.00947379471672922\n",
            "iter: 5027  x: [-99.99531593  24.99936126]  f(x): 2.234853355019502e-05  grad at x: [ 0.00936815 -0.00127747]  gradient norm: 0.009454847127308832\n",
            "iter: 5028  x: [-99.99532529  24.99936254]  f(x): 2.225922881009001e-05  grad at x: [ 0.00934941 -0.00127492]  gradient norm: 0.00943593743304607\n",
            "iter: 5029  x: [-99.99533464  24.99936382]  f(x): 2.2170280931779717e-05  grad at x: [ 0.00933071 -0.00127237]  gradient norm: 0.009417065558183125\n",
            "iter: 5030  x: [-99.99534397  24.99936509]  f(x): 2.2081688489232362e-05  grad at x: [ 0.00931205 -0.00126983]  gradient norm: 0.009398231427078685\n",
            "iter: 5031  x: [-99.99535329  24.99936636]  f(x): 2.1993450062079016e-05  grad at x: [ 0.00929343 -0.00126729]  gradient norm: 0.00937943496423511\n",
            "iter: 5032  x: [-99.99536258  24.99936762]  f(x): 2.1905564235588127e-05  grad at x: [ 0.00927484 -0.00126475]  gradient norm: 0.00936067609429749\n",
            "iter: 5033  x: [-99.99537185  24.99936889]  f(x): 2.181802960091221e-05  grad at x: [ 0.00925629 -0.00126222]  gradient norm: 0.009341954742110928\n",
            "iter: 5034  x: [-99.99538111  24.99937015]  f(x): 2.173084475467497e-05  grad at x: [ 0.00923778 -0.0012597 ]  gradient norm: 0.009323270832637003\n",
            "iter: 5035  x: [-99.99539035  24.99937141]  f(x): 2.1644008299069427e-05  grad at x: [ 0.0092193  -0.00125718]  gradient norm: 0.009304624290979066\n",
            "iter: 5036  x: [-99.99539957  24.99937267]  f(x): 2.1557518841850648e-05  grad at x: [ 0.00920086 -0.00125466]  gradient norm: 0.009286015042385113\n",
            "iter: 5037  x: [-99.99540877  24.99937392]  f(x): 2.1471374996571587e-05  grad at x: [ 0.00918246 -0.00125215]  gradient norm: 0.009267443012303143\n",
            "iter: 5038  x: [-99.99541795  24.99937518]  f(x): 2.1385575382038977e-05  grad at x: [ 0.0091641  -0.00124965]  gradient norm: 0.009248908126268522\n",
            "iter: 5039  x: [-99.99542712  24.99937642]  f(x): 2.1300118622810867e-05  grad at x: [ 0.00914577 -0.00124715]  gradient norm: 0.009230410310015664\n",
            "iter: 5040  x: [-99.99543626  24.99937767]  f(x): 2.121500334878892e-05  grad at x: [ 0.00912748 -0.00124466]  gradient norm: 0.009211949489394505\n",
            "iter: 5041  x: [-99.99544539  24.99937892]  f(x): 2.1130228195462664e-05  grad at x: [ 0.00910922 -0.00124217]  gradient norm: 0.009193525590427791\n",
            "iter: 5042  x: [-99.9954545   24.99938016]  f(x): 2.104579180361623e-05  grad at x: [ 0.009091   -0.00123968]  gradient norm: 0.009175138539251868\n",
            "iter: 5043  x: [-99.99546359  24.9993814 ]  f(x): 2.096169281957617e-05  grad at x: [ 0.00907282 -0.0012372 ]  gradient norm: 0.009156788262174935\n",
            "iter: 5044  x: [-99.99547266  24.99938264]  f(x): 2.0877929895065694e-05  grad at x: [ 0.00905468 -0.00123473]  gradient norm: 0.009138474685649831\n",
            "iter: 5045  x: [-99.99548172  24.99938387]  f(x): 2.0794501687180156e-05  grad at x: [ 0.00903657 -0.00123226]  gradient norm: 0.00912019773627308\n",
            "iter: 5046  x: [-99.99549075  24.9993851 ]  f(x): 2.071140685848644e-05  grad at x: [ 0.00901849 -0.00122979]  gradient norm: 0.009101957340811139\n",
            "iter: 5047  x: [-99.99549977  24.99938633]  f(x): 2.062864407663117e-05  grad at x: [ 0.00900046 -0.00122734]  gradient norm: 0.00908375342611878\n",
            "iter: 5048  x: [-99.99550877  24.99938756]  f(x): 2.0546212014957195e-05  grad at x: [ 0.00898246 -0.00122488]  gradient norm: 0.00906558591927895\n",
            "iter: 5049  x: [-99.99551775  24.99938878]  f(x): 2.0464109351711255e-05  grad at x: [ 0.00896449 -0.00122243]  gradient norm: 0.00904745474743284\n",
            "iter: 5050  x: [-99.99552672  24.99939001]  f(x): 2.0382334770799544e-05  grad at x: [ 0.00894656 -0.00121999]  gradient norm: 0.00902935983795076\n",
            "iter: 5051  x: [-99.99553567  24.99939123]  f(x): 2.030088696111677e-05  grad at x: [ 0.00892867 -0.00121755]  gradient norm: 0.009011301118288474\n",
            "iter: 5052  x: [-99.99554459  24.99939244]  f(x): 2.0219764616795363e-05  grad at x: [ 0.00891081 -0.00121511]  gradient norm: 0.008993278516046385\n",
            "iter: 5053  x: [-99.9955535   24.99939366]  f(x): 2.0138966437429967e-05  grad at x: [ 0.00889299 -0.00121268]  gradient norm: 0.008975291959023943\n",
            "iter: 5054  x: [-99.9955624   24.99939487]  f(x): 2.005849112755959e-05  grad at x: [ 0.0088752  -0.00121026]  gradient norm: 0.00895734137510893\n",
            "iter: 5055  x: [-99.99557127  24.99939608]  f(x): 1.9978337397015357e-05  grad at x: [ 0.00885745 -0.00120783]  gradient norm: 0.008939426692359047\n",
            "iter: 5056  x: [-99.99558013  24.99939729]  f(x): 1.9898503960787193e-05  grad at x: [ 0.00883974 -0.00120542]  gradient norm: 0.00892154783897664\n",
            "iter: 5057  x: [-99.99558897  24.9993985 ]  f(x): 1.9818989539000215e-05  grad at x: [ 0.00882206 -0.00120301]  gradient norm: 0.008903704743307747\n",
            "iter: 5058  x: [-99.99559779  24.9993997 ]  f(x): 1.9739792856761807e-05  grad at x: [ 0.00880442 -0.0012006 ]  gradient norm: 0.008885897333812\n",
            "iter: 5059  x: [-99.9956066  24.9994009]  f(x): 1.9660912644535426e-05  grad at x: [ 0.00878681 -0.0011982 ]  gradient norm: 0.008868125539150971\n",
            "iter: 5060  x: [-99.99561538  24.9994021 ]  f(x): 1.9582347637599175e-05  grad at x: [ 0.00876923 -0.0011958 ]  gradient norm: 0.008850389288070706\n",
            "iter: 5061  x: [-99.99562415  24.99940329]  f(x): 1.9504096576427924e-05  grad at x: [ 0.00875169 -0.00119341]  gradient norm: 0.008832688509491982\n",
            "iter: 5062  x: [-99.9956329   24.99940449]  f(x): 1.9426158206523426e-05  grad at x: [ 0.00873419 -0.00119103]  gradient norm: 0.00881502313247638\n",
            "iter: 5063  x: [-99.99564164  24.99940568]  f(x): 1.9348531278284324e-05  grad at x: [ 0.00871672 -0.00118864]  gradient norm: 0.008797393086201009\n",
            "iter: 5064  x: [-99.99565036  24.99940687]  f(x): 1.9271214547239975e-05  grad at x: [ 0.00869929 -0.00118627]  gradient norm: 0.008779798300015776\n",
            "iter: 5065  x: [-99.99565905  24.99940805]  f(x): 1.9194206773898598e-05  grad at x: [ 0.00868189 -0.00118389]  gradient norm: 0.008762238703413323\n",
            "iter: 5066  x: [-99.99566774  24.99940924]  f(x): 1.911750672360962e-05  grad at x: [ 0.00866453 -0.00118153]  gradient norm: 0.008744714226001812\n",
            "iter: 5067  x: [-99.9956764   24.99941042]  f(x): 1.904111316678782e-05  grad at x: [ 0.0086472  -0.00117916]  gradient norm: 0.008727224797560293\n",
            "iter: 5068  x: [-99.99568505  24.9994116 ]  f(x): 1.8965024878533985e-05  grad at x: [ 0.0086299 -0.0011768]  gradient norm: 0.008709770347956135\n",
            "iter: 5069  x: [-99.99569368  24.99941277]  f(x): 1.8889240639091632e-05  grad at x: [ 0.00861264 -0.00117445]  gradient norm: 0.008692350807253843\n",
            "iter: 5070  x: [-99.99570229  24.99941395]  f(x): 1.881375923348474e-05  grad at x: [ 0.00859542 -0.0011721 ]  gradient norm: 0.008674966105636318\n",
            "iter: 5071  x: [-99.99571089  24.99941512]  f(x): 1.8738579451597377e-05  grad at x: [ 0.00857823 -0.00116976]  gradient norm: 0.008657616173427274\n",
            "iter: 5072  x: [-99.99571946  24.99941629]  f(x): 1.8663700088050395e-05  grad at x: [ 0.00856107 -0.00116742]  gradient norm: 0.008640300941066902\n",
            "iter: 5073  x: [-99.99572803  24.99941746]  f(x): 1.8589119942544413e-05  grad at x: [ 0.00854395 -0.00116508]  gradient norm: 0.008623020339195406\n",
            "iter: 5074  x: [-99.99573657  24.99941862]  f(x): 1.8514837819224983e-05  grad at x: [ 0.00852686 -0.00116275]  gradient norm: 0.00860577429851027\n",
            "iter: 5075  x: [-99.9957451   24.99941979]  f(x): 1.8440852527285183e-05  grad at x: [ 0.00850981 -0.00116043]  gradient norm: 0.008588562749909948\n",
            "iter: 5076  x: [-99.99575361  24.99942095]  f(x): 1.8367162880574695e-05  grad at x: [ 0.00849279 -0.00115811]  gradient norm: 0.008571385624407455\n",
            "iter: 5077  x: [-99.9957621  24.9994221]  f(x): 1.8293767697707575e-05  grad at x: [ 0.0084758  -0.00115579]  gradient norm: 0.008554242853159495\n",
            "iter: 5078  x: [-99.99577057  24.99942326]  f(x): 1.8220665802040116e-05  grad at x: [ 0.00845885 -0.00115348]  gradient norm: 0.008537134367465494\n",
            "iter: 5079  x: [-99.99577903  24.99942441]  f(x): 1.8147856021541185e-05  grad at x: [ 0.00844193 -0.00115117]  gradient norm: 0.008520060098741367\n",
            "iter: 5080  x: [-99.99578748  24.99942556]  f(x): 1.8075337188882954e-05  grad at x: [ 0.00842505 -0.00114887]  gradient norm: 0.00850301997854479\n",
            "iter: 5081  x: [-99.9957959   24.99942671]  f(x): 1.8003108141435385e-05  grad at x: [ 0.0084082  -0.00114657]  gradient norm: 0.008486013938578086\n",
            "iter: 5082  x: [-99.99580431  24.99942786]  f(x): 1.7931167721355462e-05  grad at x: [ 0.00839138 -0.00114428]  gradient norm: 0.008469041910713505\n",
            "iter: 5083  x: [-99.9958127  24.999429 ]  f(x): 1.7859514775104508e-05  grad at x: [ 0.0083746  -0.00114199]  gradient norm: 0.00845210382688346\n",
            "iter: 5084  x: [-99.99582107  24.99943015]  f(x): 1.7788148154019827e-05  grad at x: [ 0.00835785 -0.00113971]  gradient norm: 0.008435199619219411\n",
            "iter: 5085  x: [-99.99582943  24.99943129]  f(x): 1.7717066713943208e-05  grad at x: [ 0.00834113 -0.00113743]  gradient norm: 0.008418329219968344\n",
            "iter: 5086  x: [-99.99583777  24.99943242]  f(x): 1.7646269315318832e-05  grad at x: [ 0.00832445 -0.00113515]  gradient norm: 0.008401492561519966\n",
            "iter: 5087  x: [-99.9958461   24.99943356]  f(x): 1.757575482317988e-05  grad at x: [ 0.0083078  -0.00113288]  gradient norm: 0.008384689576407676\n",
            "iter: 5088  x: [-99.99585441  24.99943469]  f(x): 1.750552210689143e-05  grad at x: [ 0.00829119 -0.00113062]  gradient norm: 0.008367920197251268\n",
            "iter: 5089  x: [-99.9958627   24.99943582]  f(x): 1.743557004060986e-05  grad at x: [ 0.00827461 -0.00112836]  gradient norm: 0.008351184356870553\n",
            "iter: 5090  x: [-99.99587097  24.99943695]  f(x): 1.7365897502672435e-05  grad at x: [ 0.00825806 -0.0011261 ]  gradient norm: 0.008334481988143579\n",
            "iter: 5091  x: [-99.99587923  24.99943808]  f(x): 1.7296503376294303e-05  grad at x: [ 0.00824154 -0.00112385]  gradient norm: 0.008317813024177521\n",
            "iter: 5092  x: [-99.99588747  24.9994392 ]  f(x): 1.7227386548834492e-05  grad at x: [ 0.00822506 -0.0011216 ]  gradient norm: 0.008301177398136843\n",
            "iter: 5093  x: [-99.9958957   24.99944032]  f(x): 1.715854591214027e-05  grad at x: [ 0.00820861 -0.00111936]  gradient norm: 0.008284575043329686\n",
            "iter: 5094  x: [-99.99590391  24.99944144]  f(x): 1.7089980362646497e-05  grad at x: [ 0.00819219 -0.00111712]  gradient norm: 0.008268005893236046\n",
            "iter: 5095  x: [-99.9959121   24.99944256]  f(x): 1.7021688801121085e-05  grad at x: [ 0.00817581 -0.00111488]  gradient norm: 0.008251469881450476\n",
            "iter: 5096  x: [-99.99592027  24.99944367]  f(x): 1.6953670132653215e-05  grad at x: [ 0.00815945 -0.00111265]  gradient norm: 0.008234966941683061\n",
            "iter: 5097  x: [-99.99592843  24.99944479]  f(x): 1.6885923266749337e-05  grad at x: [ 0.00814313 -0.00111043]  gradient norm: 0.008218497007786603\n",
            "iter: 5098  x: [-99.99593658  24.9994459 ]  f(x): 1.6818447117320318e-05  grad at x: [ 0.00812685 -0.00110821]  gradient norm: 0.008202060013757596\n",
            "iter: 5099  x: [-99.9959447  24.999447 ]  f(x): 1.67512406026607e-05  grad at x: [ 0.00811059 -0.00110599]  gradient norm: 0.00818565589373526\n",
            "iter: 5100  x: [-99.99595281  24.99944811]  f(x): 1.668430264520586e-05  grad at x: [ 0.00809437 -0.00110378]  gradient norm: 0.008169284581946172\n",
            "iter: 5101  x: [-99.99596091  24.99944921]  f(x): 1.6617632171858126e-05  grad at x: [ 0.00807818 -0.00110157]  gradient norm: 0.008152946012787802\n",
            "iter: 5102  x: [-99.99596899  24.99945032]  f(x): 1.655122811374378e-05  grad at x: [ 0.00806203 -0.00109937]  gradient norm: 0.008136640120773139\n",
            "iter: 5103  x: [-99.99597705  24.99945142]  f(x): 1.6485089406193748e-05  grad at x: [ 0.0080459  -0.00109717]  gradient norm: 0.008120366840529742\n",
            "iter: 5104  x: [-99.99598509  24.99945251]  f(x): 1.6419214988960357e-05  grad at x: [ 0.00802981 -0.00109497]  gradient norm: 0.008104126106857014\n",
            "iter: 5105  x: [-99.99599312  24.99945361]  f(x): 1.6353603805854204e-05  grad at x: [ 0.00801375 -0.00109278]  gradient norm: 0.008087917854640761\n",
            "iter: 5106  x: [-99.99600114  24.9994547 ]  f(x): 1.6288254805074884e-05  grad at x: [ 0.00799773 -0.0010906 ]  gradient norm: 0.008071742018938633\n",
            "iter: 5107  x: [-99.99600913  24.99945579]  f(x): 1.622316693884938e-05  grad at x: [ 0.00798173 -0.00108842]  gradient norm: 0.008055598534894692\n",
            "iter: 5108  x: [-99.99601712  24.99945688]  f(x): 1.615833916376147e-05  grad at x: [ 0.00796577 -0.00108624]  gradient norm: 0.008039487337824837\n",
            "iter: 5109  x: [-99.99602508  24.99945797]  f(x): 1.6093770440500744e-05  grad at x: [ 0.00794984 -0.00108407]  gradient norm: 0.008023408363158575\n",
            "iter: 5110  x: [-99.99603303  24.99945905]  f(x): 1.602945973386295e-05  grad at x: [ 0.00793394 -0.0010819 ]  gradient norm: 0.00800736154644286\n",
            "iter: 5111  x: [-99.99604097  24.99946013]  f(x): 1.596540601271574e-05  grad at x: [ 0.00791807 -0.00107974]  gradient norm: 0.007991346823337289\n",
            "iter: 5112  x: [-99.99604888  24.99946121]  f(x): 1.5901608250336012e-05  grad at x: [ 0.00790223 -0.00107758]  gradient norm: 0.007975364129702421\n",
            "iter: 5113  x: [-99.99605679  24.99946229]  f(x): 1.5838065423819767e-05  grad at x: [ 0.00788643 -0.00107542]  gradient norm: 0.007959413401456106\n",
            "iter: 5114  x: [-99.99606467  24.99946336]  f(x): 1.5774776514416537e-05  grad at x: [ 0.00787065 -0.00107327]  gradient norm: 0.007943494574660837\n",
            "iter: 5115  x: [-99.99607254  24.99946444]  f(x): 1.5711740507505804e-05  grad at x: [ 0.00785491 -0.00107112]  gradient norm: 0.007927607585521827\n",
            "iter: 5116  x: [-99.9960804   24.99946551]  f(x): 1.5648956392469763e-05  grad at x: [ 0.0078392  -0.00106898]  gradient norm: 0.007911752370358861\n",
            "iter: 5117  x: [-99.99608824  24.99946658]  f(x): 1.5586423162678637e-05  grad at x: [ 0.00782352 -0.00106684]  gradient norm: 0.007895928865606285\n",
            "iter: 5118  x: [-99.99609606  24.99946764]  f(x): 1.5524139815705514e-05  grad at x: [ 0.00780788 -0.00106471]  gradient norm: 0.00788013700787125\n",
            "iter: 5119  x: [-99.99610387  24.99946871]  f(x): 1.5462105352965957e-05  grad at x: [ 0.00779226 -0.00106258]  gradient norm: 0.007864376733846353\n",
            "iter: 5120  x: [-99.99611166  24.99946977]  f(x): 1.5400318779929416e-05  grad at x: [ 0.00777668 -0.00106046]  gradient norm: 0.007848647980366916\n",
            "iter: 5121  x: [-99.99611944  24.99947083]  f(x): 1.5338779106111153e-05  grad at x: [ 0.00776112 -0.00105834]  gradient norm: 0.007832950684412907\n",
            "iter: 5122  x: [-99.9961272   24.99947189]  f(x): 1.5277485344828976e-05  grad at x: [ 0.0077456  -0.00105622]  gradient norm: 0.007817284783050693\n",
            "iter: 5123  x: [-99.99613494  24.99947295]  f(x): 1.5216436513409694e-05  grad at x: [ 0.00773011 -0.00105411]  gradient norm: 0.007801650213489373\n",
            "iter: 5124  x: [-99.99614267  24.999474  ]  f(x): 1.5155631633064008e-05  grad at x: [ 0.00771465 -0.001052  ]  gradient norm: 0.007786046913052607\n",
            "iter: 5125  x: [-99.99615039  24.99947505]  f(x): 1.5095069729094804e-05  grad at x: [ 0.00769922 -0.00104989]  gradient norm: 0.007770474819235902\n",
            "iter: 5126  x: [-99.99615809  24.9994761 ]  f(x): 1.5034749830440191e-05  grad at x: [ 0.00768382 -0.00104779]  gradient norm: 0.007754933869593006\n",
            "iter: 5127  x: [-99.99616577  24.99947715]  f(x): 1.4974670970097387e-05  grad at x: [ 0.00766845 -0.0010457 ]  gradient norm: 0.007739424001848558\n",
            "iter: 5128  x: [-99.99617344  24.9994782 ]  f(x): 1.4914832184896299e-05  grad at x: [ 0.00765312 -0.00104361]  gradient norm: 0.007723945153843675\n",
            "iter: 5129  x: [-99.99618109  24.99947924]  f(x): 1.485523251547426e-05  grad at x: [ 0.00763781 -0.00104152]  gradient norm: 0.007708497263533084\n",
            "iter: 5130  x: [-99.99618873  24.99948028]  f(x): 1.4795871006374032e-05  grad at x: [ 0.00762254 -0.00103944]  gradient norm: 0.007693080269014234\n",
            "iter: 5131  x: [-99.99619635  24.99948132]  f(x): 1.4736746705819871e-05  grad at x: [ 0.00760729 -0.00103736]  gradient norm: 0.0076776941084729005\n",
            "iter: 5132  x: [-99.99620396  24.99948236]  f(x): 1.4677858666017233e-05  grad at x: [ 0.00759208 -0.00103528]  gradient norm: 0.007662338720264781\n",
            "iter: 5133  x: [-99.99621155  24.99948339]  f(x): 1.4619205942817385e-05  grad at x: [ 0.00757689 -0.00103321]  gradient norm: 0.0076470140428319825\n",
            "iter: 5134  x: [-99.99621913  24.99948443]  f(x): 1.4560787595819412e-05  grad at x: [ 0.00756174 -0.00103115]  gradient norm: 0.0076317200147330904\n",
            "iter: 5135  x: [-99.99622669  24.99948546]  f(x): 1.4502602688559843e-05  grad at x: [ 0.00754661 -0.00102908]  gradient norm: 0.00761645657469662\n",
            "iter: 5136  x: [-99.99623424  24.99948649]  f(x): 1.4444650288179352e-05  grad at x: [ 0.00753152 -0.00102703]  gradient norm: 0.00760122366153749\n",
            "iter: 5137  x: [-99.99624177  24.99948751]  f(x): 1.4386929465631008e-05  grad at x: [ 0.00751646 -0.00102497]  gradient norm: 0.0075860212142152645\n",
            "iter: 5138  x: [-99.99624929  24.99948854]  f(x): 1.4329439295441355e-05  grad at x: [ 0.00750143 -0.00102292]  gradient norm: 0.007570849171774949\n",
            "iter: 5139  x: [-99.99625679  24.99948956]  f(x): 1.427217885602432e-05  grad at x: [ 0.00748642 -0.00102088]  gradient norm: 0.007555707473433397\n",
            "iter: 5140  x: [-99.99626427  24.99949058]  f(x): 1.4215147229343306e-05  grad at x: [ 0.00747145 -0.00101883]  gradient norm: 0.007540596058493865\n",
            "iter: 5141  x: [-99.99627175  24.9994916 ]  f(x): 1.4158343501004688e-05  grad at x: [ 0.00745651 -0.0010168 ]  gradient norm: 0.007525514866374178\n",
            "iter: 5142  x: [-99.9962792   24.99949262]  f(x): 1.410176676035007e-05  grad at x: [ 0.00744159 -0.00101476]  gradient norm: 0.007510463836634877\n",
            "iter: 5143  x: [-99.99628664  24.99949363]  f(x): 1.4045416100343446e-05  grad at x: [ 0.00742671 -0.00101273]  gradient norm: 0.0074954429089529985\n",
            "iter: 5144  x: [-99.99629407  24.99949465]  f(x): 1.3989290617652267e-05  grad at x: [ 0.00741186 -0.00101071]  gradient norm: 0.007480452023147336\n",
            "iter: 5145  x: [-99.99630148  24.99949566]  f(x): 1.3933389412321316e-05  grad at x: [ 0.00739703 -0.00100869]  gradient norm: 0.0074654911190949295\n",
            "iter: 5146  x: [-99.99630888  24.99949667]  f(x): 1.38777115881847e-05  grad at x: [ 0.00738224 -0.00100667]  gradient norm: 0.007450560136844665\n",
            "iter: 5147  x: [-99.99631626  24.99949767]  f(x): 1.3822256252637478e-05  grad at x: [ 0.00736748 -0.00100466]  gradient norm: 0.007435659016559992\n",
            "iter: 5148  x: [-99.99632363  24.99949868]  f(x): 1.3767022516622453e-05  grad at x: [ 0.00735274 -0.00100265]  gradient norm: 0.007420787698518926\n",
            "iter: 5149  x: [-99.99633098  24.99949968]  f(x): 1.371200949462054e-05  grad at x: [ 0.00733803 -0.00100064]  gradient norm: 0.007405946123115004\n",
            "iter: 5150  x: [-99.99633832  24.99950068]  f(x): 1.3657216304634055e-05  grad at x: [ 0.00732336 -0.00099864]  gradient norm: 0.007391134230856332\n",
            "iter: 5151  x: [-99.99634564  24.99950168]  f(x): 1.3602642068273941e-05  grad at x: [ 0.00730871 -0.00099664]  gradient norm: 0.007376351962392776\n",
            "iter: 5152  x: [-99.99635295  24.99950268]  f(x): 1.3548285910552554e-05  grad at x: [ 0.00729409 -0.00099465]  gradient norm: 0.007361599258463491\n",
            "iter: 5153  x: [-99.99636025  24.99950367]  f(x): 1.3494146960060852e-05  grad at x: [ 0.00727951 -0.00099266]  gradient norm: 0.007346876059948433\n",
            "iter: 5154  x: [-99.99636753  24.99950466]  f(x): 1.3440224348858264e-05  grad at x: [ 0.00726495 -0.00099067]  gradient norm: 0.007332182307842124\n",
            "iter: 5155  x: [-99.99637479  24.99950565]  f(x): 1.338651721236378e-05  grad at x: [ 0.00725042 -0.00098869]  gradient norm: 0.007317517943227411\n",
            "iter: 5156  x: [-99.99638204  24.99950664]  f(x): 1.3333024689539166e-05  grad at x: [ 0.00723592 -0.00098672]  gradient norm: 0.007302882907328904\n",
            "iter: 5157  x: [-99.99638928  24.99950763]  f(x): 1.3279745922882148e-05  grad at x: [ 0.00722144 -0.00098474]  gradient norm: 0.0072882771415149\n",
            "iter: 5158  x: [-99.9963965   24.99950861]  f(x): 1.3226680058204235e-05  grad at x: [ 0.007207   -0.00098277]  gradient norm: 0.007273700587240098\n",
            "iter: 5159  x: [-99.99640371  24.9995096 ]  f(x): 1.3173826244721217e-05  grad at x: [ 0.00719259 -0.00098081]  gradient norm: 0.007259153186073763\n",
            "iter: 5160  x: [-99.9964109   24.99951058]  f(x): 1.312118363504046e-05  grad at x: [ 0.0071782  -0.00097885]  gradient norm: 0.007244634879699724\n",
            "iter: 5161  x: [-99.99641808  24.99951156]  f(x): 1.306875138525006e-05  grad at x: [ 0.00716385 -0.00097689]  gradient norm: 0.007230145609944535\n",
            "iter: 5162  x: [-99.99642524  24.99951253]  f(x): 1.3016528654705466e-05  grad at x: [ 0.00714952 -0.00097493]  gradient norm: 0.007215685318722115\n",
            "iter: 5163  x: [-99.99643239  24.99951351]  f(x): 1.2964514606220685e-05  grad at x: [ 0.00713522 -0.00097298]  gradient norm: 0.007201253948090064\n",
            "iter: 5164  x: [-99.99643953  24.99951448]  f(x): 1.2912708405841965e-05  grad at x: [ 0.00712095 -0.00097104]  gradient norm: 0.007186851440190472\n",
            "iter: 5165  x: [-99.99644665  24.99951545]  f(x): 1.2861109223048696e-05  grad at x: [ 0.00710671 -0.0009691 ]  gradient norm: 0.007172477737309108\n",
            "iter: 5166  x: [-99.99645375  24.99951642]  f(x): 1.2809716230639137e-05  grad at x: [ 0.00709249 -0.00096716]  gradient norm: 0.007158132781847271\n",
            "iter: 5167  x: [-99.99646085  24.99951739]  f(x): 1.2758528604613945e-05  grad at x: [ 0.00707831 -0.00096522]  gradient norm: 0.007143816516292659\n",
            "iter: 5168  x: [-99.99646792  24.99951835]  f(x): 1.2707545524265216e-05  grad at x: [ 0.00706415 -0.00096329]  gradient norm: 0.007129528883247536\n",
            "iter: 5169  x: [-99.99647499  24.99951932]  f(x): 1.2656766172364533e-05  grad at x: [ 0.00705002 -0.00096137]  gradient norm: 0.007115269825485056\n",
            "iter: 5170  x: [-99.99648204  24.99952028]  f(x): 1.260618973474872e-05  grad at x: [ 0.00703592 -0.00095944]  gradient norm: 0.007101039285836608\n",
            "iter: 5171  x: [-99.99648907  24.99952124]  f(x): 1.255581540060898e-05  grad at x: [ 0.00702185 -0.00095753]  gradient norm: 0.007086837207276312\n",
            "iter: 5172  x: [-99.9964961   24.99952219]  f(x): 1.2505642362281906e-05  grad at x: [ 0.00700781 -0.00095561]  gradient norm: 0.007072663532865651\n",
            "iter: 5173  x: [-99.9965031   24.99952315]  f(x): 1.245566981543368e-05  grad at x: [ 0.00699379 -0.0009537 ]  gradient norm: 0.0070585182058088315\n",
            "iter: 5174  x: [-99.9965101  24.9995241]  f(x): 1.2405896958848569e-05  grad at x: [ 0.00697981 -0.00095179]  gradient norm: 0.007044401169396464\n",
            "iter: 5175  x: [-99.99651708  24.99952506]  f(x): 1.2356322994615823e-05  grad at x: [ 0.00696585 -0.00094989]  gradient norm: 0.007030312367061886\n",
            "iter: 5176  x: [-99.99652404  24.99952601]  f(x): 1.2306947127919072e-05  grad at x: [ 0.00695191 -0.00094799]  gradient norm: 0.0070162517423248355\n",
            "iter: 5177  x: [-99.99653099  24.99952695]  f(x): 1.2257768567222493e-05  grad at x: [ 0.00693801 -0.00094609]  gradient norm: 0.007002219238847779\n",
            "iter: 5178  x: [-99.99653793  24.9995279 ]  f(x): 1.2208786524064425e-05  grad at x: [ 0.00692413 -0.0009442 ]  gradient norm: 0.006988214800380545\n",
            "iter: 5179  x: [-99.99654486  24.99952884]  f(x): 1.2160000213141259e-05  grad at x: [ 0.00691029 -0.00094231]  gradient norm: 0.0069742383707875255\n",
            "iter: 5180  x: [-99.99655177  24.99952979]  f(x): 1.211140885229222e-05  grad at x: [ 0.00689647 -0.00094043]  gradient norm: 0.006960289894046718\n",
            "iter: 5181  x: [-99.99655866  24.99953073]  f(x): 1.2063011662494219e-05  grad at x: [ 0.00688267 -0.00093855]  gradient norm: 0.0069463693142516455\n",
            "iter: 5182  x: [-99.99656555  24.99953167]  f(x): 1.2014807867850027e-05  grad at x: [ 0.00686891 -0.00093667]  gradient norm: 0.006932476575611353\n",
            "iter: 5183  x: [-99.99657242  24.9995326 ]  f(x): 1.1966796695566515e-05  grad at x: [ 0.00685517 -0.0009348 ]  gradient norm: 0.006918611622447531\n",
            "iter: 5184  x: [-99.99657927  24.99953354]  f(x): 1.1918977375956254e-05  grad at x: [ 0.00684146 -0.00093293]  gradient norm: 0.006904774399198356\n",
            "iter: 5185  x: [-99.99658611  24.99953447]  f(x): 1.1871349142318816e-05  grad at x: [ 0.00682778 -0.00093106]  gradient norm: 0.006890964850387446\n",
            "iter: 5186  x: [-99.99659294  24.9995354 ]  f(x): 1.1823911231130403e-05  grad at x: [ 0.00681412 -0.0009292 ]  gradient norm: 0.0068771829206821025\n",
            "iter: 5187  x: [-99.99659975  24.99953633]  f(x): 1.1776662881834666e-05  grad at x: [ 0.00680049 -0.00092734]  gradient norm: 0.006863428554836035\n",
            "iter: 5188  x: [-99.99660655  24.99953726]  f(x): 1.1729603336925265e-05  grad at x: [ 0.00678689 -0.00092549]  gradient norm: 0.006849701697716555\n",
            "iter: 5189  x: [-99.99661334  24.99953818]  f(x): 1.1682731842037185e-05  grad at x: [ 0.00677332 -0.00092363]  gradient norm: 0.00683600229433466\n",
            "iter: 5190  x: [-99.99662011  24.99953911]  f(x): 1.1636047645639512e-05  grad at x: [ 0.00675977 -0.00092179]  gradient norm: 0.006822330289758628\n",
            "iter: 5191  x: [-99.99662687  24.99954003]  f(x): 1.1589549999227497e-05  grad at x: [ 0.00674625 -0.00091994]  gradient norm: 0.006808685629173225\n",
            "iter: 5192  x: [-99.99663362  24.99954095]  f(x): 1.1543238157396949e-05  grad at x: [ 0.00673276 -0.0009181 ]  gradient norm: 0.006795068257904978\n",
            "iter: 5193  x: [-99.99664035  24.99954187]  f(x): 1.1497111377739707e-05  grad at x: [ 0.00671929 -0.00091627]  gradient norm: 0.006781478121394983\n",
            "iter: 5194  x: [-99.99664707  24.99954278]  f(x): 1.1451168920641712e-05  grad at x: [ 0.00670585 -0.00091443]  gradient norm: 0.006767915165142575\n",
            "iter: 5195  x: [-99.99665378  24.9995437 ]  f(x): 1.1405410049657274e-05  grad at x: [ 0.00669244 -0.00091261]  gradient norm: 0.006754379334818936\n",
            "iter: 5196  x: [-99.99666047  24.99954461]  f(x): 1.1359834031109744e-05  grad at x: [ 0.00667906 -0.00091078]  gradient norm: 0.006740870576152532\n",
            "iter: 5197  x: [-99.99666715  24.99954552]  f(x): 1.1314440134274892e-05  grad at x: [ 0.0066657  -0.00090896]  gradient norm: 0.0067273888349863925\n",
            "iter: 5198  x: [-99.99667382  24.99954643]  f(x): 1.1269227631467486e-05  grad at x: [ 0.00665237 -0.00090714]  gradient norm: 0.006713934057307231\n",
            "iter: 5199  x: [-99.99668047  24.99954734]  f(x): 1.1224195797837236e-05  grad at x: [ 0.00663906 -0.00090533]  gradient norm: 0.006700506189188168\n",
            "iter: 5200  x: [-99.99668711  24.99954824]  f(x): 1.1179343911449582e-05  grad at x: [ 0.00662579 -0.00090352]  gradient norm: 0.0066871051768159235\n",
            "iter: 5201  x: [-99.99669373  24.99954915]  f(x): 1.113467125318708e-05  grad at x: [ 0.00661253 -0.00090171]  gradient norm: 0.006673730966464585\n",
            "iter: 5202  x: [-99.99670035  24.99955005]  f(x): 1.1090177106826663e-05  grad at x: [ 0.00659931 -0.00089991]  gradient norm: 0.006660383504521842\n",
            "iter: 5203  x: [-99.99670694  24.99955095]  f(x): 1.1045860759128633e-05  grad at x: [ 0.00658611 -0.00089811]  gradient norm: 0.0066470627375190714\n",
            "iter: 5204  x: [-99.99671353  24.99955185]  f(x): 1.1001721499541315e-05  grad at x: [ 0.00657294 -0.00089631]  gradient norm: 0.0066337686120458905\n",
            "iter: 5205  x: [-99.9967201   24.99955274]  f(x): 1.0957758620471668e-05  grad at x: [ 0.00655979 -0.00089452]  gradient norm: 0.006620501074834644\n",
            "iter: 5206  x: [-99.99672666  24.99955364]  f(x): 1.0913971416991167e-05  grad at x: [ 0.00654667 -0.00089273]  gradient norm: 0.0066072600726749565\n",
            "iter: 5207  x: [-99.99673321  24.99955453]  f(x): 1.087035918720773e-05  grad at x: [ 0.00653358 -0.00089094]  gradient norm: 0.006594045552529261\n",
            "iter: 5208  x: [-99.99673974  24.99955542]  f(x): 1.0826921231869686e-05  grad at x: [ 0.00652051 -0.00088916]  gradient norm: 0.006580857461416312\n",
            "iter: 5209  x: [-99.99674626  24.99955631]  f(x): 1.0783656854643787e-05  grad at x: [ 0.00650747 -0.00088738]  gradient norm: 0.00656769574649855\n",
            "iter: 5210  x: [-99.99675277  24.9995572 ]  f(x): 1.0740565361823477e-05  grad at x: [ 0.00649446 -0.00088561]  gradient norm: 0.006554560354996657\n",
            "iter: 5211  x: [-99.99675927  24.99955808]  f(x): 1.0697646062596351e-05  grad at x: [ 0.00648147 -0.00088384]  gradient norm: 0.006541451234274043\n",
            "iter: 5212  x: [-99.99676575  24.99955897]  f(x): 1.0654898268940608e-05  grad at x: [ 0.0064685  -0.00088207]  gradient norm: 0.006528368331808678\n",
            "iter: 5213  x: [-99.99677222  24.99955985]  f(x): 1.0612321295427812e-05  grad at x: [ 0.00645557 -0.0008803 ]  gradient norm: 0.006515311595135818\n",
            "iter: 5214  x: [-99.99677867  24.99956073]  f(x): 1.0569914459498174e-05  grad at x: [ 0.00644266 -0.00087854]  gradient norm: 0.006502280971935364\n",
            "iter: 5215  x: [-99.99678511  24.99956161]  f(x): 1.0527677081345161e-05  grad at x: [ 0.00642977 -0.00087679]  gradient norm: 0.006489276409999858\n",
            "iter: 5216  x: [-99.99679154  24.99956248]  f(x): 1.048560848373507e-05  grad at x: [ 0.00641691 -0.00087503]  gradient norm: 0.006476297857182009\n",
            "iter: 5217  x: [-99.99679796  24.99956336]  f(x): 1.0443707992258895e-05  grad at x: [ 0.00640408 -0.00087328]  gradient norm: 0.006463345261475328\n",
            "iter: 5218  x: [-99.99680437  24.99956423]  f(x): 1.0401974935148645e-05  grad at x: [ 0.00639127 -0.00087154]  gradient norm: 0.006450418570960692\n",
            "iter: 5219  x: [-99.99681076  24.9995651 ]  f(x): 1.0360408643264688e-05  grad at x: [ 0.00637849 -0.00086979]  gradient norm: 0.0064375177338053795\n",
            "iter: 5220  x: [-99.99681714  24.99956597]  f(x): 1.0319008450354558e-05  grad at x: [ 0.00636573 -0.00086805]  gradient norm: 0.006424642698346596\n",
            "iter: 5221  x: [-99.9968235   24.99956684]  f(x): 1.027777369259555e-05  grad at x: [ 0.006353   -0.00086632]  gradient norm: 0.006411793412952589\n",
            "iter: 5222  x: [-99.99682985  24.99956771]  f(x): 1.0236703708944401e-05  grad at x: [ 0.00634029 -0.00086459]  gradient norm: 0.006398969826134329\n",
            "iter: 5223  x: [-99.99683619  24.99956857]  f(x): 1.0195797840946236e-05  grad at x: [ 0.00632761 -0.00086286]  gradient norm: 0.006386171886489193\n",
            "iter: 5224  x: [-99.99684252  24.99956943]  f(x): 1.0155055432811885e-05  grad at x: [ 0.00631496 -0.00086113]  gradient norm: 0.006373399542728162\n",
            "iter: 5225  x: [-99.99684884  24.9995703 ]  f(x): 1.0114475831324223e-05  grad at x: [ 0.00630233 -0.00085941]  gradient norm: 0.006360652743649577\n",
            "iter: 5226  x: [-99.99685514  24.99957116]  f(x): 1.0074058385915169e-05  grad at x: [ 0.00628972 -0.00085769]  gradient norm: 0.006347931438166348\n",
            "iter: 5227  x: [-99.99686143  24.99957201]  f(x): 1.003380244856023e-05  grad at x: [ 0.00627714 -0.00085597]  gradient norm: 0.006335235575275865\n",
            "iter: 5228  x: [-99.99686771  24.99957287]  f(x): 9.993707373962542e-06  grad at x: [ 0.00626459 -0.00085426]  gradient norm: 0.0063225651041211245\n",
            "iter: 5229  x: [-99.99687397  24.99957372]  f(x): 9.95377251925708e-06  grad at x: [ 0.00625206 -0.00085255]  gradient norm: 0.006309919973900487\n",
            "iter: 5230  x: [-99.99688022  24.99957458]  f(x): 9.913997244283569e-06  grad at x: [ 0.00623955 -0.00085085]  gradient norm: 0.0062973001339569545\n",
            "iter: 5231  x: [-99.99688646  24.99957543]  f(x): 9.87438091130099e-06  grad at x: [ 0.00622708 -0.00084915]  gradient norm: 0.006284705533690815\n",
            "iter: 5232  x: [-99.99689269  24.99957628]  f(x): 9.834922885159005e-06  grad at x: [ 0.00621462 -0.00084745]  gradient norm: 0.00627213612261692\n",
            "iter: 5233  x: [-99.9968989   24.99957712]  f(x): 9.795622533287995e-06  grad at x: [ 0.00620219 -0.00084575]  gradient norm: 0.006259591850364685\n",
            "iter: 5234  x: [-99.99690511  24.99957797]  f(x): 9.756479225601159e-06  grad at x: [ 0.00618979 -0.00084406]  gradient norm: 0.006247072666649927\n",
            "iter: 5235  x: [-99.9969113   24.99957881]  f(x): 9.717492334573272e-06  grad at x: [ 0.00617741 -0.00084237]  gradient norm: 0.006234578521303031\n",
            "iter: 5236  x: [-99.99691747  24.99957966]  f(x): 9.678661235230822e-06  grad at x: [ 0.00616505 -0.00084069]  gradient norm: 0.006222109364268944\n",
            "iter: 5237  x: [-99.99692364  24.9995805 ]  f(x): 9.639985304964306e-06  grad at x: [ 0.00615272 -0.00083901]  gradient norm: 0.006209665145549897\n",
            "iter: 5238  x: [-99.99692979  24.99958134]  f(x): 9.601463923700683e-06  grad at x: [ 0.00614042 -0.00083733]  gradient norm: 0.006197245815263643\n",
            "iter: 5239  x: [-99.99693593  24.99958217]  f(x): 9.56309647388763e-06  grad at x: [ 0.00612814 -0.00083566]  gradient norm: 0.0061848513236415405\n",
            "iter: 5240  x: [-99.99694206  24.99958301]  f(x): 9.524882340402836e-06  grad at x: [ 0.00611588 -0.00083398]  gradient norm: 0.0061724816210023134\n",
            "iter: 5241  x: [-99.99694818  24.99958384]  f(x): 9.48682091053922e-06  grad at x: [ 0.00610365 -0.00083232]  gradient norm: 0.006160136657750125\n",
            "iter: 5242  x: [-99.99695428  24.99958467]  f(x): 9.44891157417223e-06  grad at x: [ 0.00609144 -0.00083065]  gradient norm: 0.006147816384431868\n",
            "iter: 5243  x: [-99.99696037  24.99958551]  f(x): 9.411153723490148e-06  grad at x: [ 0.00607926 -0.00082899]  gradient norm: 0.0061355207516526735\n",
            "iter: 5244  x: [-99.99696645  24.99958633]  f(x): 9.373546753244863e-06  grad at x: [ 0.0060671  -0.00082733]  gradient norm: 0.006123249710160403\n",
            "iter: 5245  x: [-99.99697252  24.99958716]  f(x): 9.33609006039724e-06  grad at x: [ 0.00605497 -0.00082568]  gradient norm: 0.006111003210732994\n",
            "iter: 5246  x: [-99.99697857  24.99958799]  f(x): 9.29878304453951e-06  grad at x: [ 0.00604286 -0.00082403]  gradient norm: 0.006098781204319273\n",
            "iter: 5247  x: [-99.99698461  24.99958881]  f(x): 9.26162510744969e-06  grad at x: [ 0.00603077 -0.00082238]  gradient norm: 0.0060865836418962285\n",
            "iter: 5248  x: [-99.99699065  24.99958963]  f(x): 9.224615653526938e-06  grad at x: [ 0.00601871 -0.00082073]  gradient norm: 0.006074410474614615\n",
            "iter: 5249  x: [-99.99699666  24.99959045]  f(x): 9.187754089336044e-06  grad at x: [ 0.00600667 -0.00081909]  gradient norm: 0.006062261653652388\n",
            "iter: 5250  x: [-99.99700267  24.99959127]  f(x): 9.151039823955922e-06  grad at x: [ 0.00599466 -0.00081745]  gradient norm: 0.006050137130332146\n",
            "iter: 5251  x: [-99.99700867  24.99959209]  f(x): 9.114472268790572e-06  grad at x: [ 0.00598267 -0.00081582]  gradient norm: 0.006038036856061934\n",
            "iter: 5252  x: [-99.99701465  24.99959291]  f(x): 9.078050837645437e-06  grad at x: [ 0.0059707  -0.00081419]  gradient norm: 0.006025960782363403\n",
            "iter: 5253  x: [-99.99702062  24.99959372]  f(x): 9.041774946469834e-06  grad at x: [ 0.00595876 -0.00081256]  gradient norm: 0.00601390886078924\n",
            "iter: 5254  x: [-99.99702658  24.99959453]  f(x): 9.00564401376694e-06  grad at x: [ 0.00594684 -0.00081093]  gradient norm: 0.006001881043062063\n",
            "iter: 5255  x: [-99.99703252  24.99959534]  f(x): 8.96965746025124e-06  grad at x: [ 0.00593495 -0.00080931]  gradient norm: 0.005989877280963689\n",
            "iter: 5256  x: [-99.99703846  24.99959615]  f(x): 8.933814709003554e-06  grad at x: [ 0.00592308 -0.00080769]  gradient norm: 0.005977897526389543\n",
            "iter: 5257  x: [-99.99704438  24.99959696]  f(x): 8.898115185464696e-06  grad at x: [ 0.00591124 -0.00080608]  gradient norm: 0.00596594173134961\n",
            "iter: 5258  x: [-99.99705029  24.99959777]  f(x): 8.862558317174795e-06  grad at x: [ 0.00589941 -0.00080447]  gradient norm: 0.00595400984788396\n",
            "iter: 5259  x: [-99.99705619  24.99959857]  f(x): 8.827143534098468e-06  grad at x: [ 0.00588761 -0.00080286]  gradient norm: 0.0059421018281744275\n",
            "iter: 5260  x: [-99.99706208  24.99959937]  f(x): 8.791870268537074e-06  grad at x: [ 0.00587584 -0.00080125]  gradient norm: 0.00593021762451837\n",
            "iter: 5261  x: [-99.99706796  24.99960018]  f(x): 8.75673795494724e-06  grad at x: [ 0.00586409 -0.00079965]  gradient norm: 0.005918357189270428\n",
            "iter: 5262  x: [-99.99707382  24.99960098]  f(x): 8.72174603010266e-06  grad at x: [ 0.00585236 -0.00079805]  gradient norm: 0.005906520474899807\n",
            "iter: 5263  x: [-99.99707967  24.99960177]  f(x): 8.686893933002059e-06  grad at x: [ 0.00584065 -0.00079645]  gradient norm: 0.005894707433962116\n",
            "iter: 5264  x: [-99.99708551  24.99960257]  f(x): 8.65218110485818e-06  grad at x: [ 0.00582897 -0.00079486]  gradient norm: 0.005882918019098407\n",
            "iter: 5265  x: [-99.99709134  24.99960336]  f(x): 8.617606989177976e-06  grad at x: [ 0.00581731 -0.00079327]  gradient norm: 0.005871152183065255\n",
            "iter: 5266  x: [-99.99709716  24.99960416]  f(x): 8.583171031668309e-06  grad at x: [ 0.00580568 -0.00079168]  gradient norm: 0.005859409878705639\n",
            "iter: 5267  x: [-99.99710297  24.99960495]  f(x): 8.548872680222243e-06  grad at x: [ 0.00579407 -0.0007901 ]  gradient norm: 0.005847691058947025\n",
            "iter: 5268  x: [-99.99710876  24.99960574]  f(x): 8.514711385001606e-06  grad at x: [ 0.00578248 -0.00078852]  gradient norm: 0.005835995676832397\n",
            "iter: 5269  x: [-99.99711454  24.99960653]  f(x): 8.480686598343291e-06  grad at x: [ 0.00577092 -0.00078694]  gradient norm: 0.005824323685491146\n",
            "iter: 5270  x: [-99.99712031  24.99960732]  f(x): 8.446797774669407e-06  grad at x: [ 0.00575937 -0.00078537]  gradient norm: 0.005812675038110907\n",
            "iter: 5271  x: [-99.99712607  24.9996081 ]  f(x): 8.413044370801252e-06  grad at x: [ 0.00574786 -0.0007838 ]  gradient norm: 0.005801049688048276\n",
            "iter: 5272  x: [-99.99713182  24.99960888]  f(x): 8.379425845470974e-06  grad at x: [ 0.00573636 -0.00078223]  gradient norm: 0.005789447588663697\n",
            "iter: 5273  x: [-99.99713756  24.99960967]  f(x): 8.345941659793058e-06  grad at x: [ 0.00572489 -0.00078067]  gradient norm: 0.005777868693486572\n",
            "iter: 5274  x: [-99.99714328  24.99961045]  f(x): 8.31259127693757e-06  grad at x: [ 0.00571344 -0.00077911]  gradient norm: 0.005766312956105512\n",
            "iter: 5275  x: [-99.997149    24.99961123]  f(x): 8.27937416219857e-06  grad at x: [ 0.00570201 -0.00077755]  gradient norm: 0.005754780330194566\n",
            "iter: 5276  x: [-99.9971547  24.999612 ]  f(x): 8.246289783069904e-06  grad at x: [ 0.00569061 -0.00077599]  gradient norm: 0.005743270769542353\n",
            "iter: 5277  x: [-99.99716039  24.99961278]  f(x): 8.21333760907515e-06  grad at x: [ 0.00567922 -0.00077444]  gradient norm: 0.005731784227995729\n",
            "iter: 5278  x: [-99.99716607  24.99961355]  f(x): 8.18051711199953e-06  grad at x: [ 0.00566787 -0.00077289]  gradient norm: 0.005720320659543319\n",
            "iter: 5279  x: [-99.99717173  24.99961433]  f(x): 8.14782776564486e-06  grad at x: [ 0.00565653 -0.00077135]  gradient norm: 0.005708880018232949\n",
            "iter: 5280  x: [-99.99717739  24.9996151 ]  f(x): 8.115269045897346e-06  grad at x: [ 0.00564522 -0.0007698 ]  gradient norm: 0.005697462258197889\n",
            "iter: 5281  x: [-99.99718304  24.99961587]  f(x): 8.08284043080267e-06  grad at x: [ 0.00563393 -0.00076826]  gradient norm: 0.005686067333685971\n",
            "iter: 5282  x: [-99.99718867  24.99961664]  f(x): 8.050541400474873e-06  grad at x: [ 0.00562266 -0.00076673]  gradient norm: 0.005674695199030472\n",
            "iter: 5283  x: [-99.99719429  24.9996174 ]  f(x): 8.01837143701168e-06  grad at x: [ 0.00561141 -0.00076519]  gradient norm: 0.005663345808622914\n",
            "iter: 5284  x: [-99.9971999   24.99961817]  f(x): 7.986330024729122e-06  grad at x: [ 0.00560019 -0.00076366]  gradient norm: 0.005652019116998499\n",
            "iter: 5285  x: [-99.9972055   24.99961893]  f(x): 7.954416649985318e-06  grad at x: [ 0.00558899 -0.00076214]  gradient norm: 0.0056407150787769166\n",
            "iter: 5286  x: [-99.99721109  24.99961969]  f(x): 7.922630801022515e-06  grad at x: [ 0.00557781 -0.00076061]  gradient norm: 0.005629433648608895\n",
            "iter: 5287  x: [-99.99721667  24.99962046]  f(x): 7.89097196835122e-06  grad at x: [ 0.00556666 -0.00075909]  gradient norm: 0.005618174781315092\n",
            "iter: 5288  x: [-99.99722224  24.99962121]  f(x): 7.859439644348256e-06  grad at x: [ 0.00555552 -0.00075757]  gradient norm: 0.0056069384317462435\n",
            "iter: 5289  x: [-99.99722779  24.99962197]  f(x): 7.828033323563298e-06  grad at x: [ 0.00554441 -0.00075606]  gradient norm: 0.005595724554894852\n",
            "iter: 5290  x: [-99.99723334  24.99962273]  f(x): 7.79675250240066e-06  grad at x: [ 0.00553332 -0.00075454]  gradient norm: 0.005584533105784461\n",
            "iter: 5291  x: [-99.99723887  24.99962348]  f(x): 7.765596679421935e-06  grad at x: [ 0.00552226 -0.00075304]  gradient norm: 0.0055733640395803806\n",
            "iter: 5292  x: [-99.99724439  24.99962424]  f(x): 7.734565355102037e-06  grad at x: [ 0.00551121 -0.00075153]  gradient norm: 0.005562217311505201\n",
            "iter: 5293  x: [-99.9972499   24.99962499]  f(x): 7.7036580319061e-06  grad at x: [ 0.00550019 -0.00075003]  gradient norm: 0.005551092876868878\n",
            "iter: 5294  x: [-99.99725541  24.99962574]  f(x): 7.6728742144354e-06  grad at x: [ 0.00548919 -0.00074853]  gradient norm: 0.005539990691124092\n",
            "iter: 5295  x: [-99.99726089  24.99962649]  f(x): 7.642213409101716e-06  grad at x: [ 0.00547821 -0.00074703]  gradient norm: 0.005528910709751683\n",
            "iter: 5296  x: [-99.99726637  24.99962723]  f(x): 7.611675124282301e-06  grad at x: [ 0.00546726 -0.00074553]  gradient norm: 0.005517852888318898\n",
            "iter: 5297  x: [-99.99727184  24.99962798]  f(x): 7.581258870470263e-06  grad at x: [ 0.00545632 -0.00074404]  gradient norm: 0.005506817182536665\n",
            "iter: 5298  x: [-99.9972773   24.99962872]  f(x): 7.550964160028277e-06  grad at x: [ 0.00544541 -0.00074256]  gradient norm: 0.005495803548173197\n",
            "iter: 5299  x: [-99.99728274  24.99962946]  f(x): 7.520790507259339e-06  grad at x: [ 0.00543452 -0.00074107]  gradient norm: 0.0054848119410821515\n",
            "iter: 5300  x: [-99.99728818  24.99963021]  f(x): 7.490737428404806e-06  grad at x: [ 0.00542365 -0.00073959]  gradient norm: 0.005473842317204545\n",
            "iter: 5301  x: [-99.9972936   24.99963095]  f(x): 7.460804441631902e-06  grad at x: [ 0.0054128  -0.00073811]  gradient norm: 0.005462894632566842\n",
            "iter: 5302  x: [-99.99729901  24.99963168]  f(x): 7.430991067103313e-06  grad at x: [ 0.00540198 -0.00073663]  gradient norm: 0.005451968843309108\n",
            "iter: 5303  x: [-99.99730441  24.99963242]  f(x): 7.401296826818656e-06  grad at x: [ 0.00539117 -0.00073516]  gradient norm: 0.005441064905629653\n",
            "iter: 5304  x: [-99.99730981  24.99963316]  f(x): 7.3717212446871644e-06  grad at x: [ 0.00538039 -0.00073369]  gradient norm: 0.0054301827758141494\n",
            "iter: 5305  x: [-99.99731519  24.99963389]  f(x): 7.342263846589038e-06  grad at x: [ 0.00536963 -0.00073222]  gradient norm: 0.005419322410260913\n",
            "iter: 5306  x: [-99.99732056  24.99963462]  f(x): 7.3129241602205135e-06  grad at x: [ 0.00535889 -0.00073076]  gradient norm: 0.005408483765426504\n",
            "iter: 5307  x: [-99.99732591  24.99963535]  f(x): 7.283701715242144e-06  grad at x: [ 0.00534817 -0.0007293 ]  gradient norm: 0.005397666797883005\n",
            "iter: 5308  x: [-99.99733126  24.99963608]  f(x): 7.2545960431873486e-06  grad at x: [ 0.00533747 -0.00072784]  gradient norm: 0.0053868714642869835\n",
            "iter: 5309  x: [-99.9973366   24.99963681]  f(x): 7.225606677384898e-06  grad at x: [ 0.0053268  -0.00072638]  gradient norm: 0.005376097721353249\n",
            "iter: 5310  x: [-99.99734193  24.99963754]  f(x): 7.196733153103761e-06  grad at x: [ 0.00531615 -0.00072493]  gradient norm: 0.005365345525911174\n",
            "iter: 5311  x: [-99.99734724  24.99963826]  f(x): 7.167975007394568e-06  grad at x: [ 0.00530551 -0.00072348]  gradient norm: 0.005354614834848373\n",
            "iter: 5312  x: [-99.99735255  24.99963898]  f(x): 7.139331779228766e-06  grad at x: [ 0.0052949  -0.00072203]  gradient norm: 0.0053439056051651085\n",
            "iter: 5313  x: [-99.99735784  24.99963971]  f(x): 7.110803009426127e-06  grad at x: [ 0.00528431 -0.00072059]  gradient norm: 0.0053332177939499625\n",
            "iter: 5314  x: [-99.99736313  24.99964043]  f(x): 7.082388240637558e-06  grad at x: [ 0.00527374 -0.00071915]  gradient norm: 0.0053225513583760025\n",
            "iter: 5315  x: [-99.9973684   24.99964115]  f(x): 7.0540870171937755e-06  grad at x: [ 0.0052632  -0.00071771]  gradient norm: 0.0053119062556463755\n",
            "iter: 5316  x: [-99.99737366  24.99964186]  f(x): 7.025898885473159e-06  grad at x: [ 0.00525267 -0.00071627]  gradient norm: 0.005301282443135117\n",
            "iter: 5317  x: [-99.99737892  24.99964258]  f(x): 6.997823393517642e-06  grad at x: [ 0.00524217 -0.00071484]  gradient norm: 0.005290679878245382\n",
            "iter: 5318  x: [-99.99738416  24.99964329]  f(x): 6.9698600912529835e-06  grad at x: [ 0.00523168 -0.00071341]  gradient norm: 0.00528009851849489\n",
            "iter: 5319  x: [-99.99738939  24.99964401]  f(x): 6.9420085303302954e-06  grad at x: [ 0.00522122 -0.00071198]  gradient norm: 0.005269538321458644\n",
            "iter: 5320  x: [-99.99739461  24.99964472]  f(x): 6.914268264273187e-06  grad at x: [ 0.00521078 -0.00071056]  gradient norm: 0.00525899924482717\n",
            "iter: 5321  x: [-99.99739982  24.99964543]  f(x): 6.886638848317393e-06  grad at x: [ 0.00520035 -0.00070914]  gradient norm: 0.005248481246348278\n",
            "iter: 5322  x: [-99.99740502  24.99964614]  f(x): 6.859119839476054e-06  grad at x: [ 0.00518995 -0.00070772]  gradient norm: 0.005237984283854259\n",
            "iter: 5323  x: [-99.99741021  24.99964685]  f(x): 6.831710796616692e-06  grad at x: [ 0.00517957 -0.00070631]  gradient norm: 0.0052275083152938905\n",
            "iter: 5324  x: [-99.99741539  24.99964755]  f(x): 6.80441128029931e-06  grad at x: [ 0.00516921 -0.00070489]  gradient norm: 0.005217053298673231\n",
            "iter: 5325  x: [-99.99742056  24.99964826]  f(x): 6.777220852841295e-06  grad at x: [ 0.00515888 -0.00070348]  gradient norm: 0.005206619192082822\n",
            "iter: 5326  x: [-99.99742572  24.99964896]  f(x): 6.7501390783208074e-06  grad at x: [ 0.00514856 -0.00070208]  gradient norm: 0.005196205953701531\n",
            "iter: 5327  x: [-99.99743087  24.99964966]  f(x): 6.723165522557657e-06  grad at x: [ 0.00513826 -0.00070067]  gradient norm: 0.0051858135417917435\n",
            "iter: 5328  x: [-99.99743601  24.99965036]  f(x): 6.696299753119204e-06  grad at x: [ 0.00512798 -0.00069927]  gradient norm: 0.005175441914704175\n",
            "iter: 5329  x: [-99.99744114  24.99965106]  f(x): 6.669541339303808e-06  grad at x: [ 0.00511773 -0.00069787]  gradient norm: 0.005165091030874019\n",
            "iter: 5330  x: [-99.99744625  24.99965176]  f(x): 6.642889852141756e-06  grad at x: [ 0.00510749 -0.00069648]  gradient norm: 0.0051547608488238346\n",
            "iter: 5331  x: [-99.99745136  24.99965246]  f(x): 6.616344864308844e-06  grad at x: [ 0.00509728 -0.00069508]  gradient norm: 0.005144451327132504\n",
            "iter: 5332  x: [-99.99745646  24.99965315]  f(x): 6.589905950197876e-06  grad at x: [ 0.00508708 -0.00069369]  gradient norm: 0.0051341624244653095\n",
            "iter: 5333  x: [-99.99746155  24.99965385]  f(x): 6.5635726860564534e-06  grad at x: [ 0.00507691 -0.00069231]  gradient norm: 0.005123894099630262\n",
            "iter: 5334  x: [-99.99746662  24.99965454]  f(x): 6.537344649616601e-06  grad at x: [ 0.00506676 -0.00069092]  gradient norm: 0.005113646311436332\n",
            "iter: 5335  x: [-99.99747169  24.99965523]  f(x): 6.511221420382834e-06  grad at x: [ 0.00505662 -0.00068954]  gradient norm: 0.005103419018808012\n",
            "iter: 5336  x: [-99.99747675  24.99965592]  f(x): 6.485202579617663e-06  grad at x: [ 0.00504651 -0.00068816]  gradient norm: 0.0050932121807824434\n",
            "iter: 5337  x: [-99.99748179  24.99965661]  f(x): 6.45928771012467e-06  grad at x: [ 0.00503642 -0.00068678]  gradient norm: 0.005083025756426844\n",
            "iter: 5338  x: [-99.99748683  24.99965729]  f(x): 6.433476396457859e-06  grad at x: [ 0.00502634 -0.00068541]  gradient norm: 0.0050728597049229975\n",
            "iter: 5339  x: [-99.99749186  24.99965798]  f(x): 6.407768224769565e-06  grad at x: [ 0.00501629 -0.00068404]  gradient norm: 0.005062713985509971\n",
            "iter: 5340  x: [-99.99749687  24.99965866]  f(x): 6.382162782949563e-06  grad at x: [ 0.00500626 -0.00068267]  gradient norm: 0.005052588557541397\n",
            "iter: 5341  x: [-99.99750188  24.99965935]  f(x): 6.35665966047362e-06  grad at x: [ 0.00499624 -0.00068131]  gradient norm: 0.005042483380428188\n",
            "iter: 5342  x: [-99.99750687  24.99966003]  f(x): 6.331258448468774e-06  grad at x: [ 0.00498625 -0.00067994]  gradient norm: 0.005032398413666698\n",
            "iter: 5343  x: [-99.99751186  24.99966071]  f(x): 6.30595873971433e-06  grad at x: [ 0.00497628 -0.00067858]  gradient norm: 0.005022333616841609\n",
            "iter: 5344  x: [-99.99751684  24.99966139]  f(x): 6.280760128623506e-06  grad at x: [ 0.00496633 -0.00067723]  gradient norm: 0.005012288949621123\n",
            "iter: 5345  x: [-99.9975218   24.99966206]  f(x): 6.255662211174053e-06  grad at x: [ 0.00495639 -0.00067587]  gradient norm: 0.005002264371731687\n",
            "iter: 5346  x: [-99.99752676  24.99966274]  f(x): 6.230664584975417e-06  grad at x: [ 0.00494648 -0.00067452]  gradient norm: 0.004992259842987108\n",
            "iter: 5347  x: [-99.99753171  24.99966341]  f(x): 6.2057668493255066e-06  grad at x: [ 0.00493659 -0.00067317]  gradient norm: 0.00498227532331384\n",
            "iter: 5348  x: [-99.99753664  24.99966409]  f(x): 6.180968604998595e-06  grad at x: [ 0.00492672 -0.00067182]  gradient norm: 0.004972310772668416\n",
            "iter: 5349  x: [-99.99754157  24.99966476]  f(x): 6.156269454450183e-06  grad at x: [ 0.00491686 -0.00067048]  gradient norm: 0.0049623661511219355\n",
            "iter: 5350  x: [-99.99754649  24.99966543]  f(x): 6.1316690017380535e-06  grad at x: [ 0.00490703 -0.00066914]  gradient norm: 0.00495244141883094\n",
            "iter: 5351  x: [-99.99755139  24.9996661 ]  f(x): 6.107166852376994e-06  grad at x: [ 0.00489721 -0.0006678 ]  gradient norm: 0.004942536535981093\n",
            "iter: 5352  x: [-99.99755629  24.99966677]  f(x): 6.082762613614306e-06  grad at x: [ 0.00488742 -0.00066647]  gradient norm: 0.004932651462900782\n",
            "iter: 5353  x: [-99.99756118  24.99966743]  f(x): 6.058455894216748e-06  grad at x: [ 0.00487765 -0.00066513]  gradient norm: 0.004922786159977599\n",
            "iter: 5354  x: [-99.99756606  24.9996681 ]  f(x): 6.034246304455726e-06  grad at x: [ 0.00486789 -0.0006638 ]  gradient norm: 0.004912940587654496\n",
            "iter: 5355  x: [-99.99757092  24.99966876]  f(x): 6.010133456249509e-06  grad at x: [ 0.00485815 -0.00066248]  gradient norm: 0.004903114706489951\n",
            "iter: 5356  x: [-99.99757578  24.99966942]  f(x): 5.986116962942753e-06  grad at x: [ 0.00484844 -0.00066115]  gradient norm: 0.004893308477070602\n",
            "iter: 5357  x: [-99.99758063  24.99967009]  f(x): 5.96219643958636e-06  grad at x: [ 0.00483874 -0.00065983]  gradient norm: 0.004883521860127734\n",
            "iter: 5358  x: [-99.99758547  24.99967075]  f(x): 5.938371502646214e-06  grad at x: [ 0.00482906 -0.00065851]  gradient norm: 0.0048737548164207905\n",
            "iter: 5359  x: [-99.9975903  24.9996714]  f(x): 5.9146417701379645e-06  grad at x: [ 0.00481941 -0.00065719]  gradient norm: 0.004864007306794662\n",
            "iter: 5360  x: [-99.99759512  24.99967206]  f(x): 5.89100686162344e-06  grad at x: [ 0.00480977 -0.00065588]  gradient norm: 0.004854279292180639\n",
            "iter: 5361  x: [-99.99759993  24.99967272]  f(x): 5.867466398204729e-06  grad at x: [ 0.00480015 -0.00065457]  gradient norm: 0.004844570733596416\n",
            "iter: 5362  x: [-99.99760473  24.99967337]  f(x): 5.844020002450207e-06  grad at x: [ 0.00479055 -0.00065326]  gradient norm: 0.004834881592117932\n",
            "iter: 5363  x: [-99.99760952  24.99967403]  f(x): 5.820667298518273e-06  grad at x: [ 0.00478097 -0.00065195]  gradient norm: 0.004825211828932808\n",
            "iter: 5364  x: [-99.9976143   24.99967468]  f(x): 5.797407912026827e-06  grad at x: [ 0.0047714  -0.00065065]  gradient norm: 0.004815561405288827\n",
            "iter: 5365  x: [-99.99761907  24.99967533]  f(x): 5.774241470038836e-06  grad at x: [ 0.00476186 -0.00064934]  gradient norm: 0.004805930282490097\n",
            "iter: 5366  x: [-99.99762383  24.99967598]  f(x): 5.751167601129381e-06  grad at x: [ 0.00475234 -0.00064805]  gradient norm: 0.004796318421927127\n",
            "iter: 5367  x: [-99.99762858  24.99967663]  f(x): 5.728185935377548e-06  grad at x: [ 0.00474283 -0.00064675]  gradient norm: 0.004786725785075868\n",
            "iter: 5368  x: [-99.99763333  24.99967727]  f(x): 5.7052961043652614e-06  grad at x: [ 0.00473335 -0.00064546]  gradient norm: 0.004777152333499639\n",
            "iter: 5369  x: [-99.99763806  24.99967792]  f(x): 5.682497741164633e-06  grad at x: [ 0.00472388 -0.00064417]  gradient norm: 0.0047675980288462375\n",
            "iter: 5370  x: [-99.99764278  24.99967856]  f(x): 5.65979048020055e-06  grad at x: [ 0.00471443 -0.00064288]  gradient norm: 0.004758062832792585\n",
            "iter: 5371  x: [-99.9976475  24.9996792]  f(x): 5.6371739574514655e-06  grad at x: [ 0.004705   -0.00064159]  gradient norm: 0.004748546707131126\n",
            "iter: 5372  x: [-99.9976522   24.99967985]  f(x): 5.614647810304989e-06  grad at x: [ 0.00469559 -0.00064031]  gradient norm: 0.004739049613711588\n",
            "iter: 5373  x: [-99.9976569   24.99968049]  f(x): 5.5922116776860775e-06  grad at x: [ 0.0046862  -0.00063903]  gradient norm: 0.0047295715144973025\n",
            "iter: 5374  x: [-99.99766159  24.99968113]  f(x): 5.569865199851344e-06  grad at x: [ 0.00467683 -0.00063775]  gradient norm: 0.004720112371480723\n",
            "iter: 5375  x: [-99.99766626  24.99968176]  f(x): 5.547608018521931e-06  grad at x: [ 0.00466748 -0.00063647]  gradient norm: 0.004710672146741665\n",
            "iter: 5376  x: [-99.99767093  24.9996824 ]  f(x): 5.525439776871091e-06  grad at x: [ 0.00465814 -0.0006352 ]  gradient norm: 0.004701250802444427\n",
            "iter: 5377  x: [-99.99767559  24.99968303]  f(x): 5.5033601195231246e-06  grad at x: [ 0.00464882 -0.00063393]  gradient norm: 0.004691848300839713\n",
            "iter: 5378  x: [-99.99768024  24.99968367]  f(x): 5.481368692479595e-06  grad at x: [ 0.00463953 -0.00063266]  gradient norm: 0.004682464604235507\n",
            "iter: 5379  x: [-99.99768488  24.9996843 ]  f(x): 5.45946514318249e-06  grad at x: [ 0.00463025 -0.0006314 ]  gradient norm: 0.004673099675026198\n",
            "iter: 5380  x: [-99.99768951  24.99968493]  f(x): 5.437649120442981e-06  grad at x: [ 0.00462099 -0.00063013]  gradient norm: 0.004663753475664416\n",
            "iter: 5381  x: [-99.99769413  24.99968556]  f(x): 5.415920274563159e-06  grad at x: [ 0.00461175 -0.00062887]  gradient norm: 0.004654425968715437\n",
            "iter: 5382  x: [-99.99769874  24.99968619]  f(x): 5.39427825713813e-06  grad at x: [ 0.00460252 -0.00062762]  gradient norm: 0.004645117116774616\n",
            "iter: 5383  x: [-99.99770334  24.99968682]  f(x): 5.372722721247656e-06  grad at x: [ 0.00459332 -0.00062636]  gradient norm: 0.004635826882551874\n",
            "iter: 5384  x: [-99.99770793  24.99968745]  f(x): 5.351253321252352e-06  grad at x: [ 0.00458413 -0.00062511]  gradient norm: 0.004626555228786252\n",
            "iter: 5385  x: [-99.99771252  24.99968807]  f(x): 5.32986971298459e-06  grad at x: [ 0.00457496 -0.00062386]  gradient norm: 0.004617302118330396\n",
            "iter: 5386  x: [-99.99771709  24.99968869]  f(x): 5.308571553614865e-06  grad at x: [ 0.00456581 -0.00062261]  gradient norm: 0.004608067514095194\n",
            "iter: 5387  x: [-99.99772166  24.99968932]  f(x): 5.2873585017095575e-06  grad at x: [ 0.00455668 -0.00062137]  gradient norm: 0.004598851379076979\n",
            "iter: 5388  x: [-99.99772622  24.99968994]  f(x): 5.266230217163111e-06  grad at x: [ 0.00454757 -0.00062012]  gradient norm: 0.0045896536763303224\n",
            "iter: 5389  x: [-99.99773076  24.99969056]  f(x): 5.245186361191098e-06  grad at x: [ 0.00453847 -0.00061888]  gradient norm: 0.004580474368967083\n",
            "iter: 5390  x: [-99.9977353   24.99969118]  f(x): 5.224226596518592e-06  grad at x: [ 0.0045294  -0.00061764]  gradient norm: 0.00457131342024088\n",
            "iter: 5391  x: [-99.99773983  24.9996918 ]  f(x): 5.203350587054545e-06  grad at x: [ 0.00452034 -0.00061641]  gradient norm: 0.004562170793407255\n",
            "iter: 5392  x: [-99.99774435  24.99969241]  f(x): 5.1825579980785196e-06  grad at x: [ 0.0045113  -0.00061518]  gradient norm: 0.004553046451807194\n",
            "iter: 5393  x: [-99.99774886  24.99969303]  f(x): 5.161848496299353e-06  grad at x: [ 0.00450227 -0.00061395]  gradient norm: 0.004543940358895285\n",
            "iter: 5394  x: [-99.99775337  24.99969364]  f(x): 5.141221749725886e-06  grad at x: [ 0.00449327 -0.00061272]  gradient norm: 0.004534852478185321\n",
            "iter: 5395  x: [-99.99775786  24.99969425]  f(x): 5.120677427592039e-06  grad at x: [ 0.00448428 -0.00061149]  gradient norm: 0.004525782773219253\n",
            "iter: 5396  x: [-99.99776234  24.99969486]  f(x): 5.100215200611583e-06  grad at x: [ 0.00447531 -0.00061027]  gradient norm: 0.00451673120768176\n",
            "iter: 5397  x: [-99.99776682  24.99969548]  f(x): 5.079834740652096e-06  grad at x: [ 0.00446636 -0.00060905]  gradient norm: 0.00450769774525848\n",
            "iter: 5398  x: [-99.99777128  24.99969608]  f(x): 5.0595357210483655e-06  grad at x: [ 0.00445743 -0.00060783]  gradient norm: 0.004498682349776816\n",
            "iter: 5399  x: [-99.99777574  24.99969669]  f(x): 5.039317816284137e-06  grad at x: [ 0.00444852 -0.00060662]  gradient norm: 0.004489684985067053\n",
            "iter: 5400  x: [-99.99778019  24.9996973 ]  f(x): 5.019180702295643e-06  grad at x: [ 0.00443962 -0.0006054 ]  gradient norm: 0.004480705615099319\n",
            "iter: 5401  x: [-99.99778463  24.9996979 ]  f(x): 4.99912405622408e-06  grad at x: [ 0.00443074 -0.00060419]  gradient norm: 0.004471744203875745\n",
            "iter: 5402  x: [-99.99778906  24.99969851]  f(x): 4.979147556463787e-06  grad at x: [ 0.00442188 -0.00060298]  gradient norm: 0.004462800715453822\n",
            "iter: 5403  x: [-99.99779348  24.99969911]  f(x): 4.959250882854471e-06  grad at x: [ 0.00441303 -0.00060178]  gradient norm: 0.004453875114034731\n",
            "iter: 5404  x: [-99.9977979   24.99969971]  f(x): 4.939433716355445e-06  grad at x: [ 0.00440421 -0.00060057]  gradient norm: 0.0044449673638196465\n",
            "iter: 5405  x: [-99.9978023   24.99970031]  f(x): 4.919695739236212e-06  grad at x: [ 0.0043954  -0.00059937]  gradient norm: 0.004436077429097112\n",
            "iter: 5406  x: [-99.9978067   24.99970091]  f(x): 4.900036635064955e-06  grad at x: [ 0.00438661 -0.00059817]  gradient norm: 0.004427205274240152\n",
            "iter: 5407  x: [-99.99781108  24.99970151]  f(x): 4.880456088645523e-06  grad at x: [ 0.00437784 -0.00059698]  gradient norm: 0.004418350863680033\n",
            "iter: 5408  x: [-99.99781546  24.99970211]  f(x): 4.8609537861329185e-06  grad at x: [ 0.00436908 -0.00059578]  gradient norm: 0.004409514161960666\n",
            "iter: 5409  x: [-99.99781983  24.9997027 ]  f(x): 4.841529414788257e-06  grad at x: [ 0.00436034 -0.00059459]  gradient norm: 0.004400695133629803\n",
            "iter: 5410  x: [-99.99782419  24.9997033 ]  f(x): 4.822182663274207e-06  grad at x: [ 0.00435162 -0.0005934 ]  gradient norm: 0.00439189374337504\n",
            "iter: 5411  x: [-99.99782854  24.99970389]  f(x): 4.802913221348618e-06  grad at x: [ 0.00434292 -0.00059222]  gradient norm: 0.004383109955886855\n",
            "iter: 5412  x: [-99.99783288  24.99970448]  f(x): 4.783720780101427e-06  grad at x: [ 0.00433423 -0.00059103]  gradient norm: 0.004374343735968369\n",
            "iter: 5413  x: [-99.99783722  24.99970508]  f(x): 4.7646050318918065e-06  grad at x: [ 0.00432556 -0.00058985]  gradient norm: 0.004365595048509106\n",
            "iter: 5414  x: [-99.99784154  24.99970567]  f(x): 4.745565670157056e-06  grad at x: [ 0.00431691 -0.00058867]  gradient norm: 0.004356863858399551\n",
            "iter: 5415  x: [-99.99784586  24.99970625]  f(x): 4.7266023897177205e-06  grad at x: [ 0.00430828 -0.00058749]  gradient norm: 0.004348150130672915\n",
            "iter: 5416  x: [-99.99785017  24.99970684]  f(x): 4.7077148865860255e-06  grad at x: [ 0.00429966 -0.00058632]  gradient norm: 0.004339453830419688\n",
            "iter: 5417  x: [-99.99785447  24.99970743]  f(x): 4.688902857898527e-06  grad at x: [ 0.00429106 -0.00058514]  gradient norm: 0.004330774922758525\n",
            "iter: 5418  x: [-99.99785876  24.99970801]  f(x): 4.670166002103339e-06  grad at x: [ 0.00428248 -0.00058397]  gradient norm: 0.004322113372924564\n",
            "iter: 5419  x: [-99.99786304  24.9997086 ]  f(x): 4.651504018762016e-06  grad at x: [ 0.00427392 -0.00058281]  gradient norm: 0.004313469146180144\n",
            "iter: 5420  x: [-99.99786732  24.99970918]  f(x): 4.632916608675457e-06  grad at x: [ 0.00426537 -0.00058164]  gradient norm: 0.004304842207874968\n",
            "iter: 5421  x: [-99.99787158  24.99970976]  f(x): 4.614403473933317e-06  grad at x: [ 0.00425684 -0.00058048]  gradient norm: 0.0042962325234713805\n",
            "iter: 5422  x: [-99.99787584  24.99971034]  f(x): 4.595964317669171e-06  grad at x: [ 0.00424832 -0.00057932]  gradient norm: 0.004287640058432691\n",
            "iter: 5423  x: [-99.99788009  24.99971092]  f(x): 4.5775988442423756e-06  grad at x: [ 0.00423983 -0.00057816]  gradient norm: 0.004279064778309567\n",
            "iter: 5424  x: [-99.99788433  24.9997115 ]  f(x): 4.55930675928926e-06  grad at x: [ 0.00423135 -0.000577  ]  gradient norm: 0.004270506648766284\n",
            "iter: 5425  x: [-99.99788856  24.99971208]  f(x): 4.5410877694756586e-06  grad at x: [ 0.00422288 -0.00057585]  gradient norm: 0.004261965635467118\n",
            "iter: 5426  x: [-99.99789278  24.99971265]  f(x): 4.52294158273761e-06  grad at x: [ 0.00421444 -0.0005747 ]  gradient norm: 0.004253441704190906\n",
            "iter: 5427  x: [-99.997897    24.99971323]  f(x): 4.50486790815441e-06  grad at x: [ 0.00420601 -0.00057355]  gradient norm: 0.00424493482077377\n",
            "iter: 5428  x: [-99.9979012  24.9997138]  f(x): 4.486866456008192e-06  grad at x: [ 0.0041976 -0.0005724]  gradient norm: 0.004236444951139194\n",
            "iter: 5429  x: [-99.9979054   24.99971437]  f(x): 4.468936937651989e-06  grad at x: [ 0.0041892  -0.00057125]  gradient norm: 0.004227972061237865\n",
            "iter: 5430  x: [-99.99790959  24.99971494]  f(x): 4.451079065631158e-06  grad at x: [ 0.00418082 -0.00057011]  gradient norm: 0.00421951611710687\n",
            "iter: 5431  x: [-99.99791377  24.99971551]  f(x): 4.433292553678676e-06  grad at x: [ 0.00417246 -0.00056897]  gradient norm: 0.004211077084869701\n",
            "iter: 5432  x: [-99.99791794  24.99971608]  f(x): 4.4155771166492475e-06  grad at x: [ 0.00416412 -0.00056783]  gradient norm: 0.004202654930707135\n",
            "iter: 5433  x: [-99.99792211  24.99971665]  f(x): 4.397932470515246e-06  grad at x: [ 0.00415579 -0.0005667 ]  gradient norm: 0.004194249620857226\n",
            "iter: 5434  x: [-99.99792626  24.99971722]  f(x): 4.380358332362655e-06  grad at x: [ 0.00414748 -0.00056557]  gradient norm: 0.004185861121615315\n",
            "iter: 5435  x: [-99.99793041  24.99971778]  f(x): 4.362854420447856e-06  grad at x: [ 0.00413918 -0.00056443]  gradient norm: 0.004177489399363142\n",
            "iter: 5436  x: [-99.99793455  24.99971835]  f(x): 4.3454204541889966e-06  grad at x: [ 0.0041309  -0.00056331]  gradient norm: 0.004169134420566934\n",
            "iter: 5437  x: [-99.99793868  24.99971891]  f(x): 4.328056154052221e-06  grad at x: [ 0.00412264 -0.00056218]  gradient norm: 0.004160796151724918\n",
            "iter: 5438  x: [-99.9979428   24.99971947]  f(x): 4.310761241653185e-06  grad at x: [ 0.0041144  -0.00056105]  gradient norm: 0.004152474559417882\n",
            "iter: 5439  x: [-99.99794692  24.99972003]  f(x): 4.293535439706122e-06  grad at x: [ 0.00410617 -0.00055993]  gradient norm: 0.004144169610286781\n",
            "iter: 5440  x: [-99.99795102  24.99972059]  f(x): 4.2763784720681295e-06  grad at x: [ 0.00409796 -0.00055881]  gradient norm: 0.004135881271056087\n",
            "iter: 5441  x: [-99.99795512  24.99972115]  f(x): 4.2592900636844705e-06  grad at x: [ 0.00408976 -0.00055769]  gradient norm: 0.004127609508509481\n",
            "iter: 5442  x: [-99.99795921  24.99972171]  f(x): 4.242269940578657e-06  grad at x: [ 0.00408158 -0.00055658]  gradient norm: 0.004119354289486962\n",
            "iter: 5443  x: [-99.99796329  24.99972227]  f(x): 4.2253178299103785e-06  grad at x: [ 0.00407342 -0.00055547]  gradient norm: 0.004111115580914931\n",
            "iter: 5444  x: [-99.99796736  24.99972282]  f(x): 4.208433459853468e-06  grad at x: [ 0.00406527 -0.00055436]  gradient norm: 0.0041028933497489145\n",
            "iter: 5445  x: [-99.99797143  24.99972338]  f(x): 4.1916165597675155e-06  grad at x: [ 0.00405714 -0.00055325]  gradient norm: 0.004094687563059001\n",
            "iter: 5446  x: [-99.99797549  24.99972393]  f(x): 4.174866860016256e-06  grad at x: [ 0.00404903 -0.00055214]  gradient norm: 0.004086498187943441\n",
            "iter: 5447  x: [-99.99797954  24.99972448]  f(x): 4.158184092025627e-06  grad at x: [ 0.00404093 -0.00055104]  gradient norm: 0.0040783251915587244\n",
            "iter: 5448  x: [-99.99798358  24.99972503]  f(x): 4.141567988392555e-06  grad at x: [ 0.00403285 -0.00054993]  gradient norm: 0.004070168541174949\n",
            "iter: 5449  x: [-99.99798761  24.99972558]  f(x): 4.125018282710316e-06  grad at x: [ 0.00402478 -0.00054883]  gradient norm: 0.004062028204092294\n",
            "iter: 5450  x: [-99.99799163  24.99972613]  f(x): 4.108534709675528e-06  grad at x: [ 0.00401673 -0.00054774]  gradient norm: 0.004053904147695418\n",
            "iter: 5451  x: [-99.99799565  24.99972668]  f(x): 4.092117004971774e-06  grad at x: [ 0.0040087  -0.00054664]  gradient norm: 0.004045796339398104\n",
            "iter: 5452  x: [-99.99799966  24.99972723]  f(x): 4.075764905440804e-06  grad at x: [ 0.00400068 -0.00054555]  gradient norm: 0.004037704746729659\n",
            "iter: 5453  x: [-99.99800366  24.99972777]  f(x): 4.059478148903463e-06  grad at x: [ 0.00399268 -0.00054446]  gradient norm: 0.0040296293372485085\n",
            "iter: 5454  x: [-99.99800765  24.99972832]  f(x): 4.043256474209251e-06  grad at x: [ 0.00398469 -0.00054337]  gradient norm: 0.004021570078568445\n",
            "iter: 5455  x: [-99.99801164  24.99972886]  f(x): 4.027099621355234e-06  grad at x: [ 0.00397672 -0.00054228]  gradient norm: 0.00401352693841974\n",
            "iter: 5456  x: [-99.99801561  24.9997294 ]  f(x): 4.01100733124781e-06  grad at x: [ 0.00396877 -0.0005412 ]  gradient norm: 0.004005499884532671\n",
            "iter: 5457  x: [-99.99801958  24.99972994]  f(x): 3.994979345925258e-06  grad at x: [ 0.00396083 -0.00054011]  gradient norm: 0.003997488884750155\n",
            "iter: 5458  x: [-99.99802354  24.99973048]  f(x): 3.9790154084482044e-06  grad at x: [ 0.00395291 -0.00053903]  gradient norm: 0.003989493906975271\n",
            "iter: 5459  x: [-99.9980275   24.99973102]  f(x): 3.963115262884375e-06  grad at x: [ 0.00394501 -0.00053796]  gradient norm: 0.003981514919165505\n",
            "iter: 5460  x: [-99.99803144  24.99973156]  f(x): 3.947278654316393e-06  grad at x: [ 0.00393712 -0.00053688]  gradient norm: 0.003973551889338501\n",
            "iter: 5461  x: [-99.99803538  24.9997321 ]  f(x): 3.931505328830407e-06  grad at x: [ 0.00392924 -0.00053581]  gradient norm: 0.003965604785568228\n",
            "iter: 5462  x: [-99.99803931  24.99973263]  f(x): 3.915795033514319e-06  grad at x: [ 0.00392138 -0.00053473]  gradient norm: 0.003957673575985933\n",
            "iter: 5463  x: [-99.99804323  24.99973317]  f(x): 3.9001475165672505e-06  grad at x: [ 0.00391354 -0.00053366]  gradient norm: 0.003949758228837431\n",
            "iter: 5464  x: [-99.99804714  24.9997337 ]  f(x): 3.884562527068938e-06  grad at x: [ 0.00390571 -0.0005326 ]  gradient norm: 0.003941858712368539\n",
            "iter: 5465  x: [-99.99805105  24.99973423]  f(x): 3.869039815204457e-06  grad at x: [ 0.0038979  -0.00053153]  gradient norm: 0.0039339749949405915\n",
            "iter: 5466  x: [-99.99805495  24.99973477]  f(x): 3.853579132086058e-06  grad at x: [ 0.00389011 -0.00053047]  gradient norm: 0.003926107044942131\n",
            "iter: 5467  x: [-99.99805884  24.9997353 ]  f(x): 3.838180229867997e-06  grad at x: [ 0.00388233 -0.00052941]  gradient norm: 0.003918254830849059\n",
            "iter: 5468  x: [-99.99806272  24.99973583]  f(x): 3.822842861683525e-06  grad at x: [ 0.00387456 -0.00052835]  gradient norm: 0.003910418321194562\n",
            "iter: 5469  x: [-99.99806659  24.99973635]  f(x): 3.8075667815844556e-06  grad at x: [ 0.00386681 -0.00052729]  gradient norm: 0.0039025974845399857\n",
            "iter: 5470  x: [-99.99807046  24.99973688]  f(x): 3.7923517447064227e-06  grad at x: [ 0.00385908 -0.00052624]  gradient norm: 0.0038947922895612407\n",
            "iter: 5471  x: [-99.99807432  24.99973741]  f(x): 3.7771975071528488e-06  grad at x: [ 0.00385136 -0.00052519]  gradient norm: 0.00388700270499152\n",
            "iter: 5472  x: [-99.99807817  24.99973793]  f(x): 3.7621038259386296e-06  grad at x: [ 0.00384366 -0.00052414]  gradient norm: 0.003879228699594098\n",
            "iter: 5473  x: [-99.99808202  24.99973846]  f(x): 3.7470704590379083e-06  grad at x: [ 0.00383597 -0.00052309]  gradient norm: 0.003871470242188571\n",
            "iter: 5474  x: [-99.99808585  24.99973898]  f(x): 3.7320971654930706e-06  grad at x: [ 0.0038283  -0.00052204]  gradient norm: 0.0038637273017091\n",
            "iter: 5475  x: [-99.99808968  24.9997395 ]  f(x): 3.7171837052435214e-06  grad at x: [ 0.00382064 -0.000521  ]  gradient norm: 0.003855999847118006\n",
            "iter: 5476  x: [-99.9980935   24.99974002]  f(x): 3.702329839182466e-06  grad at x: [ 0.003813   -0.00051995]  gradient norm: 0.0038482878474368136\n",
            "iter: 5477  x: [-99.99809731  24.99974054]  f(x): 3.6875353291478373e-06  grad at x: [ 0.00380537 -0.00051891]  gradient norm: 0.003840591271743369\n",
            "iter: 5478  x: [-99.99810112  24.99974106]  f(x): 3.6727999379764718e-06  grad at x: [ 0.00379776 -0.00051788]  gradient norm: 0.003832910089201922\n",
            "iter: 5479  x: [-99.99810492  24.99974158]  f(x): 3.658123429440702e-06  grad at x: [ 0.00379017 -0.00051684]  gradient norm: 0.0038252442690320846\n",
            "iter: 5480  x: [-99.99810871  24.9997421 ]  f(x): 3.6435055682003337e-06  grad at x: [ 0.00378259 -0.00051581]  gradient norm: 0.003817593780485469\n",
            "iter: 5481  x: [-99.99811249  24.99974261]  f(x): 3.6289461199533006e-06  grad at x: [ 0.00377502 -0.00051478]  gradient norm: 0.003809958592926333\n",
            "iter: 5482  x: [-99.99811626  24.99974313]  f(x): 3.614444851272368e-06  grad at x: [ 0.00376747 -0.00051375]  gradient norm: 0.003802338675748055\n",
            "iter: 5483  x: [-99.99812003  24.99974364]  f(x): 3.60000152965385e-06  grad at x: [ 0.00375994 -0.00051272]  gradient norm: 0.0037947339984003357\n",
            "iter: 5484  x: [-99.99812379  24.99974415]  f(x): 3.585615923521476e-06  grad at x: [ 0.00375242 -0.00051169]  gradient norm: 0.0037871445303930378\n",
            "iter: 5485  x: [-99.99812754  24.99974467]  f(x): 3.5712878022670767e-06  grad at x: [ 0.00374491 -0.00051067]  gradient norm: 0.003779570241319548\n",
            "iter: 5486  x: [-99.99813129  24.99974518]  f(x): 3.557016936197195e-06  grad at x: [ 0.00373742 -0.00050965]  gradient norm: 0.0037720111008305345\n",
            "iter: 5487  x: [-99.99813503  24.99974569]  f(x): 3.5428030965333157e-06  grad at x: [ 0.00372995 -0.00050863]  gradient norm: 0.0037644670786358674\n",
            "iter: 5488  x: [-99.99813876  24.99974619]  f(x): 3.528646055350145e-06  grad at x: [ 0.00372249 -0.00050761]  gradient norm: 0.0037569381444735794\n",
            "iter: 5489  x: [-99.99814248  24.9997467 ]  f(x): 3.514545585731176e-06  grad at x: [ 0.00371504 -0.0005066 ]  gradient norm: 0.0037494242681943455\n",
            "iter: 5490  x: [-99.99814619  24.99974721]  f(x): 3.5005014615607272e-06  grad at x: [ 0.00370761 -0.00050558]  gradient norm: 0.003741925419652683\n",
            "iter: 5491  x: [-99.9981499   24.99974771]  f(x): 3.486513457721179e-06  grad at x: [ 0.0037002  -0.00050457]  gradient norm: 0.003734441568813832\n",
            "iter: 5492  x: [-99.9981536   24.99974822]  f(x): 3.472581349940156e-06  grad at x: [ 0.0036928  -0.00050356]  gradient norm: 0.0037269726856740746\n",
            "iter: 5493  x: [-99.99815729  24.99974872]  f(x): 3.4587049148888724e-06  grad at x: [ 0.00368541 -0.00050256]  gradient norm: 0.003719518740315135\n",
            "iter: 5494  x: [-99.99816098  24.99974922]  f(x): 3.4448839300737645e-06  grad at x: [ 0.00367804 -0.00050155]  gradient norm: 0.0037120797028478603\n",
            "iter: 5495  x: [-99.99816466  24.99974973]  f(x): 3.431118173885881e-06  grad at x: [ 0.00367068 -0.00050055]  gradient norm: 0.003704655543440378\n",
            "iter: 5496  x: [-99.99816833  24.99975023]  f(x): 3.417407425649655e-06  grad at x: [ 0.00366334 -0.00049955]  gradient norm: 0.0036972462323462607\n",
            "iter: 5497  x: [-99.99817199  24.99975073]  f(x): 3.4037514655689244e-06  grad at x: [ 0.00365602 -0.00049855]  gradient norm: 0.003689851739877322\n",
            "iter: 5498  x: [-99.99817565  24.99975122]  f(x): 3.3901500747201153e-06  grad at x: [ 0.0036487  -0.00049755]  gradient norm: 0.003682472036401697\n",
            "iter: 5499  x: [-99.9981793   24.99975172]  f(x): 3.3766030349990247e-06  grad at x: [ 0.00364141 -0.00049656]  gradient norm: 0.0036751070923166443\n",
            "iter: 5500  x: [-99.99818294  24.99975222]  f(x): 3.3631101292747883e-06  grad at x: [ 0.00363412 -0.00049556]  gradient norm: 0.0036677568781339847\n",
            "iter: 5501  x: [-99.99818657  24.99975271]  f(x): 3.349671141177678e-06  grad at x: [ 0.00362686 -0.00049457]  gradient norm: 0.0036604213643665\n",
            "iter: 5502  x: [-99.9981902   24.99975321]  f(x): 3.336285855302664e-06  grad at x: [ 0.0036196  -0.00049358]  gradient norm: 0.003653100521640577\n",
            "iter: 5503  x: [-99.99819382  24.9997537 ]  f(x): 3.3229540569998373e-06  grad at x: [ 0.00361236 -0.00049259]  gradient norm: 0.003645794320583561\n",
            "iter: 5504  x: [-99.99819743  24.9997542 ]  f(x): 3.309675532577166e-06  grad at x: [ 0.00360514 -0.00049161]  gradient norm: 0.0036385027319364026\n",
            "iter: 5505  x: [-99.99820104  24.99975469]  f(x): 3.296450069142894e-06  grad at x: [ 0.00359793 -0.00049063]  gradient norm: 0.0036312257264691735\n",
            "iter: 5506  x: [-99.99820463  24.99975518]  f(x): 3.2832774546556413e-06  grad at x: [ 0.00359073 -0.00048965]  gradient norm: 0.003623963275010188\n",
            "iter: 5507  x: [-99.99820822  24.99975567]  f(x): 3.2701574779686722e-06  grad at x: [ 0.00358355 -0.00048867]  gradient norm: 0.003616715348472242\n",
            "iter: 5508  x: [-99.99821181  24.99975616]  f(x): 3.257089928677246e-06  grad at x: [ 0.00357638 -0.00048769]  gradient norm: 0.0036094819177700534\n",
            "iter: 5509  x: [-99.99821538  24.99975664]  f(x): 3.244074597315896e-06  grad at x: [ 0.00356923 -0.00048671]  gradient norm: 0.0036022629539309847\n",
            "iter: 5510  x: [-99.99821895  24.99975713]  f(x): 3.231111275205886e-06  grad at x: [ 0.00356209 -0.00048574]  gradient norm: 0.0035950584280124774\n",
            "iter: 5511  x: [-99.99822252  24.99975762]  f(x): 3.2181997545536166e-06  grad at x: [ 0.00355497 -0.00048477]  gradient norm: 0.0035878683111583774\n",
            "iter: 5512  x: [-99.99822607  24.9997581 ]  f(x): 3.2053398283409944e-06  grad at x: [ 0.00354786 -0.0004838 ]  gradient norm: 0.0035806925745397325\n",
            "iter: 5513  x: [-99.99822962  24.99975858]  f(x): 3.192531290378329e-06  grad at x: [ 0.00354076 -0.00048283]  gradient norm: 0.0035735311893858317\n",
            "iter: 5514  x: [-99.99823316  24.99975907]  f(x): 3.1797739353514416e-06  grad at x: [ 0.00353368 -0.00048187]  gradient norm: 0.0035663841270123673\n",
            "iter: 5515  x: [-99.99823669  24.99975955]  f(x): 3.167067558714411e-06  grad at x: [ 0.00352661 -0.0004809 ]  gradient norm: 0.003559251358763194\n",
            "iter: 5516  x: [-99.99824022  24.99976003]  f(x): 3.1544119567387406e-06  grad at x: [ 0.00351956 -0.00047994]  gradient norm: 0.0035521328560394476\n",
            "iter: 5517  x: [-99.99824374  24.99976051]  f(x): 3.1418069265619133e-06  grad at x: [ 0.00351252 -0.00047898]  gradient norm: 0.0035450285903286666\n",
            "iter: 5518  x: [-99.99824725  24.99976099]  f(x): 3.129252266080794e-06  grad at x: [ 0.0035055  -0.00047802]  gradient norm: 0.003537938533146552\n",
            "iter: 5519  x: [-99.99825076  24.99976147]  f(x): 3.116747774002209e-06  grad at x: [ 0.00349849 -0.00047707]  gradient norm: 0.0035308626560670462\n",
            "iter: 5520  x: [-99.99825426  24.99976194]  f(x): 3.104293249886131e-06  grad at x: [ 0.00349149 -0.00047611]  gradient norm: 0.003523800930748575\n",
            "iter: 5521  x: [-99.99825775  24.99976242]  f(x): 3.091888494048206e-06  grad at x: [ 0.00348451 -0.00047516]  gradient norm: 0.003516753328880606\n",
            "iter: 5522  x: [-99.99826123  24.9997629 ]  f(x): 3.0795333076493013e-06  grad at x: [ 0.00347754 -0.00047421]  gradient norm: 0.003509719822236129\n",
            "iter: 5523  x: [-99.99826471  24.99976337]  f(x): 3.067227492549129e-06  grad at x: [ 0.00347058 -0.00047326]  gradient norm: 0.0035027003825900548\n",
            "iter: 5524  x: [-99.99826818  24.99976384]  f(x): 3.0549708514994308e-06  grad at x: [ 0.00346364 -0.00047231]  gradient norm: 0.003495694981830898\n",
            "iter: 5525  x: [-99.99827164  24.99976432]  f(x): 3.0427631879943145e-06  grad at x: [ 0.00345671 -0.00047137]  gradient norm: 0.003488703591877255\n",
            "iter: 5526  x: [-99.9982751   24.99976479]  f(x): 3.0306043063117712e-06  grad at x: [ 0.0034498  -0.00047043]  gradient norm: 0.0034817261847030828\n",
            "iter: 5527  x: [-99.99827855  24.99976526]  f(x): 3.018494011517415e-06  grad at x: [ 0.0034429  -0.00046949]  gradient norm: 0.003474762732341542\n",
            "iter: 5528  x: [-99.99828199  24.99976573]  f(x): 3.006432109456507e-06  grad at x: [ 0.00343601 -0.00046855]  gradient norm: 0.0034678132068821166\n",
            "iter: 5529  x: [-99.99828543  24.99976619]  f(x): 2.9944184067526863e-06  grad at x: [ 0.00342914 -0.00046761]  gradient norm: 0.0034608775804715695\n",
            "iter: 5530  x: [-99.99828886  24.99976666]  f(x): 2.982452710806698e-06  grad at x: [ 0.00342228 -0.00046668]  gradient norm: 0.0034539558253149087\n",
            "iter: 5531  x: [-99.99829228  24.99976713]  f(x): 2.970534829790144e-06  grad at x: [ 0.00341544 -0.00046574]  gradient norm: 0.0034470479136734632\n",
            "iter: 5532  x: [-99.9982957   24.99976759]  f(x): 2.9586645725974487e-06  grad at x: [ 0.00340861 -0.00046481]  gradient norm: 0.0034401538178386436\n",
            "iter: 5533  x: [-99.9982991   24.99976806]  f(x): 2.9468417489851554e-06  grad at x: [ 0.00340179 -0.00046388]  gradient norm: 0.003433273510214504\n",
            "iter: 5534  x: [-99.99830251  24.99976852]  f(x): 2.935066169378387e-06  grad at x: [ 0.00339499 -0.00046295]  gradient norm: 0.0034264069632070192\n",
            "iter: 5535  x: [-99.9983059   24.99976899]  f(x): 2.923337644961929e-06  grad at x: [ 0.0033882  -0.00046203]  gradient norm: 0.0034195541492784867\n",
            "iter: 5536  x: [-99.99830929  24.99976945]  f(x): 2.911655987728697e-06  grad at x: [ 0.00338142 -0.0004611 ]  gradient norm: 0.003412715040977607\n",
            "iter: 5537  x: [-99.99831267  24.99976991]  f(x): 2.900021010380461e-06  grad at x: [ 0.00337466 -0.00046018]  gradient norm: 0.00340588961088316\n",
            "iter: 5538  x: [-99.99831605  24.99977037]  f(x): 2.8884325264146495e-06  grad at x: [ 0.00336791 -0.00045926]  gradient norm: 0.0033990778316564914\n",
            "iter: 5539  x: [-99.99831941  24.99977083]  f(x): 2.8768903500353122e-06  grad at x: [ 0.00336117 -0.00045834]  gradient norm: 0.003392279675990948\n",
            "iter: 5540  x: [-99.99832277  24.99977129]  f(x): 2.8653942961918824e-06  grad at x: [ 0.00335445 -0.00045743]  gradient norm: 0.003385495116636196\n",
            "iter: 5541  x: [-99.99832613  24.99977174]  f(x): 2.8539441805763645e-06  grad at x: [ 0.00334774 -0.00045651]  gradient norm: 0.0033787241263982266\n",
            "iter: 5542  x: [-99.99832948  24.9997722 ]  f(x): 2.842539819625389e-06  grad at x: [ 0.00334105 -0.0004556 ]  gradient norm: 0.0033719666781422317\n",
            "iter: 5543  x: [-99.99833282  24.99977266]  f(x): 2.8311810305109096e-06  grad at x: [ 0.00333436 -0.00045469]  gradient norm: 0.0033652227447887664\n",
            "iter: 5544  x: [-99.99833615  24.99977311]  f(x): 2.8198676310949824e-06  grad at x: [ 0.0033277  -0.00045378]  gradient norm: 0.0033584922992884662\n",
            "iter: 5545  x: [-99.99833948  24.99977357]  f(x): 2.808599440021823e-06  grad at x: [ 0.00332104 -0.00045287]  gradient norm: 0.003351775314678371\n",
            "iter: 5546  x: [-99.9983428   24.99977402]  f(x): 2.797376276662619e-06  grad at x: [ 0.0033144  -0.00045196]  gradient norm: 0.0033450717640508814\n",
            "iter: 5547  x: [-99.99834612  24.99977447]  f(x): 2.7861979610721916e-06  grad at x: [ 0.00330777 -0.00045106]  gradient norm: 0.0033383816205294395\n",
            "iter: 5548  x: [-99.99834942  24.99977492]  f(x): 2.775064314028793e-06  grad at x: [ 0.00330115 -0.00045016]  gradient norm: 0.0033317048572938106\n",
            "iter: 5549  x: [-99.99835272  24.99977537]  f(x): 2.763975157032957e-06  grad at x: [ 0.00329455 -0.00044926]  gradient norm: 0.0033250414475810416\n",
            "iter: 5550  x: [-99.99835602  24.99977582]  f(x): 2.752930312304759e-06  grad at x: [ 0.00328796 -0.00044836]  gradient norm: 0.003318391364685461\n",
            "iter: 5551  x: [-99.99835931  24.99977627]  f(x): 2.741929602781078e-06  grad at x: [ 0.00328139 -0.00044746]  gradient norm: 0.00331175458195868\n",
            "iter: 5552  x: [-99.99836259  24.99977672]  f(x): 2.730972852066334e-06  grad at x: [ 0.00327482 -0.00044657]  gradient norm: 0.0033051310727814317\n",
            "iter: 5553  x: [-99.99836586  24.99977716]  f(x): 2.7200598845711545e-06  grad at x: [ 0.00326827 -0.00044567]  gradient norm: 0.0032985208106490124\n",
            "iter: 5554  x: [-99.99836913  24.99977761]  f(x): 2.7091905252737963e-06  grad at x: [ 0.00326174 -0.00044478]  gradient norm: 0.003291923769028558\n",
            "iter: 5555  x: [-99.99837239  24.99977805]  f(x): 2.698364599951734e-06  grad at x: [ 0.00325521 -0.00044389]  gradient norm: 0.0032853399215008084\n",
            "iter: 5556  x: [-99.99837565  24.9997785 ]  f(x): 2.6875819349933712e-06  grad at x: [ 0.0032487  -0.00044301]  gradient norm: 0.0032787692416474637\n",
            "iter: 5557  x: [-99.9983789   24.99977894]  f(x): 2.676842357580582e-06  grad at x: [ 0.00324221 -0.00044212]  gradient norm: 0.0032722117031638294\n",
            "iter: 5558  x: [-99.99838214  24.99977938]  f(x): 2.6661456955027526e-06  grad at x: [ 0.00323572 -0.00044123]  gradient norm: 0.0032656672797471285\n",
            "iter: 5559  x: [-99.99838537  24.99977982]  f(x): 2.655491777289568e-06  grad at x: [ 0.00322925 -0.00044035]  gradient norm: 0.0032591359451790702\n",
            "iter: 5560  x: [-99.9983886   24.99978026]  f(x): 2.6448804321636705e-06  grad at x: [ 0.00322279 -0.00043947]  gradient norm: 0.0032526176732986436\n",
            "iter: 5561  x: [-99.99839183  24.9997807 ]  f(x): 2.6343114899466063e-06  grad at x: [ 0.00321635 -0.00043859]  gradient norm: 0.0032461124379458\n",
            "iter: 5562  x: [-99.99839504  24.99978114]  f(x): 2.6237847812411125e-06  grad at x: [ 0.00320991 -0.00043772]  gradient norm: 0.0032396202130750527\n",
            "iter: 5563  x: [-99.99839825  24.99978158]  f(x): 2.613300137242364e-06  grad at x: [ 0.00320349 -0.00043684]  gradient norm: 0.0032331409726409173\n",
            "iter: 5564  x: [-99.99840146  24.99978202]  f(x): 2.6028573898725757e-06  grad at x: [ 0.00319709 -0.00043597]  gradient norm: 0.0032266746906823907\n",
            "iter: 5565  x: [-99.99840465  24.99978245]  f(x): 2.5924563717387923e-06  grad at x: [ 0.00319069 -0.00043509]  gradient norm: 0.003220221341298633\n",
            "iter: 5566  x: [-99.99840784  24.99978289]  f(x): 2.582096916075734e-06  grad at x: [ 0.00318431 -0.00043422]  gradient norm: 0.003213780898615047\n",
            "iter: 5567  x: [-99.99841103  24.99978332]  f(x): 2.571778856795018e-06  grad at x: [ 0.00317794 -0.00043336]  gradient norm: 0.003207353336815274\n",
            "iter: 5568  x: [-99.99841421  24.99978376]  f(x): 2.561502028482556e-06  grad at x: [ 0.00317159 -0.00043249]  gradient norm: 0.0032009386301412005\n",
            "iter: 5569  x: [-99.99841738  24.99978419]  f(x): 2.5512662663913646e-06  grad at x: [ 0.00316524 -0.00043162]  gradient norm: 0.003194536752890074\n",
            "iter: 5570  x: [-99.99842054  24.99978462]  f(x): 2.5410714064002543e-06  grad at x: [ 0.00315891 -0.00043076]  gradient norm: 0.0031881476793901844\n",
            "iter: 5571  x: [-99.9984237   24.99978505]  f(x): 2.530917285051907e-06  grad at x: [ 0.0031526 -0.0004299]  gradient norm: 0.0031817713840261415\n",
            "iter: 5572  x: [-99.99842686  24.99978548]  f(x): 2.520803739596576e-06  grad at x: [ 0.00314629 -0.00042904]  gradient norm: 0.003175407841268001\n",
            "iter: 5573  x: [-99.99843     24.99978591]  f(x): 2.5107306078537047e-06  grad at x: [ 0.00314    -0.00042818]  gradient norm: 0.003169057025585816\n",
            "iter: 5574  x: [-99.99843314  24.99978634]  f(x): 2.5006977283484552e-06  grad at x: [ 0.00313372 -0.00042733]  gradient norm: 0.0031627189115370057\n",
            "iter: 5575  x: [-99.99843628  24.99978676]  f(x): 2.490704940215269e-06  grad at x: [ 0.00312745 -0.00042647]  gradient norm: 0.003156393473707148\n",
            "iter: 5576  x: [-99.9984394   24.99978719]  f(x): 2.4807520832845196e-06  grad at x: [ 0.00312119 -0.00042562]  gradient norm: 0.0031500806867663053\n",
            "iter: 5577  x: [-99.99844252  24.99978762]  f(x): 2.470838997951301e-06  grad at x: [ 0.00311495 -0.00042477]  gradient norm: 0.0031437805253874203\n",
            "iter: 5578  x: [-99.99844564  24.99978804]  f(x): 2.4609655253002906e-06  grad at x: [ 0.00310872 -0.00042392]  gradient norm: 0.003137492964326958\n",
            "iter: 5579  x: [-99.99844875  24.99978847]  f(x): 2.4511315070632604e-06  grad at x: [ 0.00310251 -0.00042307]  gradient norm: 0.003131217978399626\n",
            "iter: 5580  x: [-99.99845185  24.99978889]  f(x): 2.4413367855710782e-06  grad at x: [ 0.0030963  -0.00042222]  gradient norm: 0.0031249555424492544\n",
            "iter: 5581  x: [-99.99845495  24.99978931]  f(x): 2.4315812037955793e-06  grad at x: [ 0.00309011 -0.00042138]  gradient norm: 0.0031187056313769527\n",
            "iter: 5582  x: [-99.99845804  24.99978973]  f(x): 2.4218646053017808e-06  grad at x: [ 0.00308393 -0.00042054]  gradient norm: 0.003112468220111994\n",
            "iter: 5583  x: [-99.99846112  24.99979015]  f(x): 2.4121868343363287e-06  grad at x: [ 0.00307776 -0.00041969]  gradient norm: 0.003106243283670053\n",
            "iter: 5584  x: [-99.9984642   24.99979057]  f(x): 2.402547735735803e-06  grad at x: [ 0.0030716  -0.00041886]  gradient norm: 0.0031000307970959276\n",
            "iter: 5585  x: [-99.99846727  24.99979099]  f(x): 2.3929471549667866e-06  grad at x: [ 0.00306546 -0.00041802]  gradient norm: 0.0030938307354907357\n",
            "iter: 5586  x: [-99.99847034  24.99979141]  f(x): 2.3833849381249244e-06  grad at x: [ 0.00305933 -0.00041718]  gradient norm: 0.0030876430740128784\n",
            "iter: 5587  x: [-99.99847339  24.99979183]  f(x): 2.373860931890584e-06  grad at x: [ 0.00305321 -0.00041635]  gradient norm: 0.003081467787850838\n",
            "iter: 5588  x: [-99.99847645  24.99979224]  f(x): 2.3643749836105072e-06  grad at x: [ 0.0030471  -0.00041551]  gradient norm: 0.00307530485227758\n",
            "iter: 5589  x: [-99.99847949  24.99979266]  f(x): 2.3549269411682727e-06  grad at x: [ 0.00304101 -0.00041468]  gradient norm: 0.0030691542425679897\n",
            "iter: 5590  x: [-99.99848254  24.99979307]  f(x): 2.3455166531077227e-06  grad at x: [ 0.00303493 -0.00041385]  gradient norm: 0.003063015934080476\n",
            "iter: 5591  x: [-99.99848557  24.99979349]  f(x): 2.3361439685499423e-06  grad at x: [ 0.00302886 -0.00041303]  gradient norm: 0.0030568899022044888\n",
            "iter: 5592  x: [-99.9984886  24.9997939]  f(x): 2.3268087372298353e-06  grad at x: [ 0.0030228 -0.0004122]  gradient norm: 0.0030507761223858005\n",
            "iter: 5593  x: [-99.99849162  24.99979431]  f(x): 2.3175108094966772e-06  grad at x: [ 0.00301676 -0.00041138]  gradient norm: 0.0030446745701284248\n",
            "iter: 5594  x: [-99.99849464  24.99979472]  f(x): 2.308250036307353e-06  grad at x: [ 0.00301072 -0.00041055]  gradient norm: 0.0030385852209917387\n",
            "iter: 5595  x: [-99.99849765  24.99979513]  f(x): 2.2990262691429875e-06  grad at x: [ 0.0030047  -0.00040973]  gradient norm: 0.0030325080505370387\n",
            "iter: 5596  x: [-99.99850065  24.99979554]  f(x): 2.289839360174974e-06  grad at x: [ 0.00299869 -0.00040891]  gradient norm: 0.0030264430344382654\n",
            "iter: 5597  x: [-99.99850365  24.99979595]  f(x): 2.2806891620960204e-06  grad at x: [ 0.00299269 -0.00040809]  gradient norm: 0.00302039014837224\n",
            "iter: 5598  x: [-99.99850665  24.99979636]  f(x): 2.2715755281977294e-06  grad at x: [ 0.00298671 -0.00040728]  gradient norm: 0.003014349368071146\n",
            "iter: 5599  x: [-99.99850963  24.99979677]  f(x): 2.262498312372618e-06  grad at x: [ 0.00298073 -0.00040646]  gradient norm: 0.003008320669325408\n",
            "iter: 5600  x: [-99.99851261  24.99979717]  f(x): 2.253457369108898e-06  grad at x: [ 0.00297477 -0.00040565]  gradient norm: 0.0030023040279817752\n",
            "iter: 5601  x: [-99.99851559  24.99979758]  f(x): 2.244452553448849e-06  grad at x: [ 0.00296882 -0.00040484]  gradient norm: 0.002996299419917074\n",
            "iter: 5602  x: [-99.99851856  24.99979799]  f(x): 2.2354837210261557e-06  grad at x: [ 0.00296289 -0.00040403]  gradient norm: 0.0029903068210644576\n",
            "iter: 5603  x: [-99.99852152  24.99979839]  f(x): 2.226550728066474e-06  grad at x: [ 0.00295696 -0.00040322]  gradient norm: 0.002984326207415318\n",
            "iter: 5604  x: [-99.99852448  24.99979879]  f(x): 2.217653431338896e-06  grad at x: [ 0.00295105 -0.00040242]  gradient norm: 0.0029783575549882494\n",
            "iter: 5605  x: [-99.99852743  24.99979919]  f(x): 2.2087916882435197e-06  grad at x: [ 0.00294514 -0.00040161]  gradient norm: 0.0029724008398892094\n",
            "iter: 5606  x: [-99.99853037  24.9997996 ]  f(x): 2.19996535663737e-06  grad at x: [ 0.00293925 -0.00040081]  gradient norm: 0.002966456038195995\n",
            "iter: 5607  x: [-99.99853331  24.9998    ]  f(x): 2.191174295083504e-06  grad at x: [ 0.00293338 -0.00040001]  gradient norm: 0.0029605231261272077\n",
            "iter: 5608  x: [-99.99853625  24.9998004 ]  f(x): 2.182418362602146e-06  grad at x: [ 0.00292751 -0.00039921]  gradient norm: 0.002954602079876169\n",
            "iter: 5609  x: [-99.99853917  24.9998008 ]  f(x): 2.1736974188300616e-06  grad at x: [ 0.00292165 -0.00039841]  gradient norm: 0.0029486928757197226\n",
            "iter: 5610  x: [-99.99854209  24.99980119]  f(x): 2.165011323939282e-06  grad at x: [ 0.00291581 -0.00039761]  gradient norm: 0.0029427954899647933\n",
            "iter: 5611  x: [-99.99854501  24.99980159]  f(x): 2.1563599386737926e-06  grad at x: [ 0.00290998 -0.00039682]  gradient norm: 0.0029369098989746297\n",
            "iter: 5612  x: [-99.99854792  24.99980199]  f(x): 2.147743124350119e-06  grad at x: [ 0.00290416 -0.00039602]  gradient norm: 0.0029310360791707213\n",
            "iter: 5613  x: [-99.99855082  24.99980239]  f(x): 2.139160742811085e-06  grad at x: [ 0.00289835 -0.00039523]  gradient norm: 0.0029251740070027187\n",
            "iter: 5614  x: [-99.99855372  24.99980278]  f(x): 2.1306126564679266e-06  grad at x: [ 0.00289255 -0.00039444]  gradient norm: 0.002919323658978515\n",
            "iter: 5615  x: [-99.99855662  24.99980317]  f(x): 2.1220987282938574e-06  grad at x: [ 0.00288677 -0.00039365]  gradient norm: 0.002913485011661366\n",
            "iter: 5616  x: [-99.9985595   24.99980357]  f(x): 2.113618821786534e-06  grad at x: [ 0.002881   -0.00039286]  gradient norm: 0.0029076580416455672\n",
            "iter: 5617  x: [-99.99856238  24.99980396]  f(x): 2.10517280096066e-06  grad at x: [ 0.00287523 -0.00039208]  gradient norm: 0.002901842725552617\n",
            "iter: 5618  x: [-99.99856526  24.99980435]  f(x): 2.096760530431954e-06  grad at x: [ 0.00286948 -0.00039129]  gradient norm: 0.002896039040090416\n",
            "iter: 5619  x: [-99.99856813  24.99980474]  f(x): 2.0883818753317555e-06  grad at x: [ 0.00286374 -0.00039051]  gradient norm: 0.002890246961995985\n",
            "iter: 5620  x: [-99.99857099  24.99980514]  f(x): 2.080036701345868e-06  grad at x: [ 0.00285802 -0.00038973]  gradient norm: 0.00288446646806363\n",
            "iter: 5621  x: [-99.99857385  24.99980552]  f(x): 2.071724874670468e-06  grad at x: [ 0.0028523  -0.00038895]  gradient norm: 0.0028786975351158153\n",
            "iter: 5622  x: [-99.9985767   24.99980591]  f(x): 2.0634462620521865e-06  grad at x: [ 0.0028466  -0.00038817]  gradient norm: 0.0028729401400322888\n",
            "iter: 5623  x: [-99.99857955  24.9998063 ]  f(x): 2.055200730785951e-06  grad at x: [ 0.0028409 -0.0003874]  gradient norm: 0.002867194259750079\n",
            "iter: 5624  x: [-99.99858239  24.99980669]  f(x): 2.046988148673913e-06  grad at x: [ 0.00283522 -0.00038662]  gradient norm: 0.002861459871236298\n",
            "iter: 5625  x: [-99.99858522  24.99980708]  f(x): 2.038808384019578e-06  grad at x: [ 0.00282955 -0.00038585]  gradient norm: 0.0028557369514852575\n",
            "iter: 5626  x: [-99.99858805  24.99980746]  f(x): 2.0306613057118283e-06  grad at x: [ 0.00282389 -0.00038508]  gradient norm: 0.002850025477578633\n",
            "iter: 5627  x: [-99.99859088  24.99980785]  f(x): 2.022546783136798e-06  grad at x: [ 0.00281824 -0.00038431]  gradient norm: 0.0028443254266252997\n",
            "iter: 5628  x: [-99.9985937   24.99980823]  f(x): 2.0144646861789026e-06  grad at x: [ 0.00281261 -0.00038354]  gradient norm: 0.0028386367757632556\n",
            "iter: 5629  x: [-99.99859651  24.99980861]  f(x): 2.006414885300259e-06  grad at x: [ 0.00280698 -0.00038277]  gradient norm: 0.0028329595022169017\n",
            "iter: 5630  x: [-99.99859932  24.999809  ]  f(x): 1.998397251416013e-06  grad at x: [ 0.00280137 -0.000382  ]  gradient norm: 0.0028272935832106385\n",
            "iter: 5631  x: [-99.99860212  24.99980938]  f(x): 1.990411656014949e-06  grad at x: [ 0.00279577 -0.00038124]  gradient norm: 0.0028216389960552704\n",
            "iter: 5632  x: [-99.99860491  24.99980976]  f(x): 1.982457971033967e-06  grad at x: [ 0.00279017 -0.00038048]  gradient norm: 0.0028159957180606417\n",
            "iter: 5633  x: [-99.9986077   24.99981014]  f(x): 1.9745360689809267e-06  grad at x: [ 0.00278459 -0.00037972]  gradient norm: 0.002810363726623959\n",
            "iter: 5634  x: [-99.99861049  24.99981052]  f(x): 1.9666458228477647e-06  grad at x: [ 0.00277902 -0.00037896]  gradient norm: 0.0028047429991696314\n",
            "iter: 5635  x: [-99.99861327  24.9998109 ]  f(x): 1.9587871061522853e-06  grad at x: [ 0.00277347 -0.0003782 ]  gradient norm: 0.0027991335131803095\n",
            "iter: 5636  x: [-99.99861604  24.99981128]  f(x): 1.9509597928940576e-06  grad at x: [ 0.00276792 -0.00037744]  gradient norm: 0.0027935352461668046\n",
            "iter: 5637  x: [-99.99861881  24.99981166]  f(x): 1.9431637575554355e-06  grad at x: [ 0.00276238 -0.00037669]  gradient norm: 0.002787948175670011\n",
            "iter: 5638  x: [-99.99862157  24.99981203]  f(x): 1.93539887517556e-06  grad at x: [ 0.00275686 -0.00037594]  gradient norm: 0.002782372279315304\n",
            "iter: 5639  x: [-99.99862433  24.99981241]  f(x): 1.9276650212697303e-06  grad at x: [ 0.00275134 -0.00037518]  gradient norm: 0.002776807534756221\n",
            "iter: 5640  x: [-99.99862708  24.99981278]  f(x): 1.9199620718317554e-06  grad at x: [ 0.00274584 -0.00037443]  gradient norm: 0.0027712539196773404\n",
            "iter: 5641  x: [-99.99862982  24.99981316]  f(x): 1.912289903406183e-06  grad at x: [ 0.00274035 -0.00037368]  gradient norm: 0.002765711411847724\n",
            "iter: 5642  x: [-99.99863257  24.99981353]  f(x): 1.9046483929692928e-06  grad at x: [ 0.00273487 -0.00037294]  gradient norm: 0.0027601799890364345\n",
            "iter: 5643  x: [-99.9986353  24.9998139]  f(x): 1.897037418006762e-06  grad at x: [ 0.0027294  -0.00037219]  gradient norm: 0.0027546596290698145\n",
            "iter: 5644  x: [-99.99863803  24.99981428]  f(x): 1.8894568564742566e-06  grad at x: [ 0.00272394 -0.00037145]  gradient norm: 0.0027491503098042904\n",
            "iter: 5645  x: [-99.99864075  24.99981465]  f(x): 1.8819065868704192e-06  grad at x: [ 0.00271849 -0.0003707 ]  gradient norm: 0.00274365200918077\n",
            "iter: 5646  x: [-99.99864347  24.99981502]  f(x): 1.8743864881573822e-06  grad at x: [ 0.00271306 -0.00036996]  gradient norm: 0.002738164705168323\n",
            "iter: 5647  x: [-99.99864618  24.99981539]  f(x): 1.8668964397631032e-06  grad at x: [ 0.00270763 -0.00036922]  gradient norm: 0.00273268837576706\n",
            "iter: 5648  x: [-99.99864889  24.99981576]  f(x): 1.8594363215744921e-06  grad at x: [ 0.00270221 -0.00036848]  gradient norm: 0.0027272229990042926\n",
            "iter: 5649  x: [-99.99865159  24.99981613]  f(x): 1.8520060140164047e-06  grad at x: [ 0.00269681 -0.00036775]  gradient norm: 0.0027217685529937366\n",
            "iter: 5650  x: [-99.99865429  24.99981649]  f(x): 1.8446053979701592e-06  grad at x: [ 0.00269142 -0.00036701]  gradient norm: 0.0027163250158772673\n",
            "iter: 5651  x: [-99.99865698  24.99981686]  f(x): 1.8372343548114337e-06  grad at x: [ 0.00268603 -0.00036628]  gradient norm: 0.0027108923658540436\n",
            "iter: 5652  x: [-99.99865967  24.99981723]  f(x): 1.8298927663321075e-06  grad at x: [ 0.00268066 -0.00036554]  gradient norm: 0.002705470581124184\n",
            "iter: 5653  x: [-99.99866235  24.99981759]  f(x): 1.822580514853119e-06  grad at x: [ 0.0026753  -0.00036481]  gradient norm: 0.00270005963997325\n",
            "iter: 5654  x: [-99.99866503  24.99981796]  f(x): 1.8152974831070083e-06  grad at x: [ 0.00266995 -0.00036408]  gradient norm: 0.0026946595206868034\n",
            "iter: 5655  x: [-99.99866769  24.99981832]  f(x): 1.8080435543529213e-06  grad at x: [ 0.00266461 -0.00036336]  gradient norm: 0.0026892702016368094\n",
            "iter: 5656  x: [-99.99867036  24.99981869]  f(x): 1.8008186122961172e-06  grad at x: [ 0.00265928 -0.00036263]  gradient norm: 0.0026838916612233936\n",
            "iter: 5657  x: [-99.99867302  24.99981905]  f(x): 1.7936225411254244e-06  grad at x: [ 0.00265396 -0.0003619 ]  gradient norm: 0.0026785238779039656\n",
            "iter: 5658  x: [-99.99867567  24.99981941]  f(x): 1.7864552254360304e-06  grad at x: [ 0.00264865 -0.00036118]  gradient norm: 0.0026731668301368926\n",
            "iter: 5659  x: [-99.99867832  24.99981977]  f(x): 1.7793165503410059e-06  grad at x: [ 0.00264336 -0.00036046]  gradient norm: 0.002667820496465987\n",
            "iter: 5660  x: [-99.99868096  24.99982013]  f(x): 1.7722064013940298e-06  grad at x: [ 0.00263807 -0.00035974]  gradient norm: 0.0026624848554641805\n",
            "iter: 5661  x: [-99.9986836   24.99982049]  f(x): 1.765124664622718e-06  grad at x: [ 0.00263279 -0.00035902]  gradient norm: 0.00265715988575977\n",
            "iter: 5662  x: [-99.99868624  24.99982085]  f(x): 1.7580712264571557e-06  grad at x: [ 0.00262753 -0.0003583 ]  gradient norm: 0.0026518455659839285\n",
            "iter: 5663  x: [-99.99868886  24.99982121]  f(x): 1.751045973836703e-06  grad at x: [ 0.00262227 -0.00035758]  gradient norm: 0.002646541874852316\n",
            "iter: 5664  x: [-99.99869149  24.99982157]  f(x): 1.7440487941346278e-06  grad at x: [ 0.00261703 -0.00035687]  gradient norm: 0.0026412487911097113\n",
            "iter: 5665  x: [-99.9986941   24.99982192]  f(x): 1.7370795751553255e-06  grad at x: [ 0.00261179 -0.00035615]  gradient norm: 0.002635966293529055\n",
            "iter: 5666  x: [-99.99869671  24.99982228]  f(x): 1.7301382051711277e-06  grad at x: [ 0.00260657 -0.00035544]  gradient norm: 0.0026306943609405694\n",
            "iter: 5667  x: [-99.99869932  24.99982263]  f(x): 1.7232245728847162e-06  grad at x: [ 0.00260136 -0.00035473]  gradient norm: 0.002625432972204559\n",
            "iter: 5668  x: [-99.99870192  24.99982299]  f(x): 1.7163385674988853e-06  grad at x: [ 0.00259616 -0.00035402]  gradient norm: 0.002620182106265811\n",
            "iter: 5669  x: [-99.99870452  24.99982334]  f(x): 1.7094800785682963e-06  grad at x: [ 0.00259096 -0.00035331]  gradient norm: 0.002614941742041911\n",
            "iter: 5670  x: [-99.99870711  24.9998237 ]  f(x): 1.7026489961812046e-06  grad at x: [ 0.00258578 -0.00035261]  gradient norm: 0.00260971185856309\n",
            "iter: 5671  x: [-99.9987097   24.99982405]  f(x): 1.6958452107747715e-06  grad at x: [ 0.00258061 -0.0003519 ]  gradient norm: 0.0026044924348323775\n",
            "iter: 5672  x: [-99.99871228  24.9998244 ]  f(x): 1.6890686133185647e-06  grad at x: [ 0.00257545 -0.0003512 ]  gradient norm: 0.002599283449967367\n",
            "iter: 5673  x: [-99.99871485  24.99982475]  f(x): 1.6823190951268498e-06  grad at x: [ 0.0025703 -0.0003505]  gradient norm: 0.0025940848830574917\n",
            "iter: 5674  x: [-99.99871742  24.9998251 ]  f(x): 1.6755965480049247e-06  grad at x: [ 0.00256516 -0.00034979]  gradient norm: 0.002588896713277627\n",
            "iter: 5675  x: [-99.99871999  24.99982545]  f(x): 1.6689008642093025e-06  grad at x: [ 0.00256003 -0.00034909]  gradient norm: 0.0025837189198589716\n",
            "iter: 5676  x: [-99.99872255  24.9998258 ]  f(x): 1.6622319363394569e-06  grad at x: [ 0.00255491 -0.0003484 ]  gradient norm: 0.0025785514820064826\n",
            "iter: 5677  x: [-99.9987251   24.99982615]  f(x): 1.6555896575145513e-06  grad at x: [ 0.0025498 -0.0003477]  gradient norm: 0.0025733943790368016\n",
            "iter: 5678  x: [-99.99872765  24.9998265 ]  f(x): 1.6489739212312092e-06  grad at x: [ 0.0025447 -0.000347 ]  gradient norm: 0.00256824759026945\n",
            "iter: 5679  x: [-99.9987302   24.99982684]  f(x): 1.6423846214296676e-06  grad at x: [ 0.00253961 -0.00034631]  gradient norm: 0.002563111095079312\n",
            "iter: 5680  x: [-99.99873274  24.99982719]  f(x): 1.635821652495685e-06  grad at x: [ 0.00253453 -0.00034562]  gradient norm: 0.0025579848728995136\n",
            "iter: 5681  x: [-99.99873527  24.99982754]  f(x): 1.6292849091856205e-06  grad at x: [ 0.00252946 -0.00034493]  gradient norm: 0.0025528689031641405\n",
            "iter: 5682  x: [-99.9987378   24.99982788]  f(x): 1.6227742866946662e-06  grad at x: [ 0.0025244  -0.00034424]  gradient norm: 0.0025477631653626413\n",
            "iter: 5683  x: [-99.99874032  24.99982823]  f(x): 1.6162896806587523e-06  grad at x: [ 0.00251935 -0.00034355]  gradient norm: 0.002542667639042706\n",
            "iter: 5684  x: [-99.99874284  24.99982857]  f(x): 1.6098309870800773e-06  grad at x: [ 0.00251431 -0.00034286]  gradient norm: 0.0025375823037529855\n",
            "iter: 5685  x: [-99.99874536  24.99982891]  f(x): 1.6033981024686911e-06  grad at x: [ 0.00250928 -0.00034218]  gradient norm: 0.002532507139155735\n",
            "iter: 5686  x: [-99.99874787  24.99982925]  f(x): 1.5969909236596476e-06  grad at x: [ 0.00250427 -0.00034149]  gradient norm: 0.0025274421248840874\n",
            "iter: 5687  x: [-99.99875037  24.9998296 ]  f(x): 1.5906093479237345e-06  grad at x: [ 0.00249926 -0.00034081]  gradient norm: 0.002522387240630379\n",
            "iter: 5688  x: [-99.99875287  24.99982994]  f(x): 1.58425327296086e-06  grad at x: [ 0.00249426 -0.00034013]  gradient norm: 0.0025173424661423085\n",
            "iter: 5689  x: [-99.99875536  24.99983028]  f(x): 1.5779225968665708e-06  grad at x: [ 0.00248927 -0.00033945]  gradient norm: 0.002512307781197655\n",
            "iter: 5690  x: [-99.99875785  24.99983062]  f(x): 1.5716172181635497e-06  grad at x: [ 0.00248429 -0.00033877]  gradient norm: 0.0025072831656305196\n",
            "iter: 5691  x: [-99.99876034  24.99983096]  f(x): 1.5653370357658572e-06  grad at x: [ 0.00247932 -0.00033809]  gradient norm: 0.002502268599304125\n",
            "iter: 5692  x: [-99.99876282  24.99983129]  f(x): 1.5590819489775529e-06  grad at x: [ 0.00247436 -0.00033741]  gradient norm: 0.0024972640621108155\n",
            "iter: 5693  x: [-99.99876529  24.99983163]  f(x): 1.5528518575252182e-06  grad at x: [ 0.00246942 -0.00033674]  gradient norm: 0.002492269533999257\n",
            "iter: 5694  x: [-99.99876776  24.99983197]  f(x): 1.5466466614885895e-06  grad at x: [ 0.00246448 -0.00033607]  gradient norm: 0.002487284994920035\n",
            "iter: 5695  x: [-99.99877023  24.9998323 ]  f(x): 1.540466261436958e-06  grad at x: [ 0.00245955 -0.00033539]  gradient norm: 0.00248231042493638\n",
            "iter: 5696  x: [-99.99877269  24.99983264]  f(x): 1.5343105582535486e-06  grad at x: [ 0.00245463 -0.00033472]  gradient norm: 0.002477345804084322\n",
            "iter: 5697  x: [-99.99877514  24.99983297]  f(x): 1.5281794532729277e-06  grad at x: [ 0.00244972 -0.00033405]  gradient norm: 0.002472391112484372\n",
            "iter: 5698  x: [-99.99877759  24.99983331]  f(x): 1.5220728481771076e-06  grad at x: [ 0.00244482 -0.00033338]  gradient norm: 0.0024674463302589646\n",
            "iter: 5699  x: [-99.99878003  24.99983364]  f(x): 1.5159906450615175e-06  grad at x: [ 0.00243993 -0.00033272]  gradient norm: 0.002462511437586853\n",
            "iter: 5700  x: [-99.99878247  24.99983397]  f(x): 1.5099327464345077e-06  grad at x: [ 0.00243505 -0.00033205]  gradient norm: 0.0024575864147040752\n",
            "iter: 5701  x: [-99.99878491  24.99983431]  f(x): 1.5038990551799538e-06  grad at x: [ 0.00243018 -0.00033139]  gradient norm: 0.0024526712418748288\n",
            "iter: 5702  x: [-99.99878734  24.99983464]  f(x): 1.4978894745582873e-06  grad at x: [ 0.00242532 -0.00033073]  gradient norm: 0.0024477658993933937\n",
            "iter: 5703  x: [-99.99878977  24.99983497]  f(x): 1.49190390820281e-06  grad at x: [ 0.00242047 -0.00033006]  gradient norm: 0.0024428703675822097\n",
            "iter: 5704  x: [-99.99879219  24.9998353 ]  f(x): 1.48594226018821e-06  grad at x: [ 0.00241563 -0.0003294 ]  gradient norm: 0.0024379846268491605\n",
            "iter: 5705  x: [-99.9987946   24.99983563]  f(x): 1.4800044349246097e-06  grad at x: [ 0.0024108  -0.00032875]  gradient norm: 0.00243310865760213\n",
            "iter: 5706  x: [-99.99879701  24.99983596]  f(x): 1.4740903371931286e-06  grad at x: [ 0.00240598 -0.00032809]  gradient norm: 0.0024282424402790825\n",
            "iter: 5707  x: [-99.99879942  24.99983628]  f(x): 1.4681998722104821e-06  grad at x: [ 0.00240116 -0.00032743]  gradient norm: 0.0024233859554024673\n",
            "iter: 5708  x: [-99.99880182  24.99983661]  f(x): 1.462332945526001e-06  grad at x: [ 0.00239636 -0.00032678]  gradient norm: 0.0024185391834956912\n",
            "iter: 5709  x: [-99.99880422  24.99983694]  f(x): 1.4564894630886472e-06  grad at x: [ 0.00239157 -0.00032612]  gradient norm: 0.002413702105139445\n",
            "iter: 5710  x: [-99.99880661  24.99983726]  f(x): 1.4506693312103017e-06  grad at x: [ 0.00238679 -0.00032547]  gradient norm: 0.0024088747009425805\n",
            "iter: 5711  x: [-99.99880899  24.99983759]  f(x): 1.4448724565656397e-06  grad at x: [ 0.00238201 -0.00032482]  gradient norm: 0.00240405695154307\n",
            "iter: 5712  x: [-99.99881138  24.99983791]  f(x): 1.439098746223474e-06  grad at x: [ 0.00237725 -0.00032417]  gradient norm: 0.002399248837635208\n",
            "iter: 5713  x: [-99.99881375  24.99983824]  f(x): 1.433348107648595e-06  grad at x: [ 0.00237249 -0.00032352]  gradient norm: 0.0023944503399724914\n",
            "iter: 5714  x: [-99.99881613  24.99983856]  f(x): 1.427620448593453e-06  grad at x: [ 0.00236775 -0.00032287]  gradient norm: 0.0023896614392783368\n",
            "iter: 5715  x: [-99.99881849  24.99983889]  f(x): 1.421915677271219e-06  grad at x: [ 0.00236301 -0.00032223]  gradient norm: 0.0023848821163916837\n",
            "iter: 5716  x: [-99.99882086  24.99983921]  f(x): 1.4162337022148617e-06  grad at x: [ 0.00235829 -0.00032158]  gradient norm: 0.002380112352150513\n",
            "iter: 5717  x: [-99.99882321  24.99983953]  f(x): 1.4105744323465533e-06  grad at x: [ 0.00235357 -0.00032094]  gradient norm: 0.0023753521274510465\n",
            "iter: 5718  x: [-99.99882557  24.99983985]  f(x): 1.4049377769070295e-06  grad at x: [ 0.00234886 -0.0003203 ]  gradient norm: 0.0023706014231895073\n",
            "iter: 5719  x: [-99.99882792  24.99984017]  f(x): 1.3993236455557527e-06  grad at x: [ 0.00234417 -0.00031966]  gradient norm: 0.0023658602203475614\n",
            "iter: 5720  x: [-99.99883026  24.99984049]  f(x): 1.3937319482692544e-06  grad at x: [ 0.00233948 -0.00031902]  gradient norm: 0.002361128499907834\n",
            "iter: 5721  x: [-99.9988326   24.99984081]  f(x): 1.3881625954054493e-06  grad at x: [ 0.0023348  -0.00031838]  gradient norm: 0.002356406242909273\n",
            "iter: 5722  x: [-99.99883494  24.99984113]  f(x): 1.3826154976700864e-06  grad at x: [ 0.00233013 -0.00031774]  gradient norm: 0.0023516934304199486\n",
            "iter: 5723  x: [-99.99883727  24.99984145]  f(x): 1.3770905661485581e-06  grad at x: [ 0.00232547 -0.00031711]  gradient norm: 0.0023469900435652115\n",
            "iter: 5724  x: [-99.99883959  24.99984176]  f(x): 1.371587712237248e-06  grad at x: [ 0.00232082 -0.00031648]  gradient norm: 0.0023422960634704127\n",
            "iter: 5725  x: [-99.99884191  24.99984208]  f(x): 1.3661068477436313e-06  grad at x: [ 0.00231618 -0.00031584]  gradient norm: 0.002337611471347308\n",
            "iter: 5726  x: [-99.99884423  24.99984239]  f(x): 1.360647884782471e-06  grad at x: [ 0.00231154 -0.00031521]  gradient norm: 0.0023329362484066905\n",
            "iter: 5727  x: [-99.99884654  24.99984271]  f(x): 1.3552107358449863e-06  grad at x: [ 0.00230692 -0.00031458]  gradient norm: 0.0023282703759185583\n",
            "iter: 5728  x: [-99.99884885  24.99984302]  f(x): 1.349795313760104e-06  grad at x: [ 0.00230231 -0.00031395]  gradient norm: 0.0023236138351801095\n",
            "iter: 5729  x: [-99.99885115  24.99984334]  f(x): 1.3444015316943808e-06  grad at x: [ 0.0022977  -0.00031332]  gradient norm: 0.0023189666075167023\n",
            "iter: 5730  x: [-99.99885345  24.99984365]  f(x): 1.339029303186725e-06  grad at x: [ 0.00229311 -0.0003127 ]  gradient norm: 0.002314328674312899\n",
            "iter: 5731  x: [-99.99885574  24.99984396]  f(x): 1.3336785420773639e-06  grad at x: [ 0.00228852 -0.00031207]  gradient norm: 0.0023097000169523003\n",
            "iter: 5732  x: [-99.99885803  24.99984428]  f(x): 1.328349162607681e-06  grad at x: [ 0.00228394 -0.00031145]  gradient norm: 0.002305080616904911\n",
            "iter: 5733  x: [-99.99886031  24.99984459]  f(x): 1.3230410793502612e-06  grad at x: [ 0.00227938 -0.00031082]  gradient norm: 0.0023004704556679367\n",
            "iter: 5734  x: [-99.99886259  24.9998449 ]  f(x): 1.3177542072110259e-06  grad at x: [ 0.00227482 -0.0003102 ]  gradient norm: 0.0022958695147686646\n",
            "iter: 5735  x: [-99.99886487  24.99984521]  f(x): 1.3124884613935738e-06  grad at x: [ 0.00227027 -0.00030958]  gradient norm: 0.0022912777757343817\n",
            "iter: 5736  x: [-99.99886714  24.99984552]  f(x): 1.307243757497117e-06  grad at x: [ 0.00226573 -0.00030896]  gradient norm: 0.0022866952201787776\n",
            "iter: 5737  x: [-99.9988694   24.99984583]  f(x): 1.3020200114470924e-06  grad at x: [ 0.0022612  -0.00030834]  gradient norm: 0.002282121829742744\n",
            "iter: 5738  x: [-99.99887166  24.99984614]  f(x): 1.2968171394961938e-06  grad at x: [ 0.00225667 -0.00030773]  gradient norm: 0.0022775575860962934\n",
            "iter: 5739  x: [-99.99887392  24.99984644]  f(x): 1.2916350581922847e-06  grad at x: [ 0.00225216 -0.00030711]  gradient norm: 0.002273002470911358\n",
            "iter: 5740  x: [-99.99887617  24.99984675]  f(x): 1.2864736845020385e-06  grad at x: [ 0.00224766 -0.0003065 ]  gradient norm: 0.002268456465971554\n",
            "iter: 5741  x: [-99.99887842  24.99984706]  f(x): 1.2813329356537962e-06  grad at x: [ 0.00224316 -0.00030589]  gradient norm: 0.002263919553035219\n",
            "iter: 5742  x: [-99.99888066  24.99984736]  f(x): 1.2762127292588542e-06  grad at x: [ 0.00223867 -0.00030527]  gradient norm: 0.00225939171394325\n",
            "iter: 5743  x: [-99.9988829   24.99984767]  f(x): 1.271112983187034e-06  grad at x: [ 0.0022342  -0.00030466]  gradient norm: 0.002254872930510306\n",
            "iter: 5744  x: [-99.99888514  24.99984797]  f(x): 1.2660336156918193e-06  grad at x: [ 0.00222973 -0.00030405]  gradient norm: 0.0022503631846364884\n",
            "iter: 5745  x: [-99.99888737  24.99984828]  f(x): 1.260974545375872e-06  grad at x: [ 0.00222527 -0.00030345]  gradient norm: 0.0022458624582782196\n",
            "iter: 5746  x: [-99.99888959  24.99984858]  f(x): 1.2559356910959793e-06  grad at x: [ 0.00222082 -0.00030284]  gradient norm: 0.002241370733364723\n",
            "iter: 5747  x: [-99.99889181  24.99984888]  f(x): 1.2509169720885312e-06  grad at x: [ 0.00221638 -0.00030223]  gradient norm: 0.002236887991910664\n",
            "iter: 5748  x: [-99.99889403  24.99984919]  f(x): 1.2459183078723802e-06  grad at x: [ 0.00221194 -0.00030163]  gradient norm: 0.002232414215930709\n",
            "iter: 5749  x: [-99.99889624  24.99984949]  f(x): 1.2409396183118466e-06  grad at x: [ 0.00220752 -0.00030103]  gradient norm: 0.0022279493874968045\n",
            "iter: 5750  x: [-99.99889845  24.99984979]  f(x): 1.2359808235839643e-06  grad at x: [ 0.0022031  -0.00030042]  gradient norm: 0.0022234934887100204\n",
            "iter: 5751  x: [-99.99890065  24.99985009]  f(x): 1.2310418442075386e-06  grad at x: [ 0.0021987  -0.00029982]  gradient norm: 0.0022190465017277476\n",
            "iter: 5752  x: [-99.99890285  24.99985039]  f(x): 1.2261226010116017e-06  grad at x: [ 0.0021943  -0.00029922]  gradient norm: 0.002214608408736499\n",
            "iter: 5753  x: [-99.99890504  24.99985069]  f(x): 1.2212230151031756e-06  grad at x: [ 0.00218991 -0.00029862]  gradient norm: 0.002210179191923746\n",
            "iter: 5754  x: [-99.99890723  24.99985099]  f(x): 1.216343007928587e-06  grad at x: [ 0.00218553 -0.00029803]  gradient norm: 0.0022057588335342437\n",
            "iter: 5755  x: [-99.99890942  24.99985128]  f(x): 1.2114825012699322e-06  grad at x: [ 0.00218116 -0.00029743]  gradient norm: 0.002201347315868109\n",
            "iter: 5756  x: [-99.9989116   24.99985158]  f(x): 1.206641417186032e-06  grad at x: [ 0.0021768  -0.00029684]  gradient norm: 0.0021969446212283385\n",
            "iter: 5757  x: [-99.99891378  24.99985188]  f(x): 1.2018196780691538e-06  grad at x: [ 0.00217245 -0.00029624]  gradient norm: 0.002192550731973291\n",
            "iter: 5758  x: [-99.99891595  24.99985217]  f(x): 1.1970172066446745e-06  grad at x: [ 0.0021681  -0.00029565]  gradient norm: 0.0021881656305176487\n",
            "iter: 5759  x: [-99.99891812  24.99985247]  f(x): 1.1922339258805862e-06  grad at x: [ 0.00216376 -0.00029506]  gradient norm: 0.00218378929925081\n",
            "iter: 5760  x: [-99.99892028  24.99985277]  f(x): 1.1874697591055655e-06  grad at x: [ 0.00215944 -0.00029447]  gradient norm: 0.0021794217206456998\n",
            "iter: 5761  x: [-99.99892244  24.99985306]  f(x): 1.1827246299491728e-06  grad at x: [ 0.00215512 -0.00029388]  gradient norm: 0.0021750628772053216\n",
            "iter: 5762  x: [-99.9989246   24.99985335]  f(x): 1.1779984623376323e-06  grad at x: [ 0.00215081 -0.00029329]  gradient norm: 0.0021707127514598817\n",
            "iter: 5763  x: [-99.99892675  24.99985365]  f(x): 1.1732911804948526e-06  grad at x: [ 0.00214651 -0.00029271]  gradient norm: 0.0021663713259687063\n",
            "iter: 5764  x: [-99.99892889  24.99985394]  f(x): 1.1686027089423892e-06  grad at x: [ 0.00214221 -0.00029212]  gradient norm: 0.0021620385833212036\n",
            "iter: 5765  x: [-99.99893104  24.99985423]  f(x): 1.1639329725266715e-06  grad at x: [ 0.00213793 -0.00029154]  gradient norm: 0.0021577145061631034\n",
            "iter: 5766  x: [-99.99893317  24.99985452]  f(x): 1.159281896355964e-06  grad at x: [ 0.00213365 -0.00029095]  gradient norm: 0.0021533990771391764\n",
            "iter: 5767  x: [-99.99893531  24.99985481]  f(x): 1.1546494058945334e-06  grad at x: [ 0.00212939 -0.00029037]  gradient norm: 0.002149092278981555\n",
            "iter: 5768  x: [-99.99893744  24.9998551 ]  f(x): 1.150035426867272e-06  grad at x: [ 0.00212513 -0.00028979]  gradient norm: 0.0021447940944223733\n",
            "iter: 5769  x: [-99.99893956  24.99985539]  f(x): 1.1454398852890922e-06  grad at x: [ 0.00212088 -0.00028921]  gradient norm: 0.0021405045062219256\n",
            "iter: 5770  x: [-99.99894168  24.99985568]  f(x): 1.1408627074960126e-06  grad at x: [ 0.00211663 -0.00028863]  gradient norm: 0.0021362234971987483\n",
            "iter: 5771  x: [-99.9989438   24.99985597]  f(x): 1.1363038201107067e-06  grad at x: [ 0.0021124  -0.00028805]  gradient norm: 0.0021319510501985797\n",
            "iter: 5772  x: [-99.99894591  24.99985626]  f(x): 1.1317631500435187e-06  grad at x: [ 0.00210818 -0.00028748]  gradient norm: 0.0021276871480962784\n",
            "iter: 5773  x: [-99.99894802  24.99985655]  f(x): 1.1272406244924397e-06  grad at x: [ 0.00210396 -0.0002869 ]  gradient norm: 0.0021234317737967847\n",
            "iter: 5774  x: [-99.99895012  24.99985684]  f(x): 1.1227361709688456e-06  grad at x: [ 0.00209975 -0.00028633]  gradient norm: 0.0021191849102604006\n",
            "iter: 5775  x: [-99.99895222  24.99985712]  f(x): 1.1182497172386515e-06  grad at x: [ 0.00209555 -0.00028576]  gradient norm: 0.0021149465404483882\n",
            "iter: 5776  x: [-99.99895432  24.99985741]  f(x): 1.1137811913810199e-06  grad at x: [ 0.00209136 -0.00028519]  gradient norm: 0.0021107166473792924\n",
            "iter: 5777  x: [-99.99895641  24.99985769]  f(x): 1.1093305217277101e-06  grad at x: [ 0.00208718 -0.00028462]  gradient norm: 0.002106495214072617\n",
            "iter: 5778  x: [-99.9989585   24.99985798]  f(x): 1.104897636951154e-06  grad at x: [ 0.002083   -0.00028405]  gradient norm: 0.0021022822236333102\n",
            "iter: 5779  x: [-99.99896058  24.99985826]  f(x): 1.1004824660017415e-06  grad at x: [ 0.00207884 -0.00028348]  gradient norm: 0.0020980776591935212\n",
            "iter: 5780  x: [-99.99896266  24.99985854]  f(x): 1.0960849380803544e-06  grad at x: [ 0.00207468 -0.00028291]  gradient norm: 0.0020938815038873183\n",
            "iter: 5781  x: [-99.99896473  24.99985883]  f(x): 1.0917049826640532e-06  grad at x: [ 0.00207053 -0.00028235]  gradient norm: 0.002089693740875972\n",
            "iter: 5782  x: [-99.9989668   24.99985911]  f(x): 1.0873425295384516e-06  grad at x: [ 0.00206639 -0.00028178]  gradient norm: 0.002085514353379954\n",
            "iter: 5783  x: [-99.99896887  24.99985939]  f(x): 1.082997508792398e-06  grad at x: [ 0.00206226 -0.00028122]  gradient norm: 0.0020813433246750983\n",
            "iter: 5784  x: [-99.99897093  24.99985967]  f(x): 1.0786698507611934e-06  grad at x: [ 0.00205813 -0.00028065]  gradient norm: 0.0020771806380391602\n",
            "iter: 5785  x: [-99.99897299  24.99985995]  f(x): 1.0743594860520769e-06  grad at x: [ 0.00205402 -0.00028009]  gradient norm: 0.0020730262767770955\n",
            "iter: 5786  x: [-99.99897505  24.99986023]  f(x): 1.0700663455452323e-06  grad at x: [ 0.00204991 -0.00027953]  gradient norm: 0.0020688802242229804\n",
            "iter: 5787  x: [-99.9989771   24.99986051]  f(x): 1.0657903604228573e-06  grad at x: [ 0.00204581 -0.00027897]  gradient norm: 0.002064742463769133\n",
            "iter: 5788  x: [-99.99897914  24.99986079]  f(x): 1.0615314621358894e-06  grad at x: [ 0.00204172 -0.00027842]  gradient norm: 0.002060612978835074\n",
            "iter: 5789  x: [-99.99898118  24.99986107]  f(x): 1.0572895824050102e-06  grad at x: [ 0.00203763 -0.00027786]  gradient norm: 0.0020564917528694444\n",
            "iter: 5790  x: [-99.99898322  24.99986135]  f(x): 1.0530646532206471e-06  grad at x: [ 0.00203356 -0.0002773 ]  gradient norm: 0.0020523787693509667\n",
            "iter: 5791  x: [-99.99898525  24.99986163]  f(x): 1.0488566068668893e-06  grad at x: [ 0.00202949 -0.00027675]  gradient norm: 0.0020482740118127644\n",
            "iter: 5792  x: [-99.99898728  24.9998619 ]  f(x): 1.0446653758675907e-06  grad at x: [ 0.00202543 -0.0002762 ]  gradient norm: 0.002044177463790843\n",
            "iter: 5793  x: [-99.99898931  24.99986218]  f(x): 1.040490893011451e-06  grad at x: [ 0.00202138 -0.00027564]  gradient norm: 0.0020400891088493667\n",
            "iter: 5794  x: [-99.99899133  24.99986245]  f(x): 1.0363330914083904e-06  grad at x: [ 0.00201734 -0.00027509]  gradient norm: 0.0020360089306369856\n",
            "iter: 5795  x: [-99.99899335  24.99986273]  f(x): 1.0321919043755237e-06  grad at x: [ 0.0020133  -0.00027454]  gradient norm: 0.0020319369127761067\n",
            "iter: 5796  x: [-99.99899536  24.999863  ]  f(x): 1.0280672655185264e-06  grad at x: [ 0.00200928 -0.00027399]  gradient norm: 0.0020278730389435393\n",
            "iter: 5797  x: [-99.99899737  24.99986328]  f(x): 1.0239591087362452e-06  grad at x: [ 0.00200526 -0.00027344]  gradient norm: 0.002023817292876257\n",
            "iter: 5798  x: [-99.99899938  24.99986355]  f(x): 1.0198673681282882e-06  grad at x: [ 0.00200125 -0.0002729 ]  gradient norm: 0.00201976965828115\n",
            "iter: 5799  x: [-99.99900138  24.99986382]  f(x): 1.015791978111102e-06  grad at x: [ 0.00199725 -0.00027235]  gradient norm: 0.0020157301189505524\n",
            "iter: 5800  x: [-99.99900337  24.9998641 ]  f(x): 1.0117328733607652e-06  grad at x: [ 0.00199325 -0.00027181]  gradient norm: 0.0020116986587068804\n",
            "iter: 5801  x: [-99.99900537  24.99986437]  f(x): 1.0076899888091384e-06  grad at x: [ 0.00198927 -0.00027126]  gradient norm: 0.00200767526139975\n",
            "iter: 5802  x: [-99.99900736  24.99986464]  f(x): 1.0036632596176112e-06  grad at x: [ 0.00198529 -0.00027072]  gradient norm: 0.0020036599108806973\n",
            "iter: 5803  x: [-99.99900934  24.99986491]  f(x): 9.99652621230826e-07  grad at x: [ 0.00198132 -0.00027018]  gradient norm: 0.0019996525910575824\n",
            "iter: 5804  x: [-99.99901132  24.99986518]  f(x): 9.956580093473643e-07  grad at x: [ 0.00197735 -0.00026964]  gradient norm: 0.0019956532858664247\n",
            "iter: 5805  x: [-99.9990133   24.99986545]  f(x): 9.916793599478218e-07  grad at x: [ 0.0019734 -0.0002691]  gradient norm: 0.0019916619793005257\n",
            "iter: 5806  x: [-99.99901527  24.99986572]  f(x): 9.877166092385722e-07  grad at x: [ 0.00196945 -0.00026856]  gradient norm: 0.0019876786553551075\n",
            "iter: 5807  x: [-99.99901724  24.99986599]  f(x): 9.83769693676185e-07  grad at x: [ 0.00196551 -0.00026802]  gradient norm: 0.001983703298052594\n",
            "iter: 5808  x: [-99.99901921  24.99986626]  f(x): 9.798385499674663e-07  grad at x: [ 0.00196158 -0.00026749]  gradient norm: 0.0019797358914435694\n",
            "iter: 5809  x: [-99.99902117  24.99986652]  f(x): 9.759231151270307e-07  grad at x: [ 0.00195766 -0.00026695]  gradient norm: 0.001975776419665981\n",
            "iter: 5810  x: [-99.99902313  24.99986679]  f(x): 9.72023326360015e-07  grad at x: [ 0.00195374 -0.00026642]  gradient norm: 0.0019718248668276956\n",
            "iter: 5811  x: [-99.99902508  24.99986706]  f(x): 9.681391211495939e-07  grad at x: [ 0.00194984 -0.00026589]  gradient norm: 0.0019678812170957818\n",
            "iter: 5812  x: [-99.99902703  24.99986732]  f(x): 9.642704372243465e-07  grad at x: [ 0.00194594 -0.00026535]  gradient norm: 0.001963945454664509\n",
            "iter: 5813  x: [-99.99902898  24.99986759]  f(x): 9.604172125592454e-07  grad at x: [ 0.00194204 -0.00026482]  gradient norm: 0.0019600175637572693\n",
            "iter: 5814  x: [-99.99903092  24.99986785]  f(x): 9.565793853738085e-07  grad at x: [ 0.00193816 -0.00026429]  gradient norm: 0.0019560975286256137\n",
            "iter: 5815  x: [-99.99903286  24.99986812]  f(x): 9.527568941605669e-07  grad at x: [ 0.00193428 -0.00026377]  gradient norm: 0.001952185333579337\n",
            "iter: 5816  x: [-99.99903479  24.99986838]  f(x): 9.489496775997084e-07  grad at x: [ 0.00193042 -0.00026324]  gradient norm: 0.0019482809629000723\n",
            "iter: 5817  x: [-99.99903672  24.99986864]  f(x): 9.451576746956271e-07  grad at x: [ 0.00192655 -0.00026271]  gradient norm: 0.001944384400982097\n",
            "iter: 5818  x: [-99.99903865  24.99986891]  f(x): 9.413808246395308e-07  grad at x: [ 0.0019227  -0.00026219]  gradient norm: 0.001940495632192488\n",
            "iter: 5819  x: [-99.99904057  24.99986917]  f(x): 9.376190668627023e-07  grad at x: [ 0.00191886 -0.00026166]  gradient norm: 0.0019366146409264826\n",
            "iter: 5820  x: [-99.99904249  24.99986943]  f(x): 9.338723410646875e-07  grad at x: [ 0.00191502 -0.00026114]  gradient norm: 0.0019327414116375604\n",
            "iter: 5821  x: [-99.99904441  24.99986969]  f(x): 9.301405871831166e-07  grad at x: [ 0.00191119 -0.00026062]  gradient norm: 0.0019288759288073629\n",
            "iter: 5822  x: [-99.99904632  24.99986995]  f(x): 9.264237453928286e-07  grad at x: [ 0.00190737 -0.0002601 ]  gradient norm: 0.0019250181769456917\n",
            "iter: 5823  x: [-99.99904822  24.99987021]  f(x): 9.227217561059215e-07  grad at x: [ 0.00190355 -0.00025958]  gradient norm: 0.00192116814059147\n",
            "iter: 5824  x: [-99.99905013  24.99987047]  f(x): 9.190345599708758e-07  grad at x: [ 0.00189974 -0.00025906]  gradient norm: 0.0019173258043127422\n",
            "iter: 5825  x: [-99.99905203  24.99987073]  f(x): 9.153620978707601e-07  grad at x: [ 0.00189594 -0.00025854]  gradient norm: 0.0019134911527057135\n",
            "iter: 5826  x: [-99.99905392  24.99987099]  f(x): 9.117043109223677e-07  grad at x: [ 0.00189215 -0.00025802]  gradient norm: 0.0019096641703947505\n",
            "iter: 5827  x: [-99.99905582  24.99987125]  f(x): 9.080611405040226e-07  grad at x: [ 0.00188837 -0.0002575 ]  gradient norm: 0.001905844842062462\n",
            "iter: 5828  x: [-99.9990577   24.99987151]  f(x): 9.04432528199045e-07  grad at x: [ 0.00188459 -0.00025699]  gradient norm: 0.001902033152391456\n",
            "iter: 5829  x: [-99.99905959  24.99987176]  f(x): 9.00818415821895e-07  grad at x: [ 0.00188082 -0.00025648]  gradient norm: 0.0018982290860925033\n",
            "iter: 5830  x: [-99.99906147  24.99987202]  f(x): 8.972187454191382e-07  grad at x: [ 0.00187706 -0.00025596]  gradient norm: 0.001894432627906454\n",
            "iter: 5831  x: [-99.99906335  24.99987227]  f(x): 8.936334593191e-07  grad at x: [ 0.00187331 -0.00025545]  gradient norm: 0.0018906437626576827\n",
            "iter: 5832  x: [-99.99906522  24.99987253]  f(x): 8.900625000260339e-07  grad at x: [ 0.00186956 -0.00025494]  gradient norm: 0.0018868624751433624\n",
            "iter: 5833  x: [-99.99906709  24.99987278]  f(x): 8.865058102728336e-07  grad at x: [ 0.00186582 -0.00025443]  gradient norm: 0.001883088750189787\n",
            "iter: 5834  x: [-99.99906896  24.99987304]  f(x): 8.829633330466449e-07  grad at x: [ 0.00186209 -0.00025392]  gradient norm: 0.0018793225726805335\n",
            "iter: 5835  x: [-99.99907082  24.99987329]  f(x): 8.794350115595442e-07  grad at x: [ 0.00185837 -0.00025341]  gradient norm: 0.001875563927526379\n",
            "iter: 5836  x: [-99.99907268  24.99987355]  f(x): 8.7592078925041e-07  grad at x: [ 0.00185465 -0.00025291]  gradient norm: 0.0018718127996681826\n",
            "iter: 5837  x: [-99.99907453  24.9998738 ]  f(x): 8.724206097822792e-07  grad at x: [ 0.00185094 -0.0002524 ]  gradient norm: 0.0018680691740749636\n",
            "iter: 5838  x: [-99.99907638  24.99987405]  f(x): 8.689344170161634e-07  grad at x: [ 0.00184724 -0.0002519 ]  gradient norm: 0.0018643330357167019\n",
            "iter: 5839  x: [-99.99907823  24.9998743 ]  f(x): 8.654621550881782e-07  grad at x: [ 0.00184354 -0.00025139]  gradient norm: 0.0018606043696478606\n",
            "iter: 5840  x: [-99.99908007  24.99987456]  f(x): 8.620037683045031e-07  grad at x: [ 0.00183986 -0.00025089]  gradient norm: 0.0018568831608957016\n",
            "iter: 5841  x: [-99.99908191  24.99987481]  f(x): 8.585592012445608e-07  grad at x: [ 0.00183618 -0.00025039]  gradient norm: 0.0018531693945719704\n",
            "iter: 5842  x: [-99.99908375  24.99987506]  f(x): 8.551283986824404e-07  grad at x: [ 0.0018325  -0.00024989]  gradient norm: 0.001849463055789372\n",
            "iter: 5843  x: [-99.99908558  24.99987531]  f(x): 8.517113056123243e-07  grad at x: [ 0.00182884 -0.00024939]  gradient norm: 0.0018457641296897329\n",
            "iter: 5844  x: [-99.99908741  24.99987556]  f(x): 8.483078672467828e-07  grad at x: [ 0.00182518 -0.00024889]  gradient norm: 0.0018420726014430407\n",
            "iter: 5845  x: [-99.99908923  24.9998758 ]  f(x): 8.449180290159649e-07  grad at x: [ 0.00182153 -0.00024839]  gradient norm: 0.0018383884562474437\n",
            "iter: 5846  x: [-99.99909106  24.99987605]  f(x): 8.415417365685525e-07  grad at x: [ 0.00181789 -0.00024789]  gradient norm: 0.0018347116793311722\n",
            "iter: 5847  x: [-99.99909287  24.9998763 ]  f(x): 8.381789357949628e-07  grad at x: [ 0.00181425 -0.0002474 ]  gradient norm: 0.0018310422559787776\n",
            "iter: 5848  x: [-99.99909469  24.99987655]  f(x): 8.348295727739512e-07  grad at x: [ 0.00181062 -0.0002469 ]  gradient norm: 0.001827380171473852\n",
            "iter: 5849  x: [-99.9990965  24.9998768]  f(x): 8.314935938012597e-07  grad at x: [ 0.001807   -0.00024641]  gradient norm: 0.0018237254111310283\n",
            "iter: 5850  x: [-99.99909831  24.99987704]  f(x): 8.281709454109278e-07  grad at x: [ 0.00180339 -0.00024592]  gradient norm: 0.001820077960320302\n",
            "iter: 5851  x: [-99.99910011  24.99987729]  f(x): 8.248615743248425e-07  grad at x: [ 0.00179978 -0.00024542]  gradient norm: 0.0018164378044126285\n",
            "iter: 5852  x: [-99.99910191  24.99987753]  f(x): 8.215654274777204e-07  grad at x: [ 0.00179618 -0.00024493]  gradient norm: 0.0018128049288080837\n",
            "iter: 5853  x: [-99.99910371  24.99987778]  f(x): 8.182824520409162e-07  grad at x: [ 0.00179259 -0.00024444]  gradient norm: 0.0018091793189630664\n",
            "iter: 5854  x: [-99.9991055   24.99987802]  f(x): 8.150125953705367e-07  grad at x: [ 0.001789   -0.00024396]  gradient norm: 0.0018055609603339752\n",
            "iter: 5855  x: [-99.99910729  24.99987827]  f(x): 8.117558050340144e-07  grad at x: [ 0.00178543 -0.00024347]  gradient norm: 0.0018019498384072897\n",
            "iter: 5856  x: [-99.99910907  24.99987851]  f(x): 8.085120288329055e-07  grad at x: [ 0.00178186 -0.00024298]  gradient norm: 0.001798345938725812\n",
            "iter: 5857  x: [-99.99911085  24.99987875]  f(x): 8.052812147757258e-07  grad at x: [ 0.00177829 -0.00024249]  gradient norm: 0.0017947492468595447\n",
            "iter: 5858  x: [-99.99911263  24.999879  ]  f(x): 8.020633110301837e-07  grad at x: [ 0.00177474 -0.00024201]  gradient norm: 0.00179115974835321\n",
            "iter: 5859  x: [-99.99911441  24.99987924]  f(x): 7.98858266045305e-07  grad at x: [ 0.00177119 -0.00024153]  gradient norm: 0.0017875774288632143\n",
            "iter: 5860  x: [-99.99911618  24.99987948]  f(x): 7.956660284268854e-07  grad at x: [ 0.00176764 -0.00024104]  gradient norm: 0.0017840022740197227\n",
            "iter: 5861  x: [-99.99911795  24.99987972]  f(x): 7.92486546983931e-07  grad at x: [ 0.00176411 -0.00024056]  gradient norm: 0.0017804342694791415\n",
            "iter: 5862  x: [-99.99911971  24.99987996]  f(x): 7.89319770732175e-07  grad at x: [ 0.00176058 -0.00024008]  gradient norm: 0.001776873400928918\n",
            "iter: 5863  x: [-99.99912147  24.9998802 ]  f(x): 7.86165648939823e-07  grad at x: [ 0.00175706 -0.0002396 ]  gradient norm: 0.0017733196541400234\n",
            "iter: 5864  x: [-99.99912323  24.99988044]  f(x): 7.830241310042548e-07  grad at x: [ 0.00175354 -0.00023912]  gradient norm: 0.0017697730148290257\n",
            "iter: 5865  x: [-99.99912498  24.99988068]  f(x): 7.798951665738029e-07  grad at x: [ 0.00175004 -0.00023864]  gradient norm: 0.0017662334687960173\n",
            "iter: 5866  x: [-99.99912673  24.99988092]  f(x): 7.76778705498561e-07  grad at x: [ 0.00174654 -0.00023816]  gradient norm: 0.0017627010018702106\n",
            "iter: 5867  x: [-99.99912848  24.99988116]  f(x): 7.736746977809284e-07  grad at x: [ 0.00174304 -0.00023769]  gradient norm: 0.0017591755998545779\n",
            "iter: 5868  x: [-99.99913022  24.99988139]  f(x): 7.705830936955716e-07  grad at x: [ 0.00173956 -0.00023721]  gradient norm: 0.0017556572486628153\n",
            "iter: 5869  x: [-99.99913196  24.99988163]  f(x): 7.675038436432883e-07  grad at x: [ 0.00173608 -0.00023674]  gradient norm: 0.0017521459341542167\n",
            "iter: 5870  x: [-99.9991337   24.99988187]  f(x): 7.644368982732584e-07  grad at x: [ 0.00173261 -0.00023626]  gradient norm: 0.0017486416422735202\n",
            "iter: 5871  x: [-99.99913543  24.9998821 ]  f(x): 7.613822084318163e-07  grad at x: [ 0.00172914 -0.00023579]  gradient norm: 0.0017451443589936236\n",
            "iter: 5872  x: [-99.99913716  24.99988234]  f(x): 7.583397251371891e-07  grad at x: [ 0.00172568 -0.00023532]  gradient norm: 0.0017416540702874254\n",
            "iter: 5873  x: [-99.99913888  24.99988258]  f(x): 7.553093996043144e-07  grad at x: [ 0.00172223 -0.00023485]  gradient norm: 0.0017381707621569457\n",
            "iter: 5874  x: [-99.99914061  24.99988281]  f(x): 7.522911832440968e-07  grad at x: [ 0.00171879 -0.00023438]  gradient norm: 0.001734694420633325\n",
            "iter: 5875  x: [-99.99914233  24.99988304]  f(x): 7.492850276862128e-07  grad at x: [ 0.00171535 -0.00023391]  gradient norm: 0.001731225031804026\n",
            "iter: 5876  x: [-99.99914404  24.99988328]  f(x): 7.462908847051543e-07  grad at x: [ 0.00171192 -0.00023344]  gradient norm: 0.0017277625817283511\n",
            "iter: 5877  x: [-99.99914575  24.99988351]  f(x): 7.433087063179321e-07  grad at x: [ 0.0017085  -0.00023298]  gradient norm: 0.0017243070565510449\n",
            "iter: 5878  x: [-99.99914746  24.99988374]  f(x): 7.403384447343958e-07  grad at x: [ 0.00170508 -0.00023251]  gradient norm: 0.001720858442445974\n",
            "iter: 5879  x: [-99.99914917  24.99988398]  f(x): 7.373800523073147e-07  grad at x: [ 0.00170167 -0.00023205]  gradient norm: 0.001717416725558843\n",
            "iter: 5880  x: [-99.99915087  24.99988421]  f(x): 7.344334816295004e-07  grad at x: [ 0.00169826 -0.00023158]  gradient norm: 0.0017139818921208011\n",
            "iter: 5881  x: [-99.99915257  24.99988444]  f(x): 7.314986854354341e-07  grad at x: [ 0.00169487 -0.00023112]  gradient norm: 0.0017105539283348353\n",
            "iter: 5882  x: [-99.99915426  24.99988467]  f(x): 7.285756166980033e-07  grad at x: [ 0.00169148 -0.00023066]  gradient norm: 0.0017071328204893764\n",
            "iter: 5883  x: [-99.99915595  24.9998849 ]  f(x): 7.256642285305229e-07  grad at x: [ 0.0016881  -0.00023019]  gradient norm: 0.0017037185548446937\n",
            "iter: 5884  x: [-99.99915764  24.99988513]  f(x): 7.227644742830884e-07  grad at x: [ 0.00168472 -0.00022973]  gradient norm: 0.0017003111177465004\n",
            "iter: 5885  x: [-99.99915932  24.99988536]  f(x): 7.198763074449909e-07  grad at x: [ 0.00168135 -0.00022928]  gradient norm: 0.0016969104955123484\n",
            "iter: 5886  x: [-99.99916101  24.99988559]  f(x): 7.169996817168399e-07  grad at x: [ 0.00167799 -0.00022882]  gradient norm: 0.0016935166745170713\n",
            "iter: 5887  x: [-99.99916268  24.99988582]  f(x): 7.141345509858156e-07  grad at x: [ 0.00167463 -0.00022836]  gradient norm: 0.0016901296411646245\n",
            "iter: 5888  x: [-99.99916436  24.99988605]  f(x): 7.112808693233392e-07  grad at x: [ 0.00167128 -0.0002279 ]  gradient norm: 0.0016867493818861642\n",
            "iter: 5889  x: [-99.99916603  24.99988628]  f(x): 7.084385909631051e-07  grad at x: [ 0.00166794 -0.00022745]  gradient norm: 0.0016833758831147666\n",
            "iter: 5890  x: [-99.9991677  24.9998865]  f(x): 7.056076703454962e-07  grad at x: [ 0.0016646  -0.00022699]  gradient norm: 0.00168000913133887\n",
            "iter: 5891  x: [-99.99916936  24.99988673]  f(x): 7.027880620954679e-07  grad at x: [ 0.00166127 -0.00022654]  gradient norm: 0.0016766491130769943\n",
            "iter: 5892  x: [-99.99917102  24.99988696]  f(x): 6.999797209966726e-07  grad at x: [ 0.00165795 -0.00022608]  gradient norm: 0.0016732958148476587\n",
            "iter: 5893  x: [-99.99917268  24.99988718]  f(x): 6.971826020380341e-07  grad at x: [ 0.00165464 -0.00022563]  gradient norm: 0.0016699492232257053\n",
            "iter: 5894  x: [-99.99917434  24.99988741]  f(x): 6.943966603666912e-07  grad at x: [ 0.00165133 -0.00022518]  gradient norm: 0.0016666093247869353\n",
            "iter: 5895  x: [-99.99917599  24.99988763]  f(x): 6.9162185131096e-07  grad at x: [ 0.00164802 -0.00022473]  gradient norm: 0.0016632761061362723\n",
            "iter: 5896  x: [-99.99917764  24.99988786]  f(x): 6.88858130401422e-07  grad at x: [ 0.00164473 -0.00022428]  gradient norm: 0.0016599495539340006\n",
            "iter: 5897  x: [-99.99917928  24.99988808]  f(x): 6.861054533024247e-07  grad at x: [ 0.00164144 -0.00022383]  gradient norm: 0.0016566296548141649\n",
            "iter: 5898  x: [-99.99918092  24.99988831]  f(x): 6.833637759041798e-07  grad at x: [ 0.00163816 -0.00022338]  gradient norm: 0.001653316395496252\n",
            "iter: 5899  x: [-99.99918256  24.99988853]  f(x): 6.806330542495369e-07  grad at x: [ 0.00163488 -0.00022294]  gradient norm: 0.0016500097626978294\n",
            "iter: 5900  x: [-99.9991842   24.99988875]  f(x): 6.779132445606981e-07  grad at x: [ 0.00163161 -0.00022249]  gradient norm: 0.0016467097431675057\n",
            "iter: 5901  x: [-99.99918583  24.99988898]  f(x): 6.752043032353673e-07  grad at x: [ 0.00162835 -0.00022205]  gradient norm: 0.00164341632368109\n",
            "iter: 5902  x: [-99.99918746  24.9998892 ]  f(x): 6.725061868476634e-07  grad at x: [ 0.00162509 -0.0002216 ]  gradient norm: 0.001640129491043513\n",
            "iter: 5903  x: [-99.99918908  24.99988942]  f(x): 6.698188521236122e-07  grad at x: [ 0.00162184 -0.00022116]  gradient norm: 0.0016368492320597058\n",
            "iter: 5904  x: [-99.9991907   24.99988964]  f(x): 6.671422559882835e-07  grad at x: [ 0.0016186  -0.00022072]  gradient norm: 0.0016335755335928407\n",
            "iter: 5905  x: [-99.99919232  24.99988986]  f(x): 6.644763555387969e-07  grad at x: [ 0.00161536 -0.00022028]  gradient norm: 0.0016303083825323316\n",
            "iter: 5906  x: [-99.99919394  24.99989008]  f(x): 6.618211080246719e-07  grad at x: [ 0.00161213 -0.00021984]  gradient norm: 0.0016270477657704728\n",
            "iter: 5907  x: [-99.99919555  24.9998903 ]  f(x): 6.591764708679019e-07  grad at x: [ 0.0016089 -0.0002194]  gradient norm: 0.0016237936702277194\n",
            "iter: 5908  x: [-99.99919716  24.99989052]  f(x): 6.565424016843375e-07  grad at x: [ 0.00160569 -0.00021896]  gradient norm: 0.0016205460828798883\n",
            "iter: 5909  x: [-99.99919876  24.99989074]  f(x): 6.539188582395962e-07  grad at x: [ 0.00160247 -0.00021852]  gradient norm: 0.001617304990704717\n",
            "iter: 5910  x: [-99.99920037  24.99989096]  f(x): 6.513057984925277e-07  grad at x: [ 0.00159927 -0.00021808]  gradient norm: 0.0016140703807362648\n",
            "iter: 5911  x: [-99.99920196  24.99989118]  f(x): 6.487031805270657e-07  grad at x: [ 0.00159607 -0.00021765]  gradient norm: 0.0016108422399813904\n",
            "iter: 5912  x: [-99.99920356  24.99989139]  f(x): 6.461109626183921e-07  grad at x: [ 0.00159288 -0.00021721]  gradient norm: 0.001607620555502314\n",
            "iter: 5913  x: [-99.99920515  24.99989161]  f(x): 6.435291032117917e-07  grad at x: [ 0.00158969 -0.00021678]  gradient norm: 0.0016044053143913376\n",
            "iter: 5914  x: [-99.99920674  24.99989183]  f(x): 6.409575609204562e-07  grad at x: [ 0.00158651 -0.00021634]  gradient norm: 0.0016011965037689235\n",
            "iter: 5915  x: [-99.99920833  24.99989204]  f(x): 6.383962945023436e-07  grad at x: [ 0.00158334 -0.00021591]  gradient norm: 0.0015979941107555353\n",
            "iter: 5916  x: [-99.99920991  24.99989226]  f(x): 6.358452629054401e-07  grad at x: [ 0.00158017 -0.00021548]  gradient norm: 0.0015947981225289175\n",
            "iter: 5917  x: [-99.99921149  24.99989248]  f(x): 6.333044252444806e-07  grad at x: [ 0.00157701 -0.00021505]  gradient norm: 0.0015916085262959363\n",
            "iter: 5918  x: [-99.99921307  24.99989269]  f(x): 6.307737407540496e-07  grad at x: [ 0.00157386 -0.00021462]  gradient norm: 0.0015884253092343368\n",
            "iter: 5919  x: [-99.99921464  24.99989291]  f(x): 6.282531688799717e-07  grad at x: [ 0.00157071 -0.00021419]  gradient norm: 0.001585248458608267\n",
            "iter: 5920  x: [-99.99921621  24.99989312]  f(x): 6.257426692098698e-07  grad at x: [ 0.00156757 -0.00021376]  gradient norm: 0.001582077961681876\n",
            "iter: 5921  x: [-99.99921778  24.99989333]  f(x): 6.232422014949917e-07  grad at x: [ 0.00156444 -0.00021333]  gradient norm: 0.0015789138057474724\n",
            "iter: 5922  x: [-99.99921935  24.99989355]  f(x): 6.207517256503405e-07  grad at x: [ 0.00156131 -0.00021291]  gradient norm: 0.001575755978126487\n",
            "iter: 5923  x: [-99.99922091  24.99989376]  f(x): 6.182712017532876e-07  grad at x: [ 0.00155818 -0.00021248]  gradient norm: 0.001572604466168512\n",
            "iter: 5924  x: [-99.99922247  24.99989397]  f(x): 6.158005900216045e-07  grad at x: [ 0.00155507 -0.00021205]  gradient norm: 0.0015694592572240982\n",
            "iter: 5925  x: [-99.99922402  24.99989418]  f(x): 6.133398508571666e-07  grad at x: [ 0.00155196 -0.00021163]  gradient norm: 0.0015663203387010802\n",
            "iter: 5926  x: [-99.99922557  24.9998944 ]  f(x): 6.108889448215939e-07  grad at x: [ 0.00154885 -0.00021121]  gradient norm: 0.0015631876980344924\n",
            "iter: 5927  x: [-99.99922712  24.99989461]  f(x): 6.084478325932082e-07  grad at x: [ 0.00154576 -0.00021078]  gradient norm: 0.0015600613226321692\n",
            "iter: 5928  x: [-99.99922867  24.99989482]  f(x): 6.060164750545415e-07  grad at x: [ 0.00154266 -0.00021036]  gradient norm: 0.0015569411999873874\n",
            "iter: 5929  x: [-99.99923021  24.99989503]  f(x): 6.0359483322564e-07  grad at x: [ 0.00153958 -0.00020994]  gradient norm: 0.0015538273175943844\n",
            "iter: 5930  x: [-99.99923175  24.99989524]  f(x): 6.011828682621779e-07  grad at x: [ 0.0015365  -0.00020952]  gradient norm: 0.0015507196629464372\n",
            "iter: 5931  x: [-99.99923329  24.99989545]  f(x): 5.987805415219351e-07  grad at x: [ 0.00153343 -0.0002091 ]  gradient norm: 0.0015476182236222667\n",
            "iter: 5932  x: [-99.99923482  24.99989566]  f(x): 5.963878144767546e-07  grad at x: [ 0.00153036 -0.00020869]  gradient norm: 0.001544522987173392\n",
            "iter: 5933  x: [-99.99923635  24.99989587]  f(x): 5.94004648776741e-07  grad at x: [ 0.0015273  -0.00020827]  gradient norm: 0.0015414339412076548\n",
            "iter: 5934  x: [-99.99923788  24.99989607]  f(x): 5.91631006206119e-07  grad at x: [ 0.00152424 -0.00020785]  gradient norm: 0.001538351073332897\n",
            "iter: 5935  x: [-99.9992394   24.99989628]  f(x): 5.892668487059408e-07  grad at x: [ 0.0015212  -0.00020744]  gradient norm: 0.0015352743711870406\n",
            "iter: 5936  x: [-99.99924092  24.99989649]  f(x): 5.869121383712673e-07  grad at x: [ 0.00151815 -0.00020702]  gradient norm: 0.0015322038224352102\n",
            "iter: 5937  x: [-99.99924244  24.9998967 ]  f(x): 5.845668374735776e-07  grad at x: [ 0.00151512 -0.00020661]  gradient norm: 0.0015291394147998116\n",
            "iter: 5938  x: [-99.99924396  24.9998969 ]  f(x): 5.822309083947549e-07  grad at x: [ 0.00151209 -0.00020619]  gradient norm: 0.0015260811359750896\n",
            "iter: 5939  x: [-99.99924547  24.99989711]  f(x): 5.799043136919922e-07  grad at x: [ 0.00150906 -0.00020578]  gradient norm: 0.0015230289737125713\n",
            "iter: 5940  x: [-99.99924698  24.99989732]  f(x): 5.775870160541734e-07  grad at x: [ 0.00150604 -0.00020537]  gradient norm: 0.0015199829157647442\n",
            "iter: 5941  x: [-99.99924848  24.99989752]  f(x): 5.752789783434841e-07  grad at x: [ 0.00150303 -0.00020496]  gradient norm: 0.0015169429499404176\n",
            "iter: 5942  x: [-99.99924999  24.99989773]  f(x): 5.729801635519763e-07  grad at x: [ 0.00150003 -0.00020455]  gradient norm: 0.0015139090640484008\n",
            "iter: 5943  x: [-99.99925149  24.99989793]  f(x): 5.706905348224718e-07  grad at x: [ 0.00149703 -0.00020414]  gradient norm: 0.001510881245925664\n",
            "iter: 5944  x: [-99.99925298  24.99989813]  f(x): 5.684100554494276e-07  grad at x: [ 0.00149403 -0.00020373]  gradient norm: 0.0015078594834392594\n",
            "iter: 5945  x: [-99.99925448  24.99989834]  f(x): 5.661386888768946e-07  grad at x: [ 0.00149104 -0.00020332]  gradient norm: 0.0015048437644843992\n",
            "iter: 5946  x: [-99.99925597  24.99989854]  f(x): 5.638763986767928e-07  grad at x: [ 0.00148806 -0.00020292]  gradient norm: 0.0015018340769562967\n",
            "iter: 5947  x: [-99.99925746  24.99989874]  f(x): 5.616231485907544e-07  grad at x: [ 0.00148509 -0.00020251]  gradient norm: 0.0014988304088064859\n",
            "iter: 5948  x: [-99.99925894  24.99989895]  f(x): 5.593789024879307e-07  grad at x: [ 0.00148212 -0.00020211]  gradient norm: 0.0014958327479874622\n",
            "iter: 5949  x: [-99.99926042  24.99989915]  f(x): 5.571436243856429e-07  grad at x: [ 0.00147915 -0.0002017 ]  gradient norm: 0.0014928410824808418\n",
            "iter: 5950  x: [-99.9992619   24.99989935]  f(x): 5.549172784690657e-07  grad at x: [ 0.00147619 -0.0002013 ]  gradient norm: 0.0014898554003245628\n",
            "iter: 5951  x: [-99.99926338  24.99989955]  f(x): 5.52699829028354e-07  grad at x: [ 0.00147324 -0.0002009 ]  gradient norm: 0.0014868756895293621\n",
            "iter: 5952  x: [-99.99926485  24.99989975]  f(x): 5.504912405197448e-07  grad at x: [ 0.00147029 -0.00020049]  gradient norm: 0.0014839019381613393\n",
            "iter: 5953  x: [-99.99926632  24.99989995]  f(x): 5.482914775252174e-07  grad at x: [ 0.00146735 -0.00020009]  gradient norm: 0.0014809341342885136\n",
            "iter: 5954  x: [-99.99926779  24.99990015]  f(x): 5.461005047708058e-07  grad at x: [ 0.00146442 -0.00019969]  gradient norm: 0.0014779722660061057\n",
            "iter: 5955  x: [-99.99926925  24.99990035]  f(x): 5.439182871489364e-07  grad at x: [ 0.00146149 -0.00019929]  gradient norm: 0.001475016321467578\n",
            "iter: 5956  x: [-99.99927072  24.99990055]  f(x): 5.417447896747777e-07  grad at x: [ 0.00145857 -0.0001989 ]  gradient norm: 0.0014720662888263933\n",
            "iter: 5957  x: [-99.99927217  24.99990075]  f(x): 5.395799774851838e-07  grad at x: [ 0.00145565 -0.0001985 ]  gradient norm: 0.0014691221562350544\n",
            "iter: 5958  x: [-99.99927363  24.99990095]  f(x): 5.374238159031017e-07  grad at x: [ 0.00145274 -0.0001981 ]  gradient norm: 0.0014661839119334268\n",
            "iter: 5959  x: [-99.99927508  24.99990115]  f(x): 5.352762703300521e-07  grad at x: [ 0.00144983 -0.0001977 ]  gradient norm: 0.0014632515441031348\n",
            "iter: 5960  x: [-99.99927653  24.99990135]  f(x): 5.331373063518247e-07  grad at x: [ 0.00144693 -0.00019731]  gradient norm: 0.0014603250410122053\n",
            "iter: 5961  x: [-99.99927798  24.99990154]  f(x): 5.310068896745385e-07  grad at x: [ 0.00144404 -0.00019691]  gradient norm: 0.0014574043909286653\n",
            "iter: 5962  x: [-99.99927942  24.99990174]  f(x): 5.288849861447772e-07  grad at x: [ 0.00144115 -0.00019652]  gradient norm: 0.001454489582148703\n",
            "iter: 5963  x: [-99.99928087  24.99990194]  f(x): 5.267715617497392e-07  grad at x: [ 0.00143827 -0.00019613]  gradient norm: 0.0014515806029976278\n",
            "iter: 5964  x: [-99.9992823   24.99990213]  f(x): 5.246665825955907e-07  grad at x: [ 0.00143539 -0.00019574]  gradient norm: 0.0014486774418007492\n",
            "iter: 5965  x: [-99.99928374  24.99990233]  f(x): 5.225700149281771e-07  grad at x: [ 0.00143252 -0.00019534]  gradient norm: 0.0014457800869124974\n",
            "iter: 5966  x: [-99.99928517  24.99990252]  f(x): 5.204818251527897e-07  grad at x: [ 0.00142966 -0.00019495]  gradient norm: 0.0014428885267445848\n",
            "iter: 5967  x: [-99.9992866   24.99990272]  f(x): 5.184019797705134e-07  grad at x: [ 0.0014268  -0.00019456]  gradient norm: 0.0014400027496786435\n",
            "iter: 5968  x: [-99.99928803  24.99990291]  f(x): 5.163304454624989e-07  grad at x: [ 0.00142394 -0.00019417]  gradient norm: 0.0014371227441836677\n",
            "iter: 5969  x: [-99.99928945  24.99990311]  f(x): 5.142671890061535e-07  grad at x: [ 0.0014211  -0.00019379]  gradient norm: 0.0014342484987004914\n",
            "iter: 5970  x: [-99.99929087  24.9999033 ]  f(x): 5.122121773146332e-07  grad at x: [ 0.00141825 -0.0001934 ]  gradient norm: 0.00143138000169715\n",
            "iter: 5971  x: [-99.99929229  24.99990349]  f(x): 5.101653774584866e-07  grad at x: [ 0.00141542 -0.00019301]  gradient norm: 0.0014285172416999197\n",
            "iter: 5972  x: [-99.99929371  24.99990369]  f(x): 5.081267566033173e-07  grad at x: [ 0.00141259 -0.00019263]  gradient norm: 0.001425660207206917\n",
            "iter: 5973  x: [-99.99929512  24.99990388]  f(x): 5.060962820891078e-07  grad at x: [ 0.00140976 -0.00019224]  gradient norm: 0.0014228088867997808\n",
            "iter: 5974  x: [-99.99929653  24.99990407]  f(x): 5.040739213520483e-07  grad at x: [ 0.00140694 -0.00019186]  gradient norm: 0.0014199632690348695\n",
            "iter: 5975  x: [-99.99929794  24.99990426]  f(x): 5.020596419615763e-07  grad at x: [ 0.00140413 -0.00019147]  gradient norm: 0.0014171233424957424\n",
            "iter: 5976  x: [-99.99929934  24.99990446]  f(x): 5.000534116404513e-07  grad at x: [ 0.00140132 -0.00019109]  gradient norm: 0.001414289095822281\n",
            "iter: 5977  x: [-99.99930074  24.99990465]  f(x): 4.980551982050866e-07  grad at x: [ 0.00139852 -0.00019071]  gradient norm: 0.0014114605176271658\n",
            "iter: 5978  x: [-99.99930214  24.99990484]  f(x): 4.960649696249224e-07  grad at x: [ 0.00139572 -0.00019033]  gradient norm: 0.0014086375965803588\n",
            "iter: 5979  x: [-99.99930354  24.99990503]  f(x): 4.940826940005558e-07  grad at x: [ 0.00139293 -0.00018994]  gradient norm: 0.0014058203213790244\n",
            "iter: 5980  x: [-99.99930493  24.99990522]  f(x): 4.921083395652493e-07  grad at x: [ 0.00139014 -0.00018956]  gradient norm: 0.0014030086807504069\n",
            "iter: 5981  x: [-99.99930632  24.99990541]  f(x): 4.901418746429541e-07  grad at x: [ 0.00138736 -0.00018919]  gradient norm: 0.0014002026633926305\n",
            "iter: 5982  x: [-99.99930771  24.9999056 ]  f(x): 4.881832677092366e-07  grad at x: [ 0.00138459 -0.00018881]  gradient norm: 0.0013974022580620607\n",
            "iter: 5983  x: [-99.99930909  24.99990579]  f(x): 4.862324873689101e-07  grad at x: [ 0.00138182 -0.00018843]  gradient norm: 0.0013946074535422647\n",
            "iter: 5984  x: [-99.99931047  24.99990597]  f(x): 4.842895023568652e-07  grad at x: [ 0.00137906 -0.00018805]  gradient norm: 0.0013918182386459307\n",
            "iter: 5985  x: [-99.99931185  24.99990616]  f(x): 4.823542814984374e-07  grad at x: [ 0.0013763  -0.00018768]  gradient norm: 0.0013890346021585458\n",
            "iter: 5986  x: [-99.99931323  24.99990635]  f(x): 4.804267937860287e-07  grad at x: [ 0.00137354 -0.0001873 ]  gradient norm: 0.00138625653294912\n",
            "iter: 5987  x: [-99.9993146   24.99990654]  f(x): 4.785070083217688e-07  grad at x: [ 0.0013708  -0.00018693]  gradient norm: 0.0013834840198885838\n",
            "iter: 5988  x: [-99.99931597  24.99990672]  f(x): 4.765948943152041e-07  grad at x: [ 0.00136806 -0.00018655]  gradient norm: 0.0013807170518469077\n",
            "iter: 5989  x: [-99.99931734  24.99990691]  f(x): 4.7469042112378356e-07  grad at x: [ 0.00136532 -0.00018618]  gradient norm: 0.001377955617752304\n",
            "iter: 5990  x: [-99.99931871  24.9999071 ]  f(x): 4.727935581920789e-07  grad at x: [ 0.00136259 -0.00018581]  gradient norm: 0.0013751997065038647\n",
            "iter: 5991  x: [-99.99932007  24.99990728]  f(x): 4.709042751302955e-07  grad at x: [ 0.00135986 -0.00018544]  gradient norm: 0.001372449307086124\n",
            "iter: 5992  x: [-99.99932143  24.99990747]  f(x): 4.690225416548637e-07  grad at x: [ 0.00135714 -0.00018507]  gradient norm: 0.0013697044084836167\n",
            "iter: 5993  x: [-99.99932278  24.99990765]  f(x): 4.671483275702004e-07  grad at x: [ 0.00135443 -0.0001847 ]  gradient norm: 0.0013669649996546369\n",
            "iter: 5994  x: [-99.99932414  24.99990784]  f(x): 4.6528160286201197e-07  grad at x: [ 0.00135172 -0.00018433]  gradient norm: 0.0013642310696682025\n",
            "iter: 5995  x: [-99.99932549  24.99990802]  f(x): 4.6342233758382583e-07  grad at x: [ 0.00134902 -0.00018396]  gradient norm: 0.0013615026075389292\n",
            "iter: 5996  x: [-99.99932684  24.99990821]  f(x): 4.61570501932287e-07  grad at x: [ 0.00134632 -0.00018359]  gradient norm: 0.0013587796023377553\n",
            "iter: 5997  x: [-99.99932819  24.99990839]  f(x): 4.59726066208938e-07  grad at x: [ 0.00134363 -0.00018322]  gradient norm: 0.0013560620431365786\n",
            "iter: 5998  x: [-99.99932953  24.99990857]  f(x): 4.578890008567271e-07  grad at x: [ 0.00134094 -0.00018286]  gradient norm: 0.00135334991906266\n",
            "iter: 5999  x: [-99.99933087  24.99990876]  f(x): 4.5605927640422756e-07  grad at x: [ 0.00133826 -0.00018249]  gradient norm: 0.0013506432192170181\n",
            "iter: 6000  x: [-99.99933221  24.99990894]  f(x): 4.542368635394984e-07  grad at x: [ 0.00133558 -0.00018212]  gradient norm: 0.0013479419327841959\n",
            "iter: 6001  x: [-99.99933354  24.99990912]  f(x): 4.524217330353956e-07  grad at x: [ 0.00133291 -0.00018176]  gradient norm: 0.0013452460489224946\n",
            "iter: 6002  x: [-99.99933488  24.9999093 ]  f(x): 4.506138557853309e-07  grad at x: [ 0.00133024 -0.0001814 ]  gradient norm: 0.0013425555568174167\n",
            "iter: 6003  x: [-99.99933621  24.99990948]  f(x): 4.4881320282294876e-07  grad at x: [ 0.00132758 -0.00018103]  gradient norm: 0.0013398704457117468\n",
            "iter: 6004  x: [-99.99933754  24.99990966]  f(x): 4.470197452649668e-07  grad at x: [ 0.00132493 -0.00018067]  gradient norm: 0.001337190704821069\n",
            "iter: 6005  x: [-99.99933886  24.99990984]  f(x): 4.4523345436615885e-07  grad at x: [ 0.00132228 -0.00018031]  gradient norm: 0.0013345163234163289\n",
            "iter: 6006  x: [-99.99934018  24.99991002]  f(x): 4.4345430148246907e-07  grad at x: [ 0.00131963 -0.00017995]  gradient norm: 0.0013318472907694321\n",
            "iter: 6007  x: [-99.9993415  24.9999102]  f(x): 4.416822580894337e-07  grad at x: [ 0.001317   -0.00017959]  gradient norm: 0.001329183596181406\n",
            "iter: 6008  x: [-99.99934282  24.99991038]  f(x): 4.399172957810644e-07  grad at x: [ 0.00131436 -0.00017923]  gradient norm: 0.0013265252289814385\n",
            "iter: 6009  x: [-99.99934413  24.99991056]  f(x): 4.3815938626937585e-07  grad at x: [ 0.00131173 -0.00017887]  gradient norm: 0.0013238721785268786\n",
            "iter: 6010  x: [-99.99934545  24.99991074]  f(x): 4.364085013665806e-07  grad at x: [ 0.00130911 -0.00017851]  gradient norm: 0.0013212244341769958\n",
            "iter: 6011  x: [-99.99934675  24.99991092]  f(x): 4.34664613000828e-07  grad at x: [ 0.00130649 -0.00017816]  gradient norm: 0.0013185819853172998\n",
            "iter: 6012  x: [-99.99934806  24.9999111 ]  f(x): 4.329276931997471e-07  grad at x: [ 0.00130388 -0.0001778 ]  gradient norm: 0.0013159448213352217\n",
            "iter: 6013  x: [-99.99934936  24.99991128]  f(x): 4.311977141443684e-07  grad at x: [ 0.00130127 -0.00017745]  gradient norm: 0.0013133129317026745\n",
            "iter: 6014  x: [-99.99935067  24.99991145]  f(x): 4.294746480760135e-07  grad at x: [ 0.00129867 -0.00017709]  gradient norm: 0.0013106863058352498\n",
            "iter: 6015  x: [-99.99935196  24.99991163]  f(x): 4.2775846738910556e-07  grad at x: [ 0.00129607 -0.00017674]  gradient norm: 0.0013080649332339821\n",
            "iter: 6016  x: [-99.99935326  24.99991181]  f(x): 4.2604914455680716e-07  grad at x: [ 0.00129348 -0.00017638]  gradient norm: 0.0013054488033727055\n",
            "iter: 6017  x: [-99.99935455  24.99991198]  f(x): 4.243466521663602e-07  grad at x: [ 0.00129089 -0.00017603]  gradient norm: 0.0013028379057524542\n",
            "iter: 6018  x: [-99.99935585  24.99991216]  f(x): 4.226509629381898e-07  grad at x: [ 0.00128831 -0.00017568]  gradient norm: 0.0013002322299315455\n",
            "iter: 6019  x: [-99.99935713  24.99991234]  f(x): 4.209620496880917e-07  grad at x: [ 0.00128573 -0.00017533]  gradient norm: 0.001297631765468296\n",
            "iter: 6020  x: [-99.99935842  24.99991251]  f(x): 4.192798853458163e-07  grad at x: [ 0.00128316 -0.00017498]  gradient norm: 0.0012950365019501438\n",
            "iter: 6021  x: [-99.9993597   24.99991269]  f(x): 4.1760444291821177e-07  grad at x: [ 0.00128059 -0.00017463]  gradient norm: 0.0012924464289373262\n",
            "iter: 6022  x: [-99.99936098  24.99991286]  f(x): 4.1593569556053867e-07  grad at x: [ 0.00127803 -0.00017428]  gradient norm: 0.001289861536073603\n",
            "iter: 6023  x: [-99.99936226  24.99991304]  f(x): 4.142736165225197e-07  grad at x: [ 0.00127548 -0.00017393]  gradient norm: 0.0012872818130036945\n",
            "iter: 6024  x: [-99.99936354  24.99991321]  f(x): 4.126181791474489e-07  grad at x: [ 0.00127293 -0.00017358]  gradient norm: 0.0012847072493723214\n",
            "iter: 6025  x: [-99.99936481  24.99991338]  f(x): 4.109693569086458e-07  grad at x: [ 0.00127038 -0.00017323]  gradient norm: 0.0012821378348814854\n",
            "iter: 6026  x: [-99.99936608  24.99991356]  f(x): 4.093271233547703e-07  grad at x: [ 0.00126784 -0.00017289]  gradient norm: 0.0012795735592059884\n",
            "iter: 6027  x: [-99.99936735  24.99991373]  f(x): 4.076914521624491e-07  grad at x: [ 0.0012653  -0.00017254]  gradient norm: 0.001277014412075994\n",
            "iter: 6028  x: [-99.99936861  24.9999139 ]  f(x): 4.060623171189354e-07  grad at x: [ 0.00126277 -0.0001722 ]  gradient norm: 0.0012744603832507864\n",
            "iter: 6029  x: [-99.99936988  24.99991407]  f(x): 4.044396921031407e-07  grad at x: [ 0.00126025 -0.00017185]  gradient norm: 0.0012719114624896509\n",
            "iter: 6030  x: [-99.99937114  24.99991425]  f(x): 4.02823551085982e-07  grad at x: [ 0.00125773 -0.00017151]  gradient norm: 0.001269367639552832\n",
            "iter: 6031  x: [-99.99937239  24.99991442]  f(x): 4.012138681830196e-07  grad at x: [ 0.00125521 -0.00017117]  gradient norm: 0.0012668289042850572\n",
            "iter: 6032  x: [-99.99937365  24.99991459]  f(x): 3.996106175646513e-07  grad at x: [ 0.0012527  -0.00017082]  gradient norm: 0.0012642952464747327\n",
            "iter: 6033  x: [-99.9993749   24.99991476]  f(x): 3.9801377352848686e-07  grad at x: [ 0.0012502  -0.00017048]  gradient norm: 0.0012617666559685064\n",
            "iter: 6034  x: [-99.99937615  24.99991493]  f(x): 3.964233104969088e-07  grad at x: [ 0.0012477  -0.00017014]  gradient norm: 0.0012592431226683889\n",
            "iter: 6035  x: [-99.9993774  24.9999151]  f(x): 3.948392029462878e-07  grad at x: [ 0.0012452 -0.0001698]  gradient norm: 0.0012567246364200678\n",
            "iter: 6036  x: [-99.99937864  24.99991527]  f(x): 3.932614254965864e-07  grad at x: [ 0.00124271 -0.00016946]  gradient norm: 0.0012542111871556344\n",
            "iter: 6037  x: [-99.99937989  24.99991544]  f(x): 3.916899528382592e-07  grad at x: [ 0.00124022 -0.00016912]  gradient norm: 0.001251702764778059\n",
            "iter: 6038  x: [-99.99938113  24.99991561]  f(x): 3.9012475978555017e-07  grad at x: [ 0.00123774 -0.00016878]  gradient norm: 0.0012491993592466338\n",
            "iter: 6039  x: [-99.99938237  24.99991578]  f(x): 3.8856582124197574e-07  grad at x: [ 0.00123527 -0.00016845]  gradient norm: 0.0012467009605225717\n",
            "iter: 6040  x: [-99.9993836   24.99991595]  f(x): 3.8701311221638567e-07  grad at x: [ 0.0012328  -0.00016811]  gradient norm: 0.001244207558595246\n",
            "iter: 6041  x: [-99.99938483  24.99991611]  f(x): 3.8546660782253675e-07  grad at x: [ 0.00123033 -0.00016777]  gradient norm: 0.0012417191434821915\n",
            "iter: 6042  x: [-99.99938606  24.99991628]  f(x): 3.8392628326062445e-07  grad at x: [ 0.00122787 -0.00016744]  gradient norm: 0.0012392357051999824\n",
            "iter: 6043  x: [-99.99938729  24.99991645]  f(x): 3.823921138368287e-07  grad at x: [ 0.00122542 -0.0001671 ]  gradient norm: 0.001236757233796235\n",
            "iter: 6044  x: [-99.99938852  24.99991662]  f(x): 3.8086407494312475e-07  grad at x: [ 0.00122297 -0.00016677]  gradient norm: 0.0012342837193176045\n",
            "iter: 6045  x: [-99.99938974  24.99991678]  f(x): 3.793421420923228e-07  grad at x: [ 0.00122052 -0.00016643]  gradient norm: 0.0012318151518670693\n",
            "iter: 6046  x: [-99.99939096  24.99991695]  f(x): 3.778262909007543e-07  grad at x: [ 0.00121808 -0.0001661 ]  gradient norm: 0.0012293515215767283\n",
            "iter: 6047  x: [-99.99939218  24.99991712]  f(x): 3.763164970354342e-07  grad at x: [ 0.00121564 -0.00016577]  gradient norm: 0.0012268928185223584\n",
            "iter: 6048  x: [-99.99939339  24.99991728]  f(x): 3.74812736318195e-07  grad at x: [ 0.00121321 -0.00016544]  gradient norm: 0.001224439032893341\n",
            "iter: 6049  x: [-99.99939461  24.99991745]  f(x): 3.733149846209229e-07  grad at x: [ 0.00121078 -0.00016511]  gradient norm: 0.0012219901548227348\n",
            "iter: 6050  x: [-99.99939582  24.99991761]  f(x): 3.718232179343483e-07  grad at x: [ 0.00120836 -0.00016478]  gradient norm: 0.0012195461744999216\n",
            "iter: 6051  x: [-99.99939703  24.99991778]  f(x): 3.703374123509073e-07  grad at x: [ 0.00120595 -0.00016445]  gradient norm: 0.0012171070821434032\n",
            "iter: 6052  x: [-99.99939823  24.99991794]  f(x): 3.688575440466382e-07  grad at x: [ 0.00120353 -0.00016412]  gradient norm: 0.0012146728679716826\n",
            "iter: 6053  x: [-99.99939944  24.9999181 ]  f(x): 3.673835892985935e-07  grad at x: [ 0.00120113 -0.00016379]  gradient norm: 0.001212243522232383\n",
            "iter: 6054  x: [-99.99940064  24.99991827]  f(x): 3.659155244832637e-07  grad at x: [ 0.00119873 -0.00016346]  gradient norm: 0.001209819035200329\n",
            "iter: 6055  x: [-99.99940184  24.99991843]  f(x): 3.6445332604391247e-07  grad at x: [ 0.00119633 -0.00016314]  gradient norm: 0.001207399397124104\n",
            "iter: 6056  x: [-99.99940303  24.9999186 ]  f(x): 3.629969705566314e-07  grad at x: [ 0.00119394 -0.00016281]  gradient norm: 0.001204984598335815\n",
            "iter: 6057  x: [-99.99940423  24.99991876]  f(x): 3.615464346630233e-07  grad at x: [ 0.00119155 -0.00016248]  gradient norm: 0.0012025746291403677\n",
            "iter: 6058  x: [-99.99940542  24.99991892]  f(x): 3.601016951039302e-07  grad at x: [ 0.00118916 -0.00016216]  gradient norm: 0.001200169479871789\n",
            "iter: 6059  x: [-99.99940661  24.99991908]  f(x): 3.5866272873474005e-07  grad at x: [ 0.00118679 -0.00016183]  gradient norm: 0.001197769140919468\n",
            "iter: 6060  x: [-99.99940779  24.99991924]  f(x): 3.5722951247547555e-07  grad at x: [ 0.00118441 -0.00016151]  gradient norm: 0.0011953736026455923\n",
            "iter: 6061  x: [-99.99940898  24.99991941]  f(x): 3.5580202334432136e-07  grad at x: [ 0.00118204 -0.00016119]  gradient norm: 0.001192982855441471\n",
            "iter: 6062  x: [-99.99941016  24.99991957]  f(x): 3.543802384566482e-07  grad at x: [ 0.00117968 -0.00016087]  gradient norm: 0.0011905968897265745\n",
            "iter: 6063  x: [-99.99941134  24.99991973]  f(x): 3.5296413502404427e-07  grad at x: [ 0.00117732 -0.00016054]  gradient norm: 0.0011882156959475737\n",
            "iter: 6064  x: [-99.99941252  24.99991989]  f(x): 3.515536903389357e-07  grad at x: [ 0.00117497 -0.00016022]  gradient norm: 0.0011858392645530602\n",
            "iter: 6065  x: [-99.99941369  24.99992005]  f(x): 3.5014888178987507e-07  grad at x: [ 0.00117262 -0.0001599 ]  gradient norm: 0.0011834675860197862\n",
            "iter: 6066  x: [-99.99941486  24.99992021]  f(x): 3.487496868617144e-07  grad at x: [ 0.00117027 -0.00015958]  gradient norm: 0.0011811006508536254\n",
            "iter: 6067  x: [-99.99941604  24.99992037]  f(x): 3.4735608311747806e-07  grad at x: [ 0.00116793 -0.00015926]  gradient norm: 0.001178738449559491\n",
            "iter: 6068  x: [-99.9994172   24.99992053]  f(x): 3.459680482152729e-07  grad at x: [ 0.00116559 -0.00015894]  gradient norm: 0.0011763809726704575\n",
            "iter: 6069  x: [-99.99941837  24.99992069]  f(x): 3.445855598930567e-07  grad at x: [ 0.00116326 -0.00015863]  gradient norm: 0.0011740282107224795\n",
            "iter: 6070  x: [-99.99941953  24.99992085]  f(x): 3.432085959980203e-07  grad at x: [ 0.00116094 -0.00015831]  gradient norm: 0.001171680154304954\n",
            "iter: 6071  x: [-99.99942069  24.999921  ]  f(x): 3.418371344400237e-07  grad at x: [ 0.00115861 -0.00015799]  gradient norm: 0.0011693367939819967\n",
            "iter: 6072  x: [-99.99942185  24.99992116]  f(x): 3.4047115325557073e-07  grad at x: [ 0.0011563  -0.00015768]  gradient norm: 0.0011669981204022065\n",
            "iter: 6073  x: [-99.99942301  24.99992132]  f(x): 3.3911063052453985e-07  grad at x: [ 0.00115398 -0.00015736]  gradient norm: 0.0011646641241569002\n",
            "iter: 6074  x: [-99.99942416  24.99992148]  f(x): 3.3775554443687996e-07  grad at x: [ 0.00115168 -0.00015705]  gradient norm: 0.0011623347958946767\n",
            "iter: 6075  x: [-99.99942531  24.99992163]  f(x): 3.3640587327516727e-07  grad at x: [ 0.00114937 -0.00015673]  gradient norm: 0.0011600101262922962\n",
            "iter: 6076  x: [-99.99942646  24.99992179]  f(x): 3.350615953990345e-07  grad at x: [ 0.00114707 -0.00015642]  gradient norm: 0.001157690106028439\n",
            "iter: 6077  x: [-99.99942761  24.99992195]  f(x): 3.337226892595473e-07  grad at x: [ 0.00114478 -0.00015611]  gradient norm: 0.0011553747258089859\n",
            "iter: 6078  x: [-99.99942875  24.9999221 ]  f(x): 3.3238913339938414e-07  grad at x: [ 0.00114249 -0.00015579]  gradient norm: 0.0011530639763679795\n",
            "iter: 6079  x: [-99.9994299   24.99992226]  f(x): 3.310609064206041e-07  grad at x: [ 0.00114021 -0.00015548]  gradient norm: 0.001150757848412261\n",
            "iter: 6080  x: [-99.99943104  24.99992241]  f(x): 3.2973798703255094e-07  grad at x: [ 0.00113793 -0.00015517]  gradient norm: 0.001148456332704994\n",
            "iter: 6081  x: [-99.99943218  24.99992257]  f(x): 3.284203540357262e-07  grad at x: [ 0.00113565 -0.00015486]  gradient norm: 0.0011461594200384625\n",
            "iter: 6082  x: [-99.99943331  24.99992272]  f(x): 3.271079863047548e-07  grad at x: [ 0.00113338 -0.00015455]  gradient norm: 0.0011438671012049518\n",
            "iter: 6083  x: [-99.99943444  24.99992288]  f(x): 3.258008627881727e-07  grad at x: [ 0.00113111 -0.00015424]  gradient norm: 0.0011415793669967457\n",
            "iter: 6084  x: [-99.99943558  24.99992303]  f(x): 3.2449896254084665e-07  grad at x: [ 0.00112885 -0.00015393]  gradient norm: 0.0011392962082634114\n",
            "iter: 6085  x: [-99.9994367   24.99992319]  f(x): 3.232022646908717e-07  grad at x: [ 0.00112659 -0.00015363]  gradient norm: 0.0011370176158545155\n",
            "iter: 6086  x: [-99.99943783  24.99992334]  f(x): 3.219107484393619e-07  grad at x: [ 0.00112434 -0.00015332]  gradient norm: 0.001134743580619625\n",
            "iter: 6087  x: [-99.99943896  24.99992349]  f(x): 3.206243930926767e-07  grad at x: [ 0.00112209 -0.00015301]  gradient norm: 0.0011324740934655886\n",
            "iter: 6088  x: [-99.99944008  24.99992365]  f(x): 3.193431780130618e-07  grad at x: [ 0.00111985 -0.00015271]  gradient norm: 0.001130209145270134\n",
            "iter: 6089  x: [-99.9994412  24.9999238]  f(x): 3.180670826684192e-07  grad at x: [ 0.00111761 -0.0001524 ]  gradient norm: 0.001127948726970192\n",
            "iter: 6090  x: [-99.99944231  24.99992395]  f(x): 3.1679608661321386e-07  grad at x: [ 0.00111537 -0.0001521 ]  gradient norm: 0.001125692829528933\n",
            "iter: 6091  x: [-99.99944343  24.9999241 ]  f(x): 3.1553016945810226e-07  grad at x: [ 0.00111314 -0.00015179]  gradient norm: 0.0011234414438823276\n",
            "iter: 6092  x: [-99.99944454  24.99992426]  f(x): 3.1426931090091605e-07  grad at x: [ 0.00111091 -0.00015149]  gradient norm: 0.0011211945609945065\n",
            "iter: 6093  x: [-99.99944565  24.99992441]  f(x): 3.13013490726838e-07  grad at x: [ 0.00110869 -0.00015119]  gradient norm: 0.0011189521718587224\n",
            "iter: 6094  x: [-99.99944676  24.99992456]  f(x): 3.117626888232229e-07  grad at x: [ 0.00110647 -0.00015088]  gradient norm: 0.0011167142675245498\n",
            "iter: 6095  x: [-99.99944787  24.99992471]  f(x): 3.1051688511684643e-07  grad at x: [ 0.00110426 -0.00015058]  gradient norm: 0.0011144808389862005\n",
            "iter: 6096  x: [-99.99944897  24.99992486]  f(x): 3.092760596517846e-07  grad at x: [ 0.00110205 -0.00015028]  gradient norm: 0.0011122518773223709\n",
            "iter: 6097  x: [-99.99945008  24.99992501]  f(x): 3.080401925100608e-07  grad at x: [ 0.00109985 -0.00014998]  gradient norm: 0.0011100273735544738\n",
            "iter: 6098  x: [-99.99945118  24.99992516]  f(x): 3.068092639069513e-07  grad at x: [ 0.00109765 -0.00014968]  gradient norm: 0.0011078073188184872\n",
            "iter: 6099  x: [-99.99945227  24.99992531]  f(x): 3.0558325409568514e-07  grad at x: [ 0.00109545 -0.00014938]  gradient norm: 0.0011055917041940667\n",
            "iter: 6100  x: [-99.99945337  24.99992546]  f(x): 3.043621434141673e-07  grad at x: [ 0.00109326 -0.00014908]  gradient norm: 0.0011033805207890292\n",
            "iter: 6101  x: [-99.99945446  24.99992561]  f(x): 3.031459122851537e-07  grad at x: [ 0.00109108 -0.00014878]  gradient norm: 0.001101173759740312\n",
            "iter: 6102  x: [-99.99945555  24.99992576]  f(x): 3.0193454121483954e-07  grad at x: [ 0.00108889 -0.00014849]  gradient norm: 0.0010989714122120549\n",
            "iter: 6103  x: [-99.99945664  24.99992591]  f(x): 3.007280107935667e-07  grad at x: [ 0.00108672 -0.00014819]  gradient norm: 0.0010967734693975173\n",
            "iter: 6104  x: [-99.99945773  24.99992605]  f(x): 2.995263016646445e-07  grad at x: [ 0.00108454 -0.00014789]  gradient norm: 0.0010945799224627583\n",
            "iter: 6105  x: [-99.99945881  24.9999262 ]  f(x): 2.983293945699251e-07  grad at x: [ 0.00108237 -0.0001476 ]  gradient norm: 0.001092390762630159\n",
            "iter: 6106  x: [-99.9994599   24.99992635]  f(x): 2.971372703032498e-07  grad at x: [ 0.00108021 -0.0001473 ]  gradient norm: 0.0010902059810939395\n",
            "iter: 6107  x: [-99.99946098  24.9999265 ]  f(x): 2.959499097716901e-07  grad at x: [ 0.00107805 -0.00014701]  gradient norm: 0.0010880255691328032\n",
            "iter: 6108  x: [-99.99946205  24.99992664]  f(x): 2.9476729393478044e-07  grad at x: [ 0.00107589 -0.00014671]  gradient norm: 0.0010858495179992124\n",
            "iter: 6109  x: [-99.99946313  24.99992679]  f(x): 2.935894038329074e-07  grad at x: [ 0.00107374 -0.00014642]  gradient norm: 0.0010836778189718703\n",
            "iter: 6110  x: [-99.9994642   24.99992694]  f(x): 2.9241622057382503e-07  grad at x: [ 0.00107159 -0.00014613]  gradient norm: 0.0010815104633314003\n",
            "iter: 6111  x: [-99.99946527  24.99992708]  f(x): 2.9124772536181454e-07  grad at x: [ 0.00106945 -0.00014583]  gradient norm: 0.001079347442414748\n",
            "iter: 6112  x: [-99.99946634  24.99992723]  f(x): 2.900838994511715e-07  grad at x: [ 0.00106731 -0.00014554]  gradient norm: 0.0010771887475297382\n",
            "iter: 6113  x: [-99.99946741  24.99992737]  f(x): 2.889247241936509e-07  grad at x: [ 0.00106518 -0.00014525]  gradient norm: 0.001075034370043397\n",
            "iter: 6114  x: [-99.99946848  24.99992752]  f(x): 2.877701809900654e-07  grad at x: [ 0.00106305 -0.00014496]  gradient norm: 0.0010728843012926704\n",
            "iter: 6115  x: [-99.99946954  24.99992766]  f(x): 2.866202513521091e-07  grad at x: [ 0.00106092 -0.00014467]  gradient norm: 0.0010707385326999474\n",
            "iter: 6116  x: [-99.9994706   24.99992781]  f(x): 2.8547491682648077e-07  grad at x: [ 0.0010588  -0.00014438]  gradient norm: 0.0010685970556322544\n",
            "iter: 6117  x: [-99.99947166  24.99992795]  f(x): 2.8433415905403795e-07  grad at x: [ 0.00105668 -0.00014409]  gradient norm: 0.00106645986151198\n",
            "iter: 6118  x: [-99.99947272  24.9999281 ]  f(x): 2.8319795975586224e-07  grad at x: [ 0.00105457 -0.0001438 ]  gradient norm: 0.0010643269417915948\n",
            "iter: 6119  x: [-99.99947377  24.99992824]  f(x): 2.82066300701474e-07  grad at x: [ 0.00105246 -0.00014352]  gradient norm: 0.0010621982878944477\n",
            "iter: 6120  x: [-99.99947482  24.99992839]  f(x): 2.809391637695274e-07  grad at x: [ 0.00105035 -0.00014323]  gradient norm: 0.0010600738913293306\n",
            "iter: 6121  x: [-99.99947587  24.99992853]  f(x): 2.798165308721843e-07  grad at x: [ 0.00104825 -0.00014294]  gradient norm: 0.0010579537435487136\n",
            "iter: 6122  x: [-99.99947692  24.99992867]  f(x): 2.786983840146991e-07  grad at x: [ 0.00104616 -0.00014266]  gradient norm: 0.001055837836061389\n",
            "iter: 6123  x: [-99.99947797  24.99992881]  f(x): 2.775847052657712e-07  grad at x: [ 0.00104406 -0.00014237]  gradient norm: 0.001053726160377109\n",
            "iter: 6124  x: [-99.99947901  24.99992896]  f(x): 2.764754767864701e-07  grad at x: [ 0.00104198 -0.00014209]  gradient norm: 0.0010516187080619479\n",
            "iter: 6125  x: [-99.99948005  24.9999291 ]  f(x): 2.753706807854269e-07  grad at x: [ 0.00103989 -0.0001418 ]  gradient norm: 0.0010495154706538193\n",
            "iter: 6126  x: [-99.99948109  24.99992924]  f(x): 2.7427029954930574e-07  grad at x: [ 0.00103781 -0.00014152]  gradient norm: 0.0010474164397207173\n",
            "iter: 6127  x: [-99.99948213  24.99992938]  f(x): 2.731743154257425e-07  grad at x: [ 0.00103574 -0.00014124]  gradient norm: 0.0010453216068287167\n",
            "iter: 6128  x: [-99.99948317  24.99992952]  f(x): 2.7208271085405847e-07  grad at x: [ 0.00103366 -0.00014095]  gradient norm: 0.001043230963601174\n",
            "iter: 6129  x: [-99.9994842   24.99992966]  f(x): 2.709954683359645e-07  grad at x: [ 0.0010316  -0.00014067]  gradient norm: 0.0010411445016633657\n",
            "iter: 6130  x: [-99.99948523  24.9999298 ]  f(x): 2.699125704480116e-07  grad at x: [ 0.00102953 -0.00014039]  gradient norm: 0.0010390622126668096\n",
            "iter: 6131  x: [-99.99948626  24.99992994]  f(x): 2.6883399981357023e-07  grad at x: [ 0.00102748 -0.00014011]  gradient norm: 0.0010369840882358229\n",
            "iter: 6132  x: [-99.99948729  24.99993008]  f(x): 2.6775973914651316e-07  grad at x: [ 0.00102542 -0.00013983]  gradient norm: 0.0010349101200520037\n",
            "iter: 6133  x: [-99.99948832  24.99993022]  f(x): 2.666897712362028e-07  grad at x: [ 0.00102337 -0.00013955]  gradient norm: 0.0010328402998260723\n",
            "iter: 6134  x: [-99.99948934  24.99993036]  f(x): 2.6562407891665513e-07  grad at x: [ 0.00102132 -0.00013927]  gradient norm: 0.0010307746192386679\n",
            "iter: 6135  x: [-99.99949036  24.9999305 ]  f(x): 2.6456264509746845e-07  grad at x: [ 0.00101928 -0.00013899]  gradient norm: 0.0010287130700005098\n",
            "iter: 6136  x: [-99.99949138  24.99993064]  f(x): 2.6350545276201787e-07  grad at x: [ 0.00101724 -0.00013871]  gradient norm: 0.0010266556438495195\n",
            "iter: 6137  x: [-99.9994924   24.99993078]  f(x): 2.6245248496862033e-07  grad at x: [ 0.00101521 -0.00013844]  gradient norm: 0.0010246023325536993\n",
            "iter: 6138  x: [-99.99949341  24.99993092]  f(x): 2.614037248348303e-07  grad at x: [ 0.00101318 -0.00013816]  gradient norm: 0.001022553127881051\n",
            "iter: 6139  x: [-99.99949442  24.99993106]  f(x): 2.603591555511503e-07  grad at x: [ 0.00101115 -0.00013788]  gradient norm: 0.0010205080216267783\n",
            "iter: 6140  x: [-99.99949544  24.9999312 ]  f(x): 2.5931876036784896e-07  grad at x: [ 0.00100913 -0.00013761]  gradient norm: 0.0010184670055880041\n",
            "iter: 6141  x: [-99.99949645  24.99993133]  f(x): 2.582825226071441e-07  grad at x: [ 0.00100711 -0.00013733]  gradient norm: 0.0010164300715880933\n",
            "iter: 6142  x: [-99.99949745  24.99993147]  f(x): 2.5725042565105267e-07  grad at x: [ 0.0010051  -0.00013706]  gradient norm: 0.00101439721145329\n",
            "iter: 6143  x: [-99.99949846  24.99993161]  f(x): 2.5622245295352034e-07  grad at x: [ 0.00100309 -0.00013678]  gradient norm: 0.0010123684170370397\n",
            "iter: 6144  x: [-99.99949946  24.99993174]  f(x): 2.5519858802637663e-07  grad at x: [ 0.00100108 -0.00013651]  gradient norm: 0.0010103436801927879\n",
            "iter: 6145  x: [-99.99950046  24.99993188]  f(x): 2.541788144675645e-07  grad at x: [ 0.00099908 -0.00013624]  gradient norm: 0.0010083229928303024\n",
            "iter: 6146  x: [-99.99950146  24.99993202]  f(x): 2.531631159191498e-07  grad at x: [ 0.00099708 -0.00013597]  gradient norm: 0.00100630634683311\n",
            "iter: 6147  x: [-99.99950246  24.99993215]  f(x): 2.5215147610826595e-07  grad at x: [ 0.00099508 -0.00013569]  gradient norm: 0.001004293734140099\n",
            "iter: 6148  x: [-99.99950345  24.99993229]  f(x): 2.511438788052966e-07  grad at x: [ 0.00099309 -0.00013542]  gradient norm: 0.0010022851466629577\n",
            "iter: 6149  x: [-99.99950445  24.99993242]  f(x): 2.501403078651425e-07  grad at x: [ 0.00099111 -0.00013515]  gradient norm: 0.0010002805763687356\n",
            "iter: 6150  x: [-99.99950544  24.99993256]  f(x): 2.491407472001069e-07  grad at x: [ 0.00098913 -0.00013488]  gradient norm: 0.000998280015226403\n",
            "iter: 6151  x: [-99.99950643  24.99993269]  f(x): 2.481451807787684e-07  grad at x: [ 0.00098715 -0.00013461]  gradient norm: 0.0009962834552049299\n",
            "iter: 6152  x: [-99.99950741  24.99993283]  f(x): 2.4715359263934284e-07  grad at x: [ 0.00098517 -0.00013434]  gradient norm: 0.0009942908883004869\n",
            "iter: 6153  x: [-99.9995084   24.99993296]  f(x): 2.461659668763684e-07  grad at x: [ 0.0009832  -0.00013407]  gradient norm: 0.0009923023065102055\n",
            "iter: 6154  x: [-99.99950938  24.9999331 ]  f(x): 2.4518228766842934e-07  grad at x: [ 0.00098124 -0.0001338 ]  gradient norm: 0.0009903177018884986\n",
            "iter: 6155  x: [-99.99951036  24.99993323]  f(x): 2.4420253924893136e-07  grad at x: [ 0.00097927 -0.00013354]  gradient norm: 0.0009883370664888196\n",
            "iter: 6156  x: [-99.99951134  24.99993336]  f(x): 2.432267059073697e-07  grad at x: [ 0.00097732 -0.00013327]  gradient norm: 0.000986360392366542\n",
            "iter: 6157  x: [-99.99951232  24.9999335 ]  f(x): 2.422547719877435e-07  grad at x: [ 0.00097536 -0.000133  ]  gradient norm: 0.0009843876715760788\n",
            "iter: 6158  x: [-99.99951329  24.99993363]  f(x): 2.412867219165417e-07  grad at x: [ 0.00097341 -0.00013274]  gradient norm: 0.0009824188962281653\n",
            "iter: 6159  x: [-99.99951427  24.99993376]  f(x): 2.4032254017563993e-07  grad at x: [ 0.00097146 -0.00013247]  gradient norm: 0.0009804540584354576\n",
            "iter: 6160  x: [-99.99951524  24.9999339 ]  f(x): 2.3936221130025647e-07  grad at x: [ 0.00096952 -0.00013221]  gradient norm: 0.0009784931503086906\n",
            "iter: 6161  x: [-99.99951621  24.99993403]  f(x): 2.3840571990865098e-07  grad at x: [ 0.00096758 -0.00013194]  gradient norm: 0.0009765361640178022\n",
            "iter: 6162  x: [-99.99951718  24.99993416]  f(x): 2.3745305065817353e-07  grad at x: [ 0.00096565 -0.00013168]  gradient norm: 0.0009745830917026491\n",
            "iter: 6163  x: [-99.99951814  24.99993429]  f(x): 2.3650418827404857e-07  grad at x: [ 0.00096372 -0.00013142]  gradient norm: 0.0009726339255322088\n",
            "iter: 6164  x: [-99.99951911  24.99993442]  f(x): 2.3555911753541207e-07  grad at x: [ 0.00096179 -0.00013115]  gradient norm: 0.000970688657676419\n",
            "iter: 6165  x: [-99.99952007  24.99993455]  f(x): 2.3461782330150493e-07  grad at x: [ 0.00095986 -0.00013089]  gradient norm: 0.0009687472803605797\n",
            "iter: 6166  x: [-99.99952103  24.99993469]  f(x): 2.3368029048541828e-07  grad at x: [ 0.00095794 -0.00013063]  gradient norm: 0.000966809785811911\n",
            "iter: 6167  x: [-99.99952199  24.99993482]  f(x): 2.3274650403849427e-07  grad at x: [ 0.00095603 -0.00013037]  gradient norm: 0.0009648761662275513\n",
            "iter: 6168  x: [-99.99952294  24.99993495]  f(x): 2.3181644900686714e-07  grad at x: [ 0.00095412 -0.00013011]  gradient norm: 0.0009629464138920029\n",
            "iter: 6169  x: [-99.9995239   24.99993508]  f(x): 2.3089011047445801e-07  grad at x: [ 0.00095221 -0.00012985]  gradient norm: 0.0009610205210596869\n",
            "iter: 6170  x: [-99.99952485  24.99993521]  f(x): 2.2996747359182134e-07  grad at x: [ 0.0009503  -0.00012959]  gradient norm: 0.0009590984800151053\n",
            "iter: 6171  x: [-99.9995258   24.99993534]  f(x): 2.2904852356099526e-07  grad at x: [ 0.0009484  -0.00012933]  gradient norm: 0.0009571802830418004\n",
            "iter: 6172  x: [-99.99952675  24.99993547]  f(x): 2.2813324566317617e-07  grad at x: [ 0.00094651 -0.00012907]  gradient norm: 0.0009552659224805963\n",
            "iter: 6173  x: [-99.99952769  24.99993559]  f(x): 2.2722162521756802e-07  grad at x: [ 0.00094461 -0.00012881]  gradient norm: 0.0009533553906441564\n",
            "iter: 6174  x: [-99.99952864  24.99993572]  f(x): 2.2631364760816371e-07  grad at x: [ 0.00094272 -0.00012855]  gradient norm: 0.000951448679873305\n",
            "iter: 6175  x: [-99.99952958  24.99993585]  f(x): 2.2540929827054963e-07  grad at x: [ 0.00094084 -0.0001283 ]  gradient norm: 0.0009495457825098263\n",
            "iter: 6176  x: [-99.99953052  24.99993598]  f(x): 2.2450856271798694e-07  grad at x: [ 0.00093896 -0.00012804]  gradient norm: 0.0009476466909518271\n",
            "iter: 6177  x: [-99.99953146  24.99993611]  f(x): 2.2361142650150268e-07  grad at x: [ 0.00093708 -0.00012778]  gradient norm: 0.0009457513975702127\n",
            "iter: 6178  x: [-99.9995324   24.99993624]  f(x): 2.2271787523554776e-07  grad at x: [ 0.0009352  -0.00012753]  gradient norm: 0.0009438598947630898\n",
            "iter: 6179  x: [-99.99953333  24.99993636]  f(x): 2.2182789461143963e-07  grad at x: [ 0.00093333 -0.00012727]  gradient norm: 0.0009419721749848869\n",
            "iter: 6180  x: [-99.99953427  24.99993649]  f(x): 2.2094147034490777e-07  grad at x: [ 0.00093147 -0.00012702]  gradient norm: 0.0009400882306356309\n",
            "iter: 6181  x: [-99.9995352   24.99993662]  f(x): 2.2005858822814791e-07  grad at x: [ 0.0009296  -0.00012676]  gradient norm: 0.0009382080541716702\n",
            "iter: 6182  x: [-99.99953613  24.99993674]  f(x): 2.1917923411533167e-07  grad at x: [ 0.00092775 -0.00012651]  gradient norm: 0.0009363316380755948\n",
            "iter: 6183  x: [-99.99953705  24.99993687]  f(x): 2.183033938982691e-07  grad at x: [ 0.00092589 -0.00012626]  gradient norm: 0.0009344589748047136\n",
            "iter: 6184  x: [-99.99953798  24.999937  ]  f(x): 2.1743105353130527e-07  grad at x: [ 0.00092404 -0.00012601]  gradient norm: 0.0009325900568444964\n",
            "iter: 6185  x: [-99.9995389   24.99993712]  f(x): 2.1656219904325925e-07  grad at x: [ 0.00092219 -0.00012575]  gradient norm: 0.0009307248767348152\n",
            "iter: 6186  x: [-99.99953983  24.99993725]  f(x): 2.1569681650002305e-07  grad at x: [ 0.00092035 -0.0001255 ]  gradient norm: 0.0009288634269902611\n",
            "iter: 6187  x: [-99.99954075  24.99993737]  f(x): 2.1483489201581245e-07  grad at x: [ 0.00091851 -0.00012525]  gradient norm: 0.0009270057001244652\n",
            "iter: 6188  x: [-99.99954167  24.9999375 ]  f(x): 2.1397641179255604e-07  grad at x: [ 0.00091667 -0.000125  ]  gradient norm: 0.0009251516887355414\n",
            "iter: 6189  x: [-99.99954258  24.99993762]  f(x): 2.1312136205480553e-07  grad at x: [ 0.00091483 -0.00012475]  gradient norm: 0.0009233013853662422\n",
            "iter: 6190  x: [-99.9995435   24.99993775]  f(x): 2.122697290883353e-07  grad at x: [ 0.00091301 -0.0001245 ]  gradient norm: 0.0009214547825874806\n",
            "iter: 6191  x: [-99.99954441  24.99993787]  f(x): 2.1142149925282325e-07  grad at x: [ 0.00091118 -0.00012425]  gradient norm: 0.0009196118730264921\n",
            "iter: 6192  x: [-99.99954532  24.999938  ]  f(x): 2.105766589431268e-07  grad at x: [ 0.00090936 -0.000124  ]  gradient norm: 0.0009177726492833109\n",
            "iter: 6193  x: [-99.99954623  24.99993812]  f(x): 2.0973519461418614e-07  grad at x: [ 0.00090754 -0.00012376]  gradient norm: 0.0009159371039851724\n",
            "iter: 6194  x: [-99.99954714  24.99993825]  f(x): 2.0889709278207988e-07  grad at x: [ 0.00090572 -0.00012351]  gradient norm: 0.0009141052297893933\n",
            "iter: 6195  x: [-99.99954804  24.99993837]  f(x): 2.080623399967475e-07  grad at x: [ 0.00090391 -0.00012326]  gradient norm: 0.0009122770193241689\n",
            "iter: 6196  x: [-99.99954895  24.99993849]  f(x): 2.0723092288088363e-07  grad at x: [ 0.0009021  -0.00012301]  gradient norm: 0.0009104524652740167\n",
            "iter: 6197  x: [-99.99954985  24.99993862]  f(x): 2.0640282811762517e-07  grad at x: [ 0.0009003  -0.00012277]  gradient norm: 0.0009086315603535356\n",
            "iter: 6198  x: [-99.99955075  24.99993874]  f(x): 2.055780424106697e-07  grad at x: [ 0.0008985  -0.00012252]  gradient norm: 0.0009068142972200421\n",
            "iter: 6199  x: [-99.99955165  24.99993886]  f(x): 2.0475655254855132e-07  grad at x: [ 0.0008967  -0.00012228]  gradient norm: 0.000905000668615336\n",
            "iter: 6200  x: [-99.99955255  24.99993898]  f(x): 2.0393834536640614e-07  grad at x: [ 0.00089491 -0.00012203]  gradient norm: 0.0009031906672821772\n",
            "iter: 6201  x: [-99.99955344  24.99993911]  f(x): 2.0312340773271154e-07  grad at x: [ 0.00089312 -0.00012179]  gradient norm: 0.0009013842859351644\n",
            "iter: 6202  x: [-99.99955433  24.99993923]  f(x): 2.023117266003798e-07  grad at x: [ 0.00089133 -0.00012155]  gradient norm: 0.0008995815173743395\n",
            "iter: 6203  x: [-99.99955523  24.99993935]  f(x): 2.0150328894217382e-07  grad at x: [ 0.00088955 -0.0001213 ]  gradient norm: 0.0008977823543424627\n",
            "iter: 6204  x: [-99.99955611  24.99993947]  f(x): 2.0069808180215832e-07  grad at x: [ 0.00088777 -0.00012106]  gradient norm: 0.0008959867896395757\n",
            "iter: 6205  x: [-99.999557    24.99993959]  f(x): 1.9989609226970206e-07  grad at x: [ 0.000886   -0.00012082]  gradient norm: 0.0008941948160657208\n",
            "iter: 6206  x: [-99.99955789  24.99993971]  f(x): 1.9909730747934812e-07  grad at x: [ 0.00088422 -0.00012058]  gradient norm: 0.0008924064264209399\n",
            "iter: 6207  x: [-99.99955877  24.99993983]  f(x): 1.9830171463662082e-07  grad at x: [ 0.00088245 -0.00012033]  gradient norm: 0.0008906216135635173\n",
            "iter: 6208  x: [-99.99955966  24.99993995]  f(x): 1.9750930099090014e-07  grad at x: [ 0.00088069 -0.00012009]  gradient norm: 0.0008888403703498174\n",
            "iter: 6209  x: [-99.99956054  24.99994007]  f(x): 1.967200538240897e-07  grad at x: [ 0.00087893 -0.00011985]  gradient norm: 0.0008870626896090032\n",
            "iter: 6210  x: [-99.99956141  24.99994019]  f(x): 1.9593396048800604e-07  grad at x: [ 0.00087717 -0.00011961]  gradient norm: 0.0008852885642275202\n",
            "iter: 6211  x: [-99.99956229  24.99994031]  f(x): 1.9515100837869252e-07  grad at x: [ 0.00087542 -0.00011937]  gradient norm: 0.0008835179870918136\n",
            "iter: 6212  x: [-99.99956317  24.99994043]  f(x): 1.9437118494870874e-07  grad at x: [ 0.00087367 -0.00011914]  gradient norm: 0.0008817509511164901\n",
            "iter: 6213  x: [-99.99956404  24.99994055]  f(x): 1.9359447769448924e-07  grad at x: [ 0.00087192 -0.0001189 ]  gradient norm: 0.0008799874492161561\n",
            "iter: 6214  x: [-99.99956491  24.99994067]  f(x): 1.9282087415621841e-07  grad at x: [ 0.00087017 -0.00011866]  gradient norm: 0.000878227474305418\n",
            "iter: 6215  x: [-99.99956578  24.99994079]  f(x): 1.920503619432293e-07  grad at x: [ 0.00086843 -0.00011842]  gradient norm: 0.0008764710193571247\n",
            "iter: 6216  x: [-99.99956665  24.99994091]  f(x): 1.912829286954141e-07  grad at x: [ 0.0008667  -0.00011819]  gradient norm: 0.0008747180773150035\n",
            "iter: 6217  x: [-99.99956752  24.99994103]  f(x): 1.9051856210823116e-07  grad at x: [ 0.00086496 -0.00011795]  gradient norm: 0.0008729686411509434\n",
            "iter: 6218  x: [-99.99956838  24.99994114]  f(x): 1.8975724993287598e-07  grad at x: [ 0.00086323 -0.00011771]  gradient norm: 0.0008712227038659541\n",
            "iter: 6219  x: [-99.99956925  24.99994126]  f(x): 1.8899897996295418e-07  grad at x: [ 0.00086151 -0.00011748]  gradient norm: 0.0008694802584600853\n",
            "iter: 6220  x: [-99.99957011  24.99994138]  f(x): 1.8824374003561277e-07  grad at x: [ 0.00085978 -0.00011724]  gradient norm: 0.000867741297935307\n",
            "iter: 6221  x: [-99.99957097  24.9999415 ]  f(x): 1.87491518054966e-07  grad at x: [ 0.00085806 -0.00011701]  gradient norm: 0.000866005815349911\n",
            "iter: 6222  x: [-99.99957183  24.99994161]  f(x): 1.8674230195480653e-07  grad at x: [ 0.00085635 -0.00011677]  gradient norm: 0.0008642738037330682\n",
            "iter: 6223  x: [-99.99957268  24.99994173]  f(x): 1.8599607971158387e-07  grad at x: [ 0.00085464 -0.00011654]  gradient norm: 0.0008625452561149099\n",
            "iter: 6224  x: [-99.99957354  24.99994185]  f(x): 1.8525283938022845e-07  grad at x: [ 0.00085293 -0.00011631]  gradient norm: 0.00086082016561005\n",
            "iter: 6225  x: [-99.99957439  24.99994196]  f(x): 1.8451256903359714e-07  grad at x: [ 0.00085122 -0.00011608]  gradient norm: 0.0008590985252777406\n",
            "iter: 6226  x: [-99.99957524  24.99994208]  f(x): 1.8377525681047e-07  grad at x: [ 0.00084952 -0.00011584]  gradient norm: 0.0008573803282335558\n",
            "iter: 6227  x: [-99.99957609  24.99994219]  f(x): 1.830408908790445e-07  grad at x: [ 0.00084782 -0.00011561]  gradient norm: 0.000855665567564909\n",
            "iter: 6228  x: [-99.99957694  24.99994231]  f(x): 1.8230945948503422e-07  grad at x: [ 0.00084612 -0.00011538]  gradient norm: 0.0008539542364436966\n",
            "iter: 6229  x: [-99.99957778  24.99994243]  f(x): 1.8158095087918888e-07  grad at x: [ 0.00084443 -0.00011515]  gradient norm: 0.0008522463279573315\n",
            "iter: 6230  x: [-99.99957863  24.99994254]  f(x): 1.808553534021866e-07  grad at x: [ 0.00084274 -0.00011492]  gradient norm: 0.0008505418353077915\n",
            "iter: 6231  x: [-99.99957947  24.99994266]  f(x): 1.8013265541068576e-07  grad at x: [ 0.00084106 -0.00011469]  gradient norm: 0.0008488407516388119\n",
            "iter: 6232  x: [-99.99958031  24.99994277]  f(x): 1.7941284531486762e-07  grad at x: [ 0.00083937 -0.00011446]  gradient norm: 0.000847143070124209\n",
            "iter: 6233  x: [-99.99958115  24.99994288]  f(x): 1.786959115892847e-07  grad at x: [ 0.0008377  -0.00011423]  gradient norm: 0.0008454487839941215\n",
            "iter: 6234  x: [-99.99958199  24.999943  ]  f(x): 1.779818427245796e-07  grad at x: [ 0.00083602 -0.000114  ]  gradient norm: 0.0008437578864214061\n",
            "iter: 6235  x: [-99.99958283  24.99994311]  f(x): 1.7727062727584734e-07  grad at x: [ 0.00083435 -0.00011377]  gradient norm: 0.0008420703706362013\n",
            "iter: 6236  x: [-99.99958366  24.99994323]  f(x): 1.7656225385044552e-07  grad at x: [ 0.00083268 -0.00011355]  gradient norm: 0.0008403862298977668\n",
            "iter: 6237  x: [-99.99958449  24.99994334]  f(x): 1.7585671108373624e-07  grad at x: [ 0.00083101 -0.00011332]  gradient norm: 0.0008387054574372012\n",
            "iter: 6238  x: [-99.99958532  24.99994345]  f(x): 1.751539876622604e-07  grad at x: [ 0.00082935 -0.00011309]  gradient norm: 0.0008370280465128045\n",
            "iter: 6239  x: [-99.99958615  24.99994357]  f(x): 1.744540723247129e-07  grad at x: [ 0.00082769 -0.00011287]  gradient norm: 0.0008353539904129575\n",
            "iter: 6240  x: [-99.99958698  24.99994368]  f(x): 1.737569538487644e-07  grad at x: [ 0.00082604 -0.00011264]  gradient norm: 0.0008336832824250811\n",
            "iter: 6241  x: [-99.99958781  24.99994379]  f(x): 1.730626210634671e-07  grad at x: [ 0.00082439 -0.00011242]  gradient norm: 0.0008320159158657173\n",
            "iter: 6242  x: [-99.99958863  24.9999439 ]  f(x): 1.7237106282484033e-07  grad at x: [ 0.00082274 -0.00011219]  gradient norm: 0.0008303518840222868\n",
            "iter: 6243  x: [-99.99958945  24.99994402]  f(x): 1.716822680520787e-07  grad at x: [ 0.00082109 -0.00011197]  gradient norm: 0.0008286911802404529\n",
            "iter: 6244  x: [-99.99959027  24.99994413]  f(x): 1.7099622571436284e-07  grad at x: [ 0.00081945 -0.00011174]  gradient norm: 0.0008270337978930796\n",
            "iter: 6245  x: [-99.99959109  24.99994424]  f(x): 1.703129247965633e-07  grad at x: [ 0.00081781 -0.00011152]  gradient norm: 0.0008253797302976692\n",
            "iter: 6246  x: [-99.99959191  24.99994435]  f(x): 1.6963235434535784e-07  grad at x: [ 0.00081618 -0.0001113 ]  gradient norm: 0.0008237289708280457\n",
            "iter: 6247  x: [-99.99959273  24.99994446]  f(x): 1.6895450345731355e-07  grad at x: [ 0.00081454 -0.00011107]  gradient norm: 0.0008220815128861945\n",
            "iter: 6248  x: [-99.99959354  24.99994457]  f(x): 1.6827936126710936e-07  grad at x: [ 0.00081291 -0.00011085]  gradient norm: 0.0008204373498741006\n",
            "iter: 6249  x: [-99.99959436  24.99994468]  f(x): 1.6760691693668417e-07  grad at x: [ 0.00081129 -0.00011063]  gradient norm: 0.0008187964751675087\n",
            "iter: 6250  x: [-99.99959517  24.9999448 ]  f(x): 1.6693715969969085e-07  grad at x: [ 0.00080967 -0.00011041]  gradient norm: 0.0008171588822247259\n",
            "iter: 6251  x: [-99.99959598  24.99994491]  f(x): 1.6627007880480956e-07  grad at x: [ 0.00080805 -0.00011019]  gradient norm: 0.0008155245644486978\n",
            "iter: 6252  x: [-99.99959678  24.99994502]  f(x): 1.6560566357316618e-07  grad at x: [ 0.00080643 -0.00010997]  gradient norm: 0.0008138935153278129\n",
            "iter: 6253  x: [-99.99959759  24.99994513]  f(x): 1.6494390333991153e-07  grad at x: [ 0.00080482 -0.00010975]  gradient norm: 0.0008122657282931776\n",
            "iter: 6254  x: [-99.9995984   24.99994524]  f(x): 1.6428478750078283e-07  grad at x: [ 0.00080321 -0.00010953]  gradient norm: 0.0008106411968331805\n",
            "iter: 6255  x: [-99.9995992   24.99994535]  f(x): 1.6362830548898162e-07  grad at x: [ 0.0008016  -0.00010931]  gradient norm: 0.0008090199144371704\n",
            "iter: 6256  x: [-99.9996      24.99994545]  f(x): 1.6297444678565757e-07  grad at x: [ 0.0008     -0.00010909]  gradient norm: 0.0008074018746216968\n",
            "iter: 6257  x: [-99.9996008   24.99994556]  f(x): 1.6232320089738797e-07  grad at x: [ 0.0007984  -0.00010887]  gradient norm: 0.0008057870708751487\n",
            "iter: 6258  x: [-99.9996016   24.99994567]  f(x): 1.6167455739092773e-07  grad at x: [ 0.0007968  -0.00010865]  gradient norm: 0.0008041754967441565\n",
            "iter: 6259  x: [-99.9996024   24.99994578]  f(x): 1.6102850585781272e-07  grad at x: [ 0.00079521 -0.00010844]  gradient norm: 0.0008025671457462303\n",
            "iter: 6260  x: [-99.99960319  24.99994589]  f(x): 1.603850359485907e-07  grad at x: [ 0.00079362 -0.00010822]  gradient norm: 0.0008009620114552018\n",
            "iter: 6261  x: [-99.99960398  24.999946  ]  f(x): 1.5974413735036448e-07  grad at x: [ 0.00079203 -0.000108  ]  gradient norm: 0.0007993600874458631\n",
            "iter: 6262  x: [-99.99960478  24.99994611]  f(x): 1.5910579977506877e-07  grad at x: [ 0.00079045 -0.00010779]  gradient norm: 0.0007977613672648451\n",
            "iter: 6263  x: [-99.99960557  24.99994621]  f(x): 1.5847001300393916e-07  grad at x: [ 0.00078887 -0.00010757]  gradient norm: 0.0007961658445423018\n",
            "iter: 6264  x: [-99.99960636  24.99994632]  f(x): 1.5783676683266196e-07  grad at x: [ 0.00078729 -0.00010736]  gradient norm: 0.0007945735128549453\n",
            "iter: 6265  x: [-99.99960714  24.99994643]  f(x): 1.5720605111424403e-07  grad at x: [ 0.00078571 -0.00010714]  gradient norm: 0.0007929843658338896\n",
            "iter: 6266  x: [-99.99960793  24.99994654]  f(x): 1.5657785573754774e-07  grad at x: [ 0.00078414 -0.00010693]  gradient norm: 0.0007913983971112091\n",
            "iter: 6267  x: [-99.99960871  24.99994664]  f(x): 1.5595217062680682e-07  grad at x: [ 0.00078257 -0.00010671]  gradient norm: 0.0007898156003189778\n",
            "iter: 6268  x: [-99.9996095   24.99994675]  f(x): 1.553289857526239e-07  grad at x: [ 0.00078101 -0.0001065 ]  gradient norm: 0.0007882359691174309\n",
            "iter: 6269  x: [-99.99961028  24.99994686]  f(x): 1.547082911210598e-07  grad at x: [ 0.00077945 -0.00010629]  gradient norm: 0.000786659497167764\n",
            "iter: 6270  x: [-99.99961106  24.99994696]  f(x): 1.540900767952615e-07  grad at x: [ 0.00077789 -0.00010608]  gradient norm: 0.0007850861781874943\n",
            "iter: 6271  x: [-99.99961183  24.99994707]  f(x): 1.5347433285063549e-07  grad at x: [ 0.00077633 -0.00010586]  gradient norm: 0.0007835160058368571\n",
            "iter: 6272  x: [-99.99961261  24.99994717]  f(x): 1.5286104942051575e-07  grad at x: [ 0.00077478 -0.00010565]  gradient norm: 0.0007819489738352899\n",
            "iter: 6273  x: [-99.99961339  24.99994728]  f(x): 1.5225021667160876e-07  grad at x: [ 0.00077323 -0.00010544]  gradient norm: 0.0007803850758993505\n",
            "iter: 6274  x: [-99.99961416  24.99994739]  f(x): 1.516418248061489e-07  grad at x: [ 0.00077168 -0.00010523]  gradient norm: 0.0007788243057484759\n",
            "iter: 6275  x: [-99.99961493  24.99994749]  f(x): 1.5103586407124117e-07  grad at x: [ 0.00077014 -0.00010502]  gradient norm: 0.000777266657129305\n",
            "iter: 6276  x: [-99.9996157  24.9999476]  f(x): 1.5043232475902963e-07  grad at x: [ 0.0007686  -0.00010481]  gradient norm: 0.0007757121238166376\n",
            "iter: 6277  x: [-99.99961647  24.9999477 ]  f(x): 1.4983119718468927e-07  grad at x: [ 0.00076706 -0.0001046 ]  gradient norm: 0.0007741606995571121\n",
            "iter: 6278  x: [-99.99961724  24.9999478 ]  f(x): 1.4923247171944566e-07  grad at x: [ 0.00076553 -0.00010439]  gradient norm: 0.0007726123781546492\n",
            "iter: 6279  x: [-99.999618    24.99994791]  f(x): 1.4863613875731517e-07  grad at x: [ 0.000764   -0.00010418]  gradient norm: 0.0007710671533850088\n",
            "iter: 6280  x: [-99.99961877  24.99994801]  f(x): 1.4804218874799295e-07  grad at x: [ 0.00076247 -0.00010397]  gradient norm: 0.0007695250190812329\n",
            "iter: 6281  x: [-99.99961953  24.99994812]  f(x): 1.4745061216335786e-07  grad at x: [ 0.00076094 -0.00010377]  gradient norm: 0.0007679859690472421\n",
            "iter: 6282  x: [-99.99962029  24.99994822]  f(x): 1.4686139951944123e-07  grad at x: [ 0.00075942 -0.00010356]  gradient norm: 0.0007664499971151183\n",
            "iter: 6283  x: [-99.99962105  24.99994832]  f(x): 1.4627454136618728e-07  grad at x: [ 0.0007579  -0.00010335]  gradient norm: 0.0007649170971188637\n",
            "iter: 6284  x: [-99.99962181  24.99994843]  f(x): 1.4569002829663417e-07  grad at x: [ 0.00075639 -0.00010314]  gradient norm: 0.0007633872629187211\n",
            "iter: 6285  x: [-99.99962256  24.99994853]  f(x): 1.4510785094854759e-07  grad at x: [ 0.00075487 -0.00010294]  gradient norm: 0.0007618604884059747\n",
            "iter: 6286  x: [-99.99962332  24.99994863]  f(x): 1.4452799998060698e-07  grad at x: [ 0.00075336 -0.00010273]  gradient norm: 0.0007603367674408676\n",
            "iter: 6287  x: [-99.99962407  24.99994874]  f(x): 1.4395046609598457e-07  grad at x: [ 0.00075186 -0.00010253]  gradient norm: 0.0007588160939146838\n",
            "iter: 6288  x: [-99.99962482  24.99994884]  f(x): 1.4337524003001665e-07  grad at x: [ 0.00075035 -0.00010232]  gradient norm: 0.0007572984617177475\n",
            "iter: 6289  x: [-99.99962557  24.99994894]  f(x): 1.4280231257176106e-07  grad at x: [ 0.00074885 -0.00010212]  gradient norm: 0.0007557838647967052\n",
            "iter: 6290  x: [-99.99962632  24.99994904]  f(x): 1.422316745318293e-07  grad at x: [ 0.00074736 -0.00010191]  gradient norm: 0.0007542722970700417\n",
            "iter: 6291  x: [-99.99962707  24.99994915]  f(x): 1.4166331676432282e-07  grad at x: [ 0.00074586 -0.00010171]  gradient norm: 0.0007527637524863237\n",
            "iter: 6292  x: [-99.99962782  24.99994925]  f(x): 1.4109723015460686e-07  grad at x: [ 0.00074437 -0.0001015 ]  gradient norm: 0.0007512582249921976\n",
            "iter: 6293  x: [-99.99962856  24.99994935]  f(x): 1.4053340562066593e-07  grad at x: [ 0.00074288 -0.0001013 ]  gradient norm: 0.0007497557085362296\n",
            "iter: 6294  x: [-99.9996293   24.99994945]  f(x): 1.3997183413299967e-07  grad at x: [ 0.00074139 -0.0001011 ]  gradient norm: 0.0007482561971223484\n",
            "iter: 6295  x: [-99.99963004  24.99994955]  f(x): 1.394125066834971e-07  grad at x: [ 0.00073991 -0.0001009 ]  gradient norm: 0.0007467596847272812\n",
            "iter: 6296  x: [-99.99963078  24.99994965]  f(x): 1.388554143060782e-07  grad at x: [ 0.00073843 -0.0001007 ]  gradient norm: 0.0007452661653559169\n",
            "iter: 6297  x: [-99.99963152  24.99994975]  f(x): 1.3830054806602565e-07  grad at x: [ 0.00073696 -0.00010049]  gradient norm: 0.0007437756330131437\n",
            "iter: 6298  x: [-99.99963226  24.99994985]  f(x): 1.377478990807992e-07  grad at x: [ 0.00073548 -0.00010029]  gradient norm: 0.0007422880817601727\n",
            "iter: 6299  x: [-99.99963299  24.99994995]  f(x): 1.3719745847836877e-07  grad at x: [ 0.00073401 -0.00010009]  gradient norm: 0.0007408035056028522\n",
            "iter: 6300  x: [-99.99963373  24.99995005]  f(x): 1.366492174386167e-07  grad at x: [ 7.32542441e-04 -9.98921511e-05]  gradient norm: 0.0007393218986033531\n",
            "iter: 6301  x: [-99.99963446  24.99995015]  f(x): 1.3610316716187264e-07  grad at x: [ 7.31077356e-04 -9.96923668e-05]  gradient norm: 0.0007378432547956853\n",
            "iter: 6302  x: [-99.99963519  24.99995025]  f(x): 1.3555929891040313e-07  grad at x: [ 7.29615201e-04 -9.94929820e-05]  gradient norm: 0.0007363675682983415\n",
            "iter: 6303  x: [-99.99963592  24.99995035]  f(x): 1.3501760395662656e-07  grad at x: [ 7.28155971e-04 -9.92939961e-05]  gradient norm: 0.0007348948331744524\n",
            "iter: 6304  x: [-99.99963665  24.99995045]  f(x): 1.3447807361385867e-07  grad at x: [ 7.26699659e-04 -9.90954081e-05]  gradient norm: 0.0007334250435153102\n",
            "iter: 6305  x: [-99.99963738  24.99995055]  f(x): 1.3394069923577036e-07  grad at x: [ 7.25246260e-04 -9.88972173e-05]  gradient norm: 0.0007319581934394077\n",
            "iter: 6306  x: [-99.9996381   24.99995065]  f(x): 1.3340547219668165e-07  grad at x: [ 7.23795767e-04 -9.86994228e-05]  gradient norm: 0.0007304942770389968\n",
            "iter: 6307  x: [-99.99963883  24.99995075]  f(x): 1.3287238393158223e-07  grad at x: [ 7.22348176e-04 -9.85020240e-05]  gradient norm: 0.0007290332884898528\n",
            "iter: 6308  x: [-99.99963955  24.99995085]  f(x): 1.3234142588521538e-07  grad at x: [ 7.20903479e-04 -9.83050199e-05]  gradient norm: 0.0007275752219123886\n",
            "iter: 6309  x: [-99.99964027  24.99995095]  f(x): 1.3181258954251834e-07  grad at x: [ 7.19461672e-04 -9.81084099e-05]  gradient norm: 0.0007261200714551784\n",
            "iter: 6310  x: [-99.99964099  24.99995104]  f(x): 1.3128586643863788e-07  grad at x: [ 7.18022749e-04 -9.79121931e-05]  gradient norm: 0.0007246678313231183\n",
            "iter: 6311  x: [-99.99964171  24.99995114]  f(x): 1.307612481179078e-07  grad at x: [ 7.16586704e-04 -9.77163687e-05]  gradient norm: 0.0007232184956647826\n",
            "iter: 6312  x: [-99.99964242  24.99995124]  f(x): 1.3023872617462017e-07  grad at x: [ 7.1515353e-04 -9.7520936e-05]  gradient norm: 0.0007217720586850676\n",
            "iter: 6313  x: [-99.99964314  24.99995134]  f(x): 1.2971829222300072e-07  grad at x: [ 7.13723223e-04 -9.73258941e-05]  gradient norm: 0.0007203285145626285\n",
            "iter: 6314  x: [-99.99964385  24.99995143]  f(x): 1.2919993792620626e-07  grad at x: [ 7.12295777e-04 -9.71312423e-05]  gradient norm: 0.0007188878575305227\n",
            "iter: 6315  x: [-99.99964456  24.99995153]  f(x): 1.2868365497721834e-07  grad at x: [ 7.10871185e-04 -9.69369798e-05]  gradient norm: 0.0007174500818237276\n",
            "iter: 6316  x: [-99.99964528  24.99995163]  f(x): 1.281694350876402e-07  grad at x: [ 7.09449443e-04 -9.67431058e-05]  gradient norm: 0.0007160151816480994\n",
            "iter: 6317  x: [-99.99964598  24.99995173]  f(x): 1.2765727002865106e-07  grad at x: [ 7.08030544e-04 -9.65496196e-05]  gradient norm: 0.000714583151294938\n",
            "iter: 6318  x: [-99.99964669  24.99995182]  f(x): 1.2714715157972487e-07  grad at x: [ 7.06614483e-04 -9.63565204e-05]  gradient norm: 0.0007131539849982607\n",
            "iter: 6319  x: [-99.9996474   24.99995192]  f(x): 1.2663907155984216e-07  grad at x: [ 7.05201254e-04 -9.61638074e-05]  gradient norm: 0.000711727677022166\n",
            "iter: 6320  x: [-99.9996481   24.99995201]  f(x): 1.261330218262778e-07  grad at x: [ 7.03790851e-04 -9.59714798e-05]  gradient norm: 0.0007103042216579535\n",
            "iter: 6321  x: [-99.99964881  24.99995211]  f(x): 1.2562899427476272e-07  grad at x: [ 7.02383270e-04 -9.57795368e-05]  gradient norm: 0.0007088836132250842\n",
            "iter: 6322  x: [-99.99964951  24.99995221]  f(x): 1.2512698080975755e-07  grad at x: [ 7.00978503e-04 -9.55879777e-05]  gradient norm: 0.0007074658459876563\n",
            "iter: 6323  x: [-99.99965021  24.9999523 ]  f(x): 1.2462697339393724e-07  grad at x: [ 6.99576546e-04 -9.53968018e-05]  gradient norm: 0.0007060509142942518\n",
            "iter: 6324  x: [-99.99965091  24.9999524 ]  f(x): 1.2412896400812407e-07  grad at x: [ 6.98177393e-04 -9.52060082e-05]  gradient norm: 0.0007046388124652915\n",
            "iter: 6325  x: [-99.99965161  24.99995249]  f(x): 1.2363294467144835e-07  grad at x: [ 6.96781038e-04 -9.50155961e-05]  gradient norm: 0.0007032295348503171\n",
            "iter: 6326  x: [-99.99965231  24.99995259]  f(x): 1.231389074207279e-07  grad at x: [ 6.95387476e-04 -9.48255649e-05]  gradient norm: 0.0007018230757697495\n",
            "iter: 6327  x: [-99.999653    24.99995268]  f(x): 1.2264684435061197e-07  grad at x: [ 6.93996701e-04 -9.46359138e-05]  gradient norm: 0.0007004194296294528\n",
            "iter: 6328  x: [-99.9996537   24.99995278]  f(x): 1.2215674756365374e-07  grad at x: [ 6.92608708e-04 -9.44466420e-05]  gradient norm: 0.0006990185907789684\n",
            "iter: 6329  x: [-99.99965439  24.99995287]  f(x): 1.2166860920023222e-07  grad at x: [ 6.91223490e-04 -9.42577487e-05]  gradient norm: 0.00069762055359696\n",
            "iter: 6330  x: [-99.99965508  24.99995297]  f(x): 1.2118242143736858e-07  grad at x: [ 6.89841043e-04 -9.40692332e-05]  gradient norm: 0.0006962253124883312\n",
            "iter: 6331  x: [-99.99965577  24.99995306]  f(x): 1.2069817648044033e-07  grad at x: [ 6.88461361e-04 -9.38810947e-05]  gradient norm: 0.000694832861860866\n",
            "iter: 6332  x: [-99.99965646  24.99995315]  f(x): 1.2021586657119292e-07  grad at x: [ 6.87084439e-04 -9.36933325e-05]  gradient norm: 0.0006934431961485899\n",
            "iter: 6333  x: [-99.99965714  24.99995325]  f(x): 1.1973548396907826e-07  grad at x: [ 6.85710270e-04 -9.35059459e-05]  gradient norm: 0.0006920563097583267\n",
            "iter: 6334  x: [-99.99965783  24.99995334]  f(x): 1.1925702097072156e-07  grad at x: [ 6.84338849e-04 -9.33189340e-05]  gradient norm: 0.0006906721971260217\n",
            "iter: 6335  x: [-99.99965851  24.99995343]  f(x): 1.1878046991878691e-07  grad at x: [ 6.82970171e-04 -9.31322961e-05]  gradient norm: 0.0006892908527429823\n",
            "iter: 6336  x: [-99.9996592   24.99995353]  f(x): 1.183058231636252e-07  grad at x: [ 6.81604231e-04 -9.29460315e-05]  gradient norm: 0.0006879122710451535\n",
            "iter: 6337  x: [-99.99965988  24.99995362]  f(x): 1.1783307309205911e-07  grad at x: [ 6.80241023e-04 -9.27601395e-05]  gradient norm: 0.0006865364464966419\n",
            "iter: 6338  x: [-99.99966056  24.99995371]  f(x): 1.1736221212720971e-07  grad at x: [ 6.78880541e-04 -9.25746192e-05]  gradient norm: 0.0006851633735897146\n",
            "iter: 6339  x: [-99.99966124  24.99995381]  f(x): 1.168932327283238e-07  grad at x: [ 6.77522780e-04 -9.23894699e-05]  gradient norm: 0.0006837930468448003\n",
            "iter: 6340  x: [-99.99966192  24.9999539 ]  f(x): 1.1642612737171157e-07  grad at x: [ 6.76167734e-04 -9.22046910e-05]  gradient norm: 0.0006824254607551262\n",
            "iter: 6341  x: [-99.99966259  24.99995399]  f(x): 1.1596088856961679e-07  grad at x: [ 6.74815398e-04 -9.20202816e-05]  gradient norm: 0.0006810606098420808\n",
            "iter: 6342  x: [-99.99966327  24.99995408]  f(x): 1.1549750886014872e-07  grad at x: [ 6.73465768e-04 -9.18362411e-05]  gradient norm: 0.0006796984886260929\n",
            "iter: 6343  x: [-99.99966394  24.99995417]  f(x): 1.150359808177388e-07  grad at x: [ 6.72118836e-04 -9.16525686e-05]  gradient norm: 0.0006783390916576717\n",
            "iter: 6344  x: [-99.99966461  24.99995427]  f(x): 1.1457629704245662e-07  grad at x: [ 6.70774599e-04 -9.14692634e-05]  gradient norm: 0.0006769824134863671\n",
            "iter: 6345  x: [-99.99966528  24.99995436]  f(x): 1.141184501605878e-07  grad at x: [ 6.69433049e-04 -9.12863249e-05]  gradient norm: 0.0006756284486626885\n",
            "iter: 6346  x: [-99.99966595  24.99995445]  f(x): 1.136624328337271e-07  grad at x: [ 6.68094183e-04 -9.11037523e-05]  gradient norm: 0.0006742771917653069\n",
            "iter: 6347  x: [-99.99966662  24.99995454]  f(x): 1.1320823774881153e-07  grad at x: [ 6.66757995e-04 -9.09215447e-05]  gradient norm: 0.0006729286373719327\n",
            "iter: 6348  x: [-99.99966729  24.99995463]  f(x): 1.1275585762847348e-07  grad at x: [ 6.65424479e-04 -9.07397017e-05]  gradient norm: 0.0006715827800903579\n",
            "iter: 6349  x: [-99.99966795  24.99995472]  f(x): 1.1230528522078589e-07  grad at x: [ 6.64093630e-04 -9.05582223e-05]  gradient norm: 0.0006702396145283741\n",
            "iter: 6350  x: [-99.99966862  24.99995481]  f(x): 1.1185651329854731e-07  grad at x: [ 6.62765443e-04 -9.03771058e-05]  gradient norm: 0.0006688991352918534\n",
            "iter: 6351  x: [-99.99966928  24.9999549 ]  f(x): 1.1140953467021768e-07  grad at x: [ 6.61439912e-04 -9.01963516e-05]  gradient norm: 0.0006675613370177087\n",
            "iter: 6352  x: [-99.99966994  24.99995499]  f(x): 1.1096434216908237e-07  grad at x: [ 6.60117032e-04 -9.00159589e-05]  gradient norm: 0.0006662262143418927\n",
            "iter: 6353  x: [-99.9996706   24.99995508]  f(x): 1.1052092865350265e-07  grad at x: [ 6.58796798e-04 -8.98359270e-05]  gradient norm: 0.0006648937619003585\n",
            "iter: 6354  x: [-99.99967126  24.99995517]  f(x): 1.1007928702584921e-07  grad at x: [ 6.57479204e-04 -8.96562551e-05]  gradient norm: 0.0006635639743863412\n",
            "iter: 6355  x: [-99.99967192  24.99995526]  f(x): 1.0963941019430626e-07  grad at x: [ 6.56164246e-04 -8.94769426e-05]  gradient norm: 0.0006622368464357937\n",
            "iter: 6356  x: [-99.99967257  24.99995535]  f(x): 1.0920129111084895e-07  grad at x: [ 6.54851917e-04 -8.92979887e-05]  gradient norm: 0.0006609123727419512\n",
            "iter: 6357  x: [-99.99967323  24.99995544]  f(x): 1.0876492275209158e-07  grad at x: [ 6.53542214e-04 -8.91193928e-05]  gradient norm: 0.0006595905479980488\n",
            "iter: 6358  x: [-99.99967388  24.99995553]  f(x): 1.0833029811921706e-07  grad at x: [ 6.52235129e-04 -8.89411540e-05]  gradient norm: 0.0006582713668973216\n",
            "iter: 6359  x: [-99.99967453  24.99995562]  f(x): 1.0789741024747246e-07  grad at x: [ 6.50930659e-04 -8.87632717e-05]  gradient norm: 0.000656954824162126\n",
            "iter: 6360  x: [-99.99967519  24.99995571]  f(x): 1.0746625219614204e-07  grad at x: [ 6.49628798e-04 -8.85857451e-05]  gradient norm: 0.0006556409145138581\n",
            "iter: 6361  x: [-99.99967584  24.9999558 ]  f(x): 1.0703681704910892e-07  grad at x: [ 6.48329540e-04 -8.84085736e-05]  gradient norm: 0.0006543296326748741\n",
            "iter: 6362  x: [-99.99967648  24.99995588]  f(x): 1.0660909792397757e-07  grad at x: [ 6.47032881e-04 -8.82317565e-05]  gradient norm: 0.0006530209733966515\n",
            "iter: 6363  x: [-99.99967713  24.99995597]  f(x): 1.0618308797097081e-07  grad at x: [ 6.45738815e-04 -8.80552930e-05]  gradient norm: 0.0006517149314569087\n",
            "iter: 6364  x: [-99.99967778  24.99995606]  f(x): 1.0575878035570817e-07  grad at x: [ 6.44447337e-04 -8.78791824e-05]  gradient norm: 0.0006504115016071231\n",
            "iter: 6365  x: [-99.99967842  24.99995615]  f(x): 1.0533616826773958e-07  grad at x: [ 6.43158443e-04 -8.77034240e-05]  gradient norm: 0.0006491106785987721\n",
            "iter: 6366  x: [-99.99967906  24.99995624]  f(x): 1.0491524493871998e-07  grad at x: [ 6.41872126e-04 -8.75280172e-05]  gradient norm: 0.0006478124572396551\n",
            "iter: 6367  x: [-99.99967971  24.99995632]  f(x): 1.0449600362364162e-07  grad at x: [ 6.40588382e-04 -8.73529611e-05]  gradient norm: 0.0006465168323366117\n",
            "iter: 6368  x: [-99.99968035  24.99995641]  f(x): 1.0407843759261376e-07  grad at x: [ 6.39307205e-04 -8.71782552e-05]  gradient norm: 0.0006452237986702405\n",
            "iter: 6369  x: [-99.99968099  24.9999565 ]  f(x): 1.0366254015746252e-07  grad at x: [ 6.38028590e-04 -8.70038987e-05]  gradient norm: 0.0006439333510774621\n",
            "iter: 6370  x: [-99.99968162  24.99995659]  f(x): 1.0324830464402714e-07  grad at x: [ 6.36752533e-04 -8.68298909e-05]  gradient norm: 0.0006426454843660761\n",
            "iter: 6371  x: [-99.99968226  24.99995667]  f(x): 1.0283572441958685e-07  grad at x: [ 6.35479028e-04 -8.66562311e-05]  gradient norm: 0.0006413601934002043\n",
            "iter: 6372  x: [-99.9996829   24.99995676]  f(x): 1.0242479286618982e-07  grad at x: [ 6.34208070e-04 -8.64829186e-05]  gradient norm: 0.0006400774730177272\n",
            "iter: 6373  x: [-99.99968353  24.99995685]  f(x): 1.020155033977405e-07  grad at x: [ 6.32939654e-04 -8.63099528e-05]  gradient norm: 0.0006387973180837268\n",
            "iter: 6374  x: [-99.99968416  24.99995693]  f(x): 1.0160784944250515e-07  grad at x: [ 6.31673775e-04 -8.61373329e-05]  gradient norm: 0.0006375197234360836\n",
            "iter: 6375  x: [-99.99968479  24.99995702]  f(x): 1.0120182447835669e-07  grad at x: [ 6.30410427e-04 -8.59650582e-05]  gradient norm: 0.0006362446839962018\n",
            "iter: 6376  x: [-99.99968543  24.9999571 ]  f(x): 1.0079742198834874e-07  grad at x: [ 6.29149606e-04 -8.57931281e-05]  gradient norm: 0.0006349721946301231\n",
            "iter: 6377  x: [-99.99968605  24.99995719]  f(x): 1.0039463548729105e-07  grad at x: [ 6.27891307e-04 -8.56215419e-05]  gradient norm: 0.0006337022502320503\n",
            "iter: 6378  x: [-99.99968668  24.99995727]  f(x): 9.999345852159484e-08  grad at x: [ 6.26635524e-04 -8.54502988e-05]  gradient norm: 0.0006324348457243476\n",
            "iter: 6379  x: [-99.99968731  24.99995736]  f(x): 9.959388466053461e-08  grad at x: [ 6.25382253e-04 -8.52793982e-05]  gradient norm: 0.0006311699760303388\n",
            "iter: 6380  x: [-99.99968793  24.99995745]  f(x): 9.919590749557599e-08  grad at x: [ 6.24131489e-04 -8.51088394e-05]  gradient norm: 0.000629907636072388\n",
            "iter: 6381  x: [-99.99968856  24.99995753]  f(x): 9.879952064976966e-08  grad at x: [ 6.22883226e-04 -8.49386217e-05]  gradient norm: 0.0006286478208019802\n",
            "iter: 6382  x: [-99.99968918  24.99995762]  f(x): 9.840471776846052e-08  grad at x: [ 6.21637459e-04 -8.47687444e-05]  gradient norm: 0.0006273905251706006\n",
            "iter: 6383  x: [-99.9996898  24.9999577]  f(x): 9.801149251952462e-08  grad at x: [ 6.20394185e-04 -8.45992070e-05]  gradient norm: 0.0006261357441306944\n",
            "iter: 6384  x: [-99.99969042  24.99995778]  f(x): 9.761983859240263e-08  grad at x: [ 6.19153396e-04 -8.44300085e-05]  gradient norm: 0.0006248834726327865\n",
            "iter: 6385  x: [-99.99969104  24.99995787]  f(x): 9.722974971680226e-08  grad at x: [ 6.17915089e-04 -8.42611485e-05]  gradient norm: 0.0006236337056856445\n",
            "iter: 6386  x: [-99.99969166  24.99995795]  f(x): 9.6841219635565e-08  grad at x: [ 6.16679259e-04 -8.40926262e-05]  gradient norm: 0.0006223864382698742\n",
            "iter: 6387  x: [-99.99969228  24.99995804]  f(x): 9.645424212218354e-08  grad at x: [ 6.15445901e-04 -8.39244410e-05]  gradient norm: 0.0006211416653942434\n",
            "iter: 6388  x: [-99.99969289  24.99995812]  f(x): 9.606881097192299e-08  grad at x: [ 6.14215009e-04 -8.37565921e-05]  gradient norm: 0.000619899382067519\n",
            "iter: 6389  x: [-99.99969351  24.99995821]  f(x): 9.56849200017587e-08  grad at x: [ 6.12986579e-04 -8.35890789e-05]  gradient norm: 0.0006186595832984686\n",
            "iter: 6390  x: [-99.99969412  24.99995829]  f(x): 9.53025630593043e-08  grad at x: [ 6.11760606e-04 -8.34219007e-05]  gradient norm: 0.0006174222641249805\n",
            "iter: 6391  x: [-99.99969473  24.99995837]  f(x): 9.492173401339193e-08  grad at x: [ 6.10537084e-04 -8.32550569e-05]  gradient norm: 0.0006161874195839831\n",
            "iter: 6392  x: [-99.99969534  24.99995846]  f(x): 9.454242676355828e-08  grad at x: [ 6.09316010e-04 -8.30885468e-05]  gradient norm: 0.0006149550447424861\n",
            "iter: 6393  x: [-99.99969595  24.99995854]  f(x): 9.416463523007075e-08  grad at x: [ 6.08097378e-04 -8.29223697e-05]  gradient norm: 0.0006137251346655789\n",
            "iter: 6394  x: [-99.99969656  24.99995862]  f(x): 9.378835334642422e-08  grad at x: [ 6.06881184e-04 -8.27565250e-05]  gradient norm: 0.0006124976843921101\n",
            "iter: 6395  x: [-99.99969717  24.9999587 ]  f(x): 9.34135750843015e-08  grad at x: [ 6.05667421e-04 -8.25910119e-05]  gradient norm: 0.00061127268901629\n",
            "iter: 6396  x: [-99.99969777  24.99995879]  f(x): 9.304029443674931e-08  grad at x: [ 6.04456086e-04 -8.24258299e-05]  gradient norm: 0.0006100501436332896\n",
            "iter: 6397  x: [-99.99969838  24.99995887]  f(x): 9.266850541753052e-08  grad at x: [ 6.03247174e-04 -8.22609783e-05]  gradient norm: 0.0006088300433373193\n",
            "iter: 6398  x: [-99.99969898  24.99995895]  f(x): 9.229820207049768e-08  grad at x: [ 6.02040680e-04 -8.20964563e-05]  gradient norm: 0.000607612383252671\n",
            "iter: 6399  x: [-99.99969958  24.99995903]  f(x): 9.19293784514916e-08  grad at x: [ 6.00836598e-04 -8.19322634e-05]  gradient norm: 0.0006063971584745153\n",
            "iter: 6400  x: [-99.99970018  24.99995912]  f(x): 9.156202865422472e-08  grad at x: [ 5.99634925e-04 -8.17683989e-05]  gradient norm: 0.0006051843641543451\n",
            "iter: 6401  x: [-99.99970078  24.9999592 ]  f(x): 9.119614678482808e-08  grad at x: [ 5.98435655e-04 -8.16048621e-05]  gradient norm: 0.000603973995416452\n",
            "iter: 6402  x: [-99.99970138  24.99995928]  f(x): 9.083172697827049e-08  grad at x: [ 5.97238784e-04 -8.14416523e-05]  gradient norm: 0.0006027660474123289\n",
            "iter: 6403  x: [-99.99970198  24.99995936]  f(x): 9.046876339908373e-08  grad at x: [ 5.96044306e-04 -8.12787690e-05]  gradient norm: 0.0006015605153235499\n",
            "iter: 6404  x: [-99.99970257  24.99995944]  f(x): 9.010725022315405e-08  grad at x: [ 5.94852218e-04 -8.11162115e-05]  gradient norm: 0.0006003573943016078\n",
            "iter: 6405  x: [-99.99970317  24.99995952]  f(x): 8.97471816473411e-08  grad at x: [ 5.93662513e-04 -8.09539791e-05]  gradient norm: 0.0005991566794999154\n",
            "iter: 6406  x: [-99.99970376  24.9999596 ]  f(x): 8.938855190567899e-08  grad at x: [ 5.92475188e-04 -8.07920711e-05]  gradient norm: 0.0005979583661282079\n",
            "iter: 6407  x: [-99.99970435  24.99995968]  f(x): 8.903135525205764e-08  grad at x: [ 5.91290238e-04 -8.06304870e-05]  gradient norm: 0.0005967624493952603\n",
            "iter: 6408  x: [-99.99970495  24.99995977]  f(x): 8.867558596074001e-08  grad at x: [ 5.90107658e-04 -8.04692260e-05]  gradient norm: 0.0005955689245108076\n",
            "iter: 6409  x: [-99.99970554  24.99995985]  f(x): 8.832123831736202e-08  grad at x: [ 5.88927442e-04 -8.03082876e-05]  gradient norm: 0.0005943777866554639\n",
            "iter: 6410  x: [-99.99970613  24.99995993]  f(x): 8.796830664515816e-08  grad at x: [ 5.87749587e-04 -8.01476710e-05]  gradient norm: 0.0005931890310690452\n",
            "iter: 6411  x: [-99.99970671  24.99996001]  f(x): 8.761678529468771e-08  grad at x: [ 5.86574088e-04 -7.99873756e-05]  gradient norm: 0.0005920026530166489\n",
            "iter: 6412  x: [-99.9997073   24.99996009]  f(x): 8.726666862016404e-08  grad at x: [ 5.85400940e-04 -7.98274009e-05]  gradient norm: 0.0005908186477089701\n",
            "iter: 6413  x: [-99.99970788  24.99996017]  f(x): 8.691795101220275e-08  grad at x: [ 5.84230138e-04 -7.96677461e-05]  gradient norm: 0.0005896370104130261\n",
            "iter: 6414  x: [-99.99970847  24.99996025]  f(x): 8.657062688102723e-08  grad at x: [ 5.83061678e-04 -7.95084106e-05]  gradient norm: 0.0005884577363958341\n",
            "iter: 6415  x: [-99.99970905  24.99996033]  f(x): 8.622469065641265e-08  grad at x: [ 5.81895554e-04 -7.93493938e-05]  gradient norm: 0.0005872808209244114\n",
            "iter: 6416  x: [-99.99970963  24.9999604 ]  f(x): 8.588013679588277e-08  grad at x: [ 5.80731763e-04 -7.91906950e-05]  gradient norm: 0.0005861062592939364\n",
            "iter: 6417  x: [-99.99971021  24.99996048]  f(x): 8.553695976838013e-08  grad at x: [ 5.79570300e-04 -7.90323136e-05]  gradient norm: 0.0005849340467723866\n",
            "iter: 6418  x: [-99.99971079  24.99996056]  f(x): 8.519515407838891e-08  grad at x: [ 5.78411159e-04 -7.88742490e-05]  gradient norm: 0.0005837641786831012\n",
            "iter: 6419  x: [-99.99971137  24.99996064]  f(x): 8.485471424194704e-08  grad at x: [ 5.77254337e-04 -7.87165005e-05]  gradient norm: 0.0005825966503231787\n",
            "iter: 6420  x: [-99.99971195  24.99996072]  f(x): 8.451563480192514e-08  grad at x: [ 5.76099828e-04 -7.85590675e-05]  gradient norm: 0.0005814314570159587\n",
            "iter: 6421  x: [-99.99971253  24.9999608 ]  f(x): 8.417791032873096e-08  grad at x: [ 5.74947629e-04 -7.84019493e-05]  gradient norm: 0.0005802685941139016\n",
            "iter: 6422  x: [-99.9997131   24.99996088]  f(x): 8.384153539570785e-08  grad at x: [ 5.73797733e-04 -7.82451454e-05]  gradient norm: 0.000579108056914106\n",
            "iter: 6423  x: [-99.99971367  24.99996096]  f(x): 8.350650461965274e-08  grad at x: [ 5.72650138e-04 -7.80886551e-05]  gradient norm: 0.0005779498407981535\n",
            "iter: 6424  x: [-99.99971425  24.99996103]  f(x): 8.31728126277543e-08  grad at x: [ 5.71504838e-04 -7.79324778e-05]  gradient norm: 0.0005767939411185048\n",
            "iter: 6425  x: [-99.99971482  24.99996111]  f(x): 8.284045406628123e-08  grad at x: [ 5.70361828e-04 -7.77766129e-05]  gradient norm: 0.0005756403532285805\n",
            "iter: 6426  x: [-99.99971539  24.99996119]  f(x): 8.250942360833916e-08  grad at x: [ 5.69221104e-04 -7.76210597e-05]  gradient norm: 0.0005744890725099622\n",
            "iter: 6427  x: [-99.99971596  24.99996127]  f(x): 8.217971595373607e-08  grad at x: [ 5.68082662e-04 -7.74658175e-05]  gradient norm: 0.0005733400943723928\n",
            "iter: 6428  x: [-99.99971653  24.99996134]  f(x): 8.185132580495267e-08  grad at x: [ 5.66946497e-04 -7.73108859e-05]  gradient norm: 0.000572193414170253\n",
            "iter: 6429  x: [-99.99971709  24.99996142]  f(x): 8.152424790690303e-08  grad at x: [ 5.65812604e-04 -7.71562641e-05]  gradient norm: 0.0005710490273414465\n",
            "iter: 6430  x: [-99.99971766  24.9999615 ]  f(x): 8.11984770150917e-08  grad at x: [ 5.64680979e-04 -7.70019516e-05]  gradient norm: 0.0005699069292966763\n",
            "iter: 6431  x: [-99.99971822  24.99996158]  f(x): 8.087400790337436e-08  grad at x: [ 5.63551617e-04 -7.68479477e-05]  gradient norm: 0.0005687671154466453\n",
            "iter: 6432  x: [-99.99971879  24.99996165]  f(x): 8.055083536417786e-08  grad at x: [ 5.62424513e-04 -7.66942518e-05]  gradient norm: 0.0005676295812030161\n",
            "iter: 6433  x: [-99.99971935  24.99996173]  f(x): 8.022895422358243e-08  grad at x: [ 5.61299664e-04 -7.65408633e-05]  gradient norm: 0.0005664943220318538\n",
            "iter: 6434  x: [-99.99971991  24.99996181]  f(x): 7.990835932627977e-08  grad at x: [ 5.60177065e-04 -7.63877816e-05]  gradient norm: 0.0005653613334011436\n",
            "iter: 6435  x: [-99.99972047  24.99996188]  f(x): 7.958904551908494e-08  grad at x: [ 5.59056711e-04 -7.62350060e-05]  gradient norm: 0.0005642306107225483\n",
            "iter: 6436  x: [-99.99972103  24.99996196]  f(x): 7.92710076904176e-08  grad at x: [ 5.57938597e-04 -7.60825360e-05]  gradient norm: 0.0005631021494912538\n",
            "iter: 6437  x: [-99.99972159  24.99996203]  f(x): 7.895424074708529e-08  grad at x: [ 5.56822720e-04 -7.59303709e-05]  gradient norm: 0.0005619759452043665\n",
            "iter: 6438  x: [-99.99972215  24.99996211]  f(x): 7.863873959762542e-08  grad at x: [ 5.55709075e-04 -7.57785102e-05]  gradient norm: 0.0005608519933017103\n",
            "iter: 6439  x: [-99.9997227   24.99996219]  f(x): 7.832449919209023e-08  grad at x: [ 5.54597657e-04 -7.56269532e-05]  gradient norm: 0.0005597302893075922\n",
            "iter: 6440  x: [-99.99972326  24.99996226]  f(x): 7.801151449029702e-08  grad at x: [ 5.53488461e-04 -7.54756992e-05]  gradient norm: 0.0005586108287181587\n",
            "iter: 6441  x: [-99.99972381  24.99996234]  f(x): 7.769978047782362e-08  grad at x: [ 5.52381484e-04 -7.53247478e-05]  gradient norm: 0.0005574936070586769\n",
            "iter: 6442  x: [-99.99972436  24.99996241]  f(x): 7.738929215777564e-08  grad at x: [ 5.51276721e-04 -7.51740983e-05]  gradient norm: 0.0005563786198544141\n",
            "iter: 6443  x: [-99.99972491  24.99996249]  f(x): 7.708004454291802e-08  grad at x: [ 5.50174168e-04 -7.50237501e-05]  gradient norm: 0.0005552658626024763\n",
            "iter: 6444  x: [-99.99972546  24.99996256]  f(x): 7.67720326871804e-08  grad at x: [ 5.49073820e-04 -7.48737027e-05]  gradient norm: 0.000554155330885413\n",
            "iter: 6445  x: [-99.99972601  24.99996264]  f(x): 7.646525164563795e-08  grad at x: [ 5.47975672e-04 -7.47239552e-05]  gradient norm: 0.0005530470202275317\n",
            "iter: 6446  x: [-99.99972656  24.99996271]  f(x): 7.61596964989978e-08  grad at x: [ 5.46879721e-04 -7.45745073e-05]  gradient norm: 0.0005519409261832204\n",
            "iter: 6447  x: [-99.99972711  24.99996279]  f(x): 7.585536235293764e-08  grad at x: [ 5.45785961e-04 -7.44253583e-05]  gradient norm: 0.0005508370443350289\n",
            "iter: 6448  x: [-99.99972765  24.99996286]  f(x): 7.555224432249779e-08  grad at x: [ 5.44694389e-04 -7.42765076e-05]  gradient norm: 0.0005497353702373453\n",
            "iter: 6449  x: [-99.9997282   24.99996294]  f(x): 7.525033755528504e-08  grad at x: [ 5.43605001e-04 -7.41279546e-05]  gradient norm: 0.0005486358995008805\n",
            "iter: 6450  x: [-99.99972874  24.99996301]  f(x): 7.494963720814021e-08  grad at x: [ 5.42517791e-04 -7.39796987e-05]  gradient norm: 0.0005475386277081836\n",
            "iter: 6451  x: [-99.99972928  24.99996308]  f(x): 7.465013845486116e-08  grad at x: [ 5.41432755e-04 -7.38317393e-05]  gradient norm: 0.0005464435504418042\n",
            "iter: 6452  x: [-99.99972983  24.99996316]  f(x): 7.435183650177372e-08  grad at x: [ 5.40349889e-04 -7.36840758e-05]  gradient norm: 0.000545350663341574\n",
            "iter: 6453  x: [-99.99973037  24.99996323]  f(x): 7.405472656401413e-08  grad at x: [ 5.39269190e-04 -7.35367076e-05]  gradient norm: 0.0005442599620182037\n",
            "iter: 6454  x: [-99.9997309   24.99996331]  f(x): 7.375880387399095e-08  grad at x: [ 5.38190651e-04 -7.33896342e-05]  gradient norm: 0.0005431714420843237\n",
            "iter: 6455  x: [-99.99973144  24.99996338]  f(x): 7.346406369555677e-08  grad at x: [ 5.3711427e-04 -7.3242855e-05]  gradient norm: 0.0005420850992069668\n",
            "iter: 6456  x: [-99.99973198  24.99996345]  f(x): 7.317050129438134e-08  grad at x: [ 5.36040041e-04 -7.30963692e-05]  gradient norm: 0.0005410009289987637\n",
            "iter: 6457  x: [-99.99973252  24.99996352]  f(x): 7.287811196768358e-08  grad at x: [ 5.34967961e-04 -7.29501765e-05]  gradient norm: 0.0005399189271277071\n",
            "iter: 6458  x: [-99.99973305  24.9999636 ]  f(x): 7.258689102937742e-08  grad at x: [ 5.33898025e-04 -7.28042762e-05]  gradient norm: 0.0005388390892627498\n",
            "iter: 6459  x: [-99.99973358  24.99996367]  f(x): 7.229683380950612e-08  grad at x: [ 5.32830229e-04 -7.26586676e-05]  gradient norm: 0.0005377614110718846\n",
            "iter: 6460  x: [-99.99973412  24.99996374]  f(x): 7.200793566227007e-08  grad at x: [ 5.31764569e-04 -7.25133503e-05]  gradient norm: 0.0005366858882522255\n",
            "iter: 6461  x: [-99.99973465  24.99996382]  f(x): 7.17201919505613e-08  grad at x: [ 5.30701040e-04 -7.23683236e-05]  gradient norm: 0.000535612516472725\n",
            "iter: 6462  x: [-99.99973518  24.99996389]  f(x): 7.143359806130257e-08  grad at x: [ 5.29639638e-04 -7.22235869e-05]  gradient norm: 0.0005345412914314574\n",
            "iter: 6463  x: [-99.99973571  24.99996396]  f(x): 7.114814940481072e-08  grad at x: [ 5.28580358e-04 -7.20791397e-05]  gradient norm: 0.0005334722088536974\n",
            "iter: 6464  x: [-99.99973624  24.99996403]  f(x): 7.086384139994058e-08  grad at x: [ 5.27523198e-04 -7.19349815e-05]  gradient norm: 0.000532405264436559\n",
            "iter: 6465  x: [-99.99973677  24.9999641 ]  f(x): 7.058066948933216e-08  grad at x: [ 5.26468151e-04 -7.17911115e-05]  gradient norm: 0.0005313404539062772\n",
            "iter: 6466  x: [-99.99973729  24.99996418]  f(x): 7.029862913156642e-08  grad at x: [ 5.25415215e-04 -7.16475293e-05]  gradient norm: 0.000530277772989087\n",
            "iter: 6467  x: [-99.99973782  24.99996425]  f(x): 7.001771580831716e-08  grad at x: [ 5.24364384e-04 -7.15042342e-05]  gradient norm: 0.0005292172174384245\n",
            "iter: 6468  x: [-99.99973834  24.99996432]  f(x): 6.973792501755753e-08  grad at x: [ 5.23315656e-04 -7.13612258e-05]  gradient norm: 0.0005281587830096458\n",
            "iter: 6469  x: [-99.99973887  24.99996439]  f(x): 6.945925226558183e-08  grad at x: [ 5.22269024e-04 -7.12185033e-05]  gradient norm: 0.0005271024654299459\n",
            "iter: 6470  x: [-99.99973939  24.99996446]  f(x): 6.918169309615807e-08  grad at x: [ 5.21224486e-04 -7.10760663e-05]  gradient norm: 0.0005260482605090832\n",
            "iter: 6471  x: [-99.99973991  24.99996453]  f(x): 6.890524304717211e-08  grad at x: [ 5.20182037e-04 -7.09339142e-05]  gradient norm: 0.0005249961639752128\n",
            "iter: 6472  x: [-99.99974043  24.9999646 ]  f(x): 6.862989769380492e-08  grad at x: [ 5.19141673e-04 -7.07920463e-05]  gradient norm: 0.0005239461716390528\n",
            "iter: 6473  x: [-99.99974095  24.99996467]  f(x): 6.83556526195735e-08  grad at x: [ 5.18103390e-04 -7.06504622e-05]  gradient norm: 0.0005228982792841204\n",
            "iter: 6474  x: [-99.99974147  24.99996475]  f(x): 6.808250343105387e-08  grad at x: [ 5.17067183e-04 -7.05091613e-05]  gradient norm: 0.0005218524827230541\n",
            "iter: 6475  x: [-99.99974198  24.99996482]  f(x): 6.78104457496777e-08  grad at x: [ 5.16033049e-04 -7.03681430e-05]  gradient norm: 0.0005208087777665722\n",
            "iter: 6476  x: [-99.9997425   24.99996489]  f(x): 6.75394752056222e-08  grad at x: [ 5.15000983e-04 -7.02274067e-05]  gradient norm: 0.0005197671602001119\n",
            "iter: 6477  x: [-99.99974301  24.99996496]  f(x): 6.726958746555407e-08  grad at x: [ 5.13970981e-04 -7.00869519e-05]  gradient norm: 0.0005187276258907138\n",
            "iter: 6478  x: [-99.99974353  24.99996503]  f(x): 6.7000778197418e-08  grad at x: [ 5.12943039e-04 -6.99467780e-05]  gradient norm: 0.000517690170651976\n",
            "iter: 6479  x: [-99.99974404  24.9999651 ]  f(x): 6.673304309111646e-08  grad at x: [ 5.11917153e-04 -6.98068844e-05]  gradient norm: 0.0005166547903237382\n",
            "iter: 6480  x: [-99.99974455  24.99996517]  f(x): 6.646637785212807e-08  grad at x: [ 5.10893318e-04 -6.96672707e-05]  gradient norm: 0.0005156214807477597\n",
            "iter: 6481  x: [-99.99974506  24.99996524]  f(x): 6.620077820796435e-08  grad at x: [ 5.09871532e-04 -6.95279361e-05]  gradient norm: 0.0005145902377930011\n",
            "iter: 6482  x: [-99.99974557  24.99996531]  f(x): 6.59362399010719e-08  grad at x: [ 5.08851789e-04 -6.93888803e-05]  gradient norm: 0.0005135610573284228\n",
            "iter: 6483  x: [-99.99974608  24.99996537]  f(x): 6.567275868903591e-08  grad at x: [ 5.07834085e-04 -6.92501025e-05]  gradient norm: 0.0005125339352239456\n",
            "iter: 6484  x: [-99.99974659  24.99996544]  f(x): 6.541033034404396e-08  grad at x: [ 5.06818417e-04 -6.91116023e-05]  gradient norm: 0.0005115088673485297\n",
            "iter: 6485  x: [-99.9997471   24.99996551]  f(x): 6.51489506605241e-08  grad at x: [ 5.05804780e-04 -6.89733791e-05]  gradient norm: 0.0005104858496002572\n",
            "iter: 6486  x: [-99.9997476   24.99996558]  f(x): 6.488861545502806e-08  grad at x: [ 5.04793171e-04 -6.88354323e-05]  gradient norm: 0.0005094648779063305\n",
            "iter: 6487  x: [-99.99974811  24.99996565]  f(x): 6.462932055106452e-08  grad at x: [ 5.03783584e-04 -6.86977615e-05]  gradient norm: 0.0005084459481638713\n",
            "iter: 6488  x: [-99.99974861  24.99996572]  f(x): 6.437106178725317e-08  grad at x: [ 5.02776017e-04 -6.85603659e-05]  gradient norm: 0.0005074290562719214\n",
            "iter: 6489  x: [-99.99974911  24.99996579]  f(x): 6.411383502392234e-08  grad at x: [ 5.01770465e-04 -6.84232452e-05]  gradient norm: 0.0005064141981576834\n",
            "iter: 6490  x: [-99.99974962  24.99996586]  f(x): 6.385763614275338e-08  grad at x: [ 5.00766924e-04 -6.82863987e-05]  gradient norm: 0.0005054013697755611\n",
            "iter: 6491  x: [-99.99975012  24.99996593]  f(x): 6.360246102584849e-08  grad at x: [ 4.99765390e-04 -6.81498259e-05]  gradient norm: 0.0005043905670245965\n",
            "iter: 6492  x: [-99.99975062  24.99996599]  f(x): 6.334830559078554e-08  grad at x: [ 4.98765860e-04 -6.80135263e-05]  gradient norm: 0.0005033817858873542\n",
            "iter: 6493  x: [-99.99975112  24.99996606]  f(x): 6.309516576279486e-08  grad at x: [ 4.97768328e-04 -6.78774992e-05]  gradient norm: 0.0005023750223201582\n",
            "iter: 6494  x: [-99.99975161  24.99996613]  f(x): 6.284303748112232e-08  grad at x: [ 4.96772791e-04 -6.77417442e-05]  gradient norm: 0.0005013702722783724\n",
            "iter: 6495  x: [-99.99975211  24.9999662 ]  f(x): 6.259191670651661e-08  grad at x: [ 4.95779246e-04 -6.76062607e-05]  gradient norm: 0.0005003675317464817\n",
            "iter: 6496  x: [-99.99975261  24.99996626]  f(x): 6.234179940657369e-08  grad at x: [ 4.94787687e-04 -6.74710482e-05]  gradient norm: 0.0004993667966798501\n",
            "iter: 6497  x: [-99.9997531   24.99996633]  f(x): 6.209268157729952e-08  grad at x: [ 4.93798112e-04 -6.73361061e-05]  gradient norm: 0.0004983680630911235\n",
            "iter: 6498  x: [-99.99975359  24.9999664 ]  f(x): 6.184455922167829e-08  grad at x: [ 4.92810515e-04 -6.72014339e-05]  gradient norm: 0.0004973713269647871\n",
            "iter: 6499  x: [-99.99975409  24.99996647]  f(x): 6.159742836368099e-08  grad at x: [ 4.91824894e-04 -6.70670310e-05]  gradient norm: 0.0004963765843134867\n",
            "iter: 6500  x: [-99.99975458  24.99996653]  f(x): 6.135128504118038e-08  grad at x: [ 4.90841245e-04 -6.69328970e-05]  gradient norm: 0.0004953838311498686\n",
            "iter: 6501  x: [-99.99975507  24.9999666 ]  f(x): 6.110612530591107e-08  grad at x: [ 4.89859562e-04 -6.67990312e-05]  gradient norm: 0.0004943930634865787\n",
            "iter: 6502  x: [-99.99975556  24.99996667]  f(x): 6.086194523061428e-08  grad at x: [ 4.88879843e-04 -6.66654331e-05]  gradient norm: 0.0004934042773653844\n",
            "iter: 6503  x: [-99.99975605  24.99996673]  f(x): 6.061874089482304e-08  grad at x: [ 4.87902083e-04 -6.65321022e-05]  gradient norm: 0.0004924174687998916\n",
            "iter: 6504  x: [-99.99975654  24.9999668 ]  f(x): 6.037650840541534e-08  grad at x: [ 4.86926279e-04 -6.63990380e-05]  gradient norm: 0.0004914326338590686\n",
            "iter: 6505  x: [-99.99975702  24.99996687]  f(x): 6.01352438761922e-08  grad at x: [ 4.85952427e-04 -6.62662400e-05]  gradient norm: 0.0004904497685846827\n",
            "iter: 6506  x: [-99.99975751  24.99996693]  f(x): 5.989494344168957e-08  grad at x: [ 4.84980522e-04 -6.61337075e-05]  gradient norm: 0.0004894688690476221\n",
            "iter: 6507  x: [-99.99975799  24.999967  ]  f(x): 5.965560324948646e-08  grad at x: [ 4.84010561e-04 -6.60014401e-05]  gradient norm: 0.0004884899313168552\n",
            "iter: 6508  x: [-99.99975848  24.99996707]  f(x): 5.941721946110719e-08  grad at x: [ 4.83042540e-04 -6.58694372e-05]  gradient norm: 0.0004875129514632701\n",
            "iter: 6509  x: [-99.99975896  24.99996713]  f(x): 5.9179788251510965e-08  grad at x: [ 4.82076455e-04 -6.57376983e-05]  gradient norm: 0.0004865379255577553\n",
            "iter: 6510  x: [-99.99975944  24.9999672 ]  f(x): 5.894330581565748e-08  grad at x: [ 4.81112302e-04 -6.56062229e-05]  gradient norm: 0.00048556484969840014\n",
            "iter: 6511  x: [-99.99975992  24.99996726]  f(x): 5.870776836886773e-08  grad at x: [ 4.80150077e-04 -6.54750105e-05]  gradient norm: 0.00048459372001241505\n",
            "iter: 6512  x: [-99.99976041  24.99996733]  f(x): 5.847317212605421e-08  grad at x: [ 4.79189777e-04 -6.53440604e-05]  gradient norm: 0.0004836245325706884\n",
            "iter: 6513  x: [-99.99976088  24.99996739]  f(x): 5.823951332900378e-08  grad at x: [ 4.78231397e-04 -6.52133723e-05]  gradient norm: 0.0004826572835004307\n",
            "iter: 6514  x: [-99.99976136  24.99996746]  f(x): 5.8006788232869835e-08  grad at x: [ 4.77274935e-04 -6.50829456e-05]  gradient norm: 0.0004816919689298124\n",
            "iter: 6515  x: [-99.99976184  24.99996752]  f(x): 5.777499310544028e-08  grad at x: [ 4.76320385e-04 -6.49527797e-05]  gradient norm: 0.000480728584985084\n",
            "iter: 6516  x: [-99.99976232  24.99996759]  f(x): 5.754412423501157e-08  grad at x: [ 4.75367744e-04 -6.48228741e-05]  gradient norm: 0.0004797671278235372\n",
            "iter: 6517  x: [-99.99976279  24.99996765]  f(x): 5.7314177915643946e-08  grad at x: [ 4.74417008e-04 -6.46932284e-05]  gradient norm: 0.0004788075935723825\n",
            "iter: 6518  x: [-99.99976327  24.99996772]  f(x): 5.7085150461341404e-08  grad at x: [ 4.73468174e-04 -6.45638419e-05]  gradient norm: 0.0004778499783879514\n",
            "iter: 6519  x: [-99.99976374  24.99996778]  f(x): 5.6857038199001063e-08  grad at x: [ 4.72521238e-04 -6.44347142e-05]  gradient norm: 0.00047689427842657564\n",
            "iter: 6520  x: [-99.99976421  24.99996785]  f(x): 5.662983747507781e-08  grad at x: [ 4.71576196e-04 -6.43058448e-05]  gradient norm: 0.0004759404898727479\n",
            "iter: 6521  x: [-99.99976468  24.99996791]  f(x): 5.6403544642332335e-08  grad at x: [ 4.70633043e-04 -6.41772331e-05]  gradient norm: 0.0004749886088837598\n",
            "iter: 6522  x: [-99.99976515  24.99996798]  f(x): 5.617815607942837e-08  grad at x: [ 4.69691777e-04 -6.40488787e-05]  gradient norm: 0.00047403863167226514\n",
            "iter: 6523  x: [-99.99976562  24.99996804]  f(x): 5.5953668164346454e-08  grad at x: [ 4.68752394e-04 -6.39207809e-05]  gradient norm: 0.0004730905543945956\n",
            "iter: 6524  x: [-99.99976609  24.9999681 ]  f(x): 5.573007730817468e-08  grad at x: [ 4.67814889e-04 -6.37929393e-05]  gradient norm: 0.00047214437329348605\n",
            "iter: 6525  x: [-99.99976656  24.99996817]  f(x): 5.5507379920789725e-08  grad at x: [ 4.66879259e-04 -6.36653535e-05]  gradient norm: 0.00047120008455342927\n",
            "iter: 6526  x: [-99.99976703  24.99996823]  f(x): 5.528557243150013e-08  grad at x: [ 4.65945500e-04 -6.35380228e-05]  gradient norm: 0.000470257684388039\n",
            "iter: 6527  x: [-99.99976749  24.99996829]  f(x): 5.506465128233355e-08  grad at x: [ 4.65013609e-04 -6.34109467e-05]  gradient norm: 0.00046931716901188925\n",
            "iter: 6528  x: [-99.99976796  24.99996836]  f(x): 5.4844612933919265e-08  grad at x: [ 4.64083582e-04 -6.32841248e-05]  gradient norm: 0.0004683785346657947\n",
            "iter: 6529  x: [-99.99976842  24.99996842]  f(x): 5.462545385970684e-08  grad at x: [ 4.63155415e-04 -6.31575566e-05]  gradient norm: 0.00046744177759249053\n",
            "iter: 6530  x: [-99.99976889  24.99996848]  f(x): 5.4407170545478385e-08  grad at x: [ 4.62229104e-04 -6.30312414e-05]  gradient norm: 0.0004665068940347115\n",
            "iter: 6531  x: [-99.99976935  24.99996855]  f(x): 5.418975948931338e-08  grad at x: [ 4.61304646e-04 -6.29051790e-05]  gradient norm: 0.0004655738802351927\n",
            "iter: 6532  x: [-99.99976981  24.99996861]  f(x): 5.397321720787288e-08  grad at x: [ 4.60382037e-04 -6.27793686e-05]  gradient norm: 0.0004646427324638701\n",
            "iter: 6533  x: [-99.99977027  24.99996867]  f(x): 5.375754023043963e-08  grad at x: [ 4.59461273e-04 -6.26538099e-05]  gradient norm: 0.00046371344699259966\n",
            "iter: 6534  x: [-99.99977073  24.99996874]  f(x): 5.354272509799034e-08  grad at x: [ 4.58542350e-04 -6.25285022e-05]  gradient norm: 0.0004627860200913175\n",
            "iter: 6535  x: [-99.99977119  24.9999688 ]  f(x): 5.332876837055542e-08  grad at x: [ 4.57625265e-04 -6.24034452e-05]  gradient norm: 0.00046186044806004086\n",
            "iter: 6536  x: [-99.99977164  24.99996886]  f(x): 5.311566661369195e-08  grad at x: [ 4.56710015e-04 -6.22786384e-05]  gradient norm: 0.00046093672717062564\n",
            "iter: 6537  x: [-99.9997721   24.99996892]  f(x): 5.290341641124796e-08  grad at x: [ 4.55796595e-04 -6.21540811e-05]  gradient norm: 0.00046001485372212914\n",
            "iter: 6538  x: [-99.99977256  24.99996899]  f(x): 5.26920143590215e-08  grad at x: [ 4.54885002e-04 -6.20297729e-05]  gradient norm: 0.00045909482401360833\n",
            "iter: 6539  x: [-99.99977301  24.99996905]  f(x): 5.248145707161771e-08  grad at x: [ 4.53975232e-04 -6.19057134e-05]  gradient norm: 0.00045817663437420163\n",
            "iter: 6540  x: [-99.99977347  24.99996911]  f(x): 5.2271741168810384e-08  grad at x: [ 4.53067281e-04 -6.17819019e-05]  gradient norm: 0.00045726028110392613\n",
            "iter: 6541  x: [-99.99977392  24.99996917]  f(x): 5.206286328864509e-08  grad at x: [ 4.52161147e-04 -6.16583381e-05]  gradient norm: 0.0004563457605309601\n",
            "iter: 6542  x: [-99.99977437  24.99996923]  f(x): 5.185482008734096e-08  grad at x: [ 4.51256824e-04 -6.15350215e-05]  gradient norm: 0.00045543306901164285\n",
            "iter: 6543  x: [-99.99977482  24.99996929]  f(x): 5.1647608226393114e-08  grad at x: [ 4.50354311e-04 -6.14119514e-05]  gradient norm: 0.00045452220287415276\n",
            "iter: 6544  x: [-99.99977527  24.99996936]  f(x): 5.144122438559533e-08  grad at x: [ 4.49453602e-04 -6.12891275e-05]  gradient norm: 0.00045361315847578903\n",
            "iter: 6545  x: [-99.99977572  24.99996942]  f(x): 5.123566524997453e-08  grad at x: [ 4.48554695e-04 -6.11665493e-05]  gradient norm: 0.00045270593214569003\n",
            "iter: 6546  x: [-99.99977617  24.99996948]  f(x): 5.103092752890622e-08  grad at x: [ 4.47657585e-04 -6.10442162e-05]  gradient norm: 0.00045180052026931627\n",
            "iter: 6547  x: [-99.99977662  24.99996954]  f(x): 5.0827007943256083e-08  grad at x: [ 4.46762270e-04 -6.09221277e-05]  gradient norm: 0.0004508969192321282\n",
            "iter: 6548  x: [-99.99977707  24.9999696 ]  f(x): 5.0623903219011105e-08  grad at x: [ 4.45868746e-04 -6.08002835e-05]  gradient norm: 0.0004499951253914251\n",
            "iter: 6549  x: [-99.99977751  24.99996966]  f(x): 5.0421610099957234e-08  grad at x: [ 4.44977008e-04 -6.06786829e-05]  gradient norm: 0.0004490951351326676\n",
            "iter: 6550  x: [-99.99977796  24.99996972]  f(x): 5.022012534758327e-08  grad at x: [ 4.44087054e-04 -6.05573255e-05]  gradient norm: 0.0004481969448694771\n",
            "iter: 6551  x: [-99.9997784   24.99996978]  f(x): 5.001944572860341e-08  grad at x: [ 4.43198880e-04 -6.04362109e-05]  gradient norm: 0.00044730055098827413\n",
            "iter: 6552  x: [-99.99977884  24.99996984]  f(x): 4.981956802105722e-08  grad at x: [ 4.42312482e-04 -6.03153385e-05]  gradient norm: 0.00044640594987547925\n",
            "iter: 6553  x: [-99.99977929  24.9999699 ]  f(x): 4.9620489026823354e-08  grad at x: [ 4.41427857e-04 -6.01947078e-05]  gradient norm: 0.0004455131379738351\n",
            "iter: 6554  x: [-99.99977973  24.99996996]  f(x): 4.9422205552466705e-08  grad at x: [ 4.40545002e-04 -6.00743184e-05]  gradient norm: 0.0004446221116969632\n",
            "iter: 6555  x: [-99.99978017  24.99997002]  f(x): 4.922471441615839e-08  grad at x: [ 4.39663912e-04 -5.99541697e-05]  gradient norm: 0.0004437328674604052\n",
            "iter: 6556  x: [-99.99978061  24.99997008]  f(x): 4.90280124592602e-08  grad at x: [ 4.38784584e-04 -5.98342614e-05]  gradient norm: 0.00044284540173410495\n",
            "iter: 6557  x: [-99.99978105  24.99997014]  f(x): 4.883209652234083e-08  grad at x: [ 4.37907015e-04 -5.97145929e-05]  gradient norm: 0.00044195971093456395\n",
            "iter: 6558  x: [-99.99978148  24.9999702 ]  f(x): 4.863696346262405e-08  grad at x: [ 4.37031201e-04 -5.95951637e-05]  gradient norm: 0.0004410757915035648\n",
            "iter: 6559  x: [-99.99978192  24.99997026]  f(x): 4.8442610155169005e-08  grad at x: [ 4.36157138e-04 -5.94759734e-05]  gradient norm: 0.00044019363991393154\n",
            "iter: 6560  x: [-99.99978236  24.99997032]  f(x): 4.82490334855302e-08  grad at x: [ 4.35284824e-04 -5.93570214e-05]  gradient norm: 0.00043931325263656774\n",
            "iter: 6561  x: [-99.99978279  24.99997038]  f(x): 4.8056230350361994e-08  grad at x: [ 4.34414254e-04 -5.92383074e-05]  gradient norm: 0.00043843462614333734\n",
            "iter: 6562  x: [-99.99978323  24.99997044]  f(x): 4.786419765101426e-08  grad at x: [ 4.33545426e-04 -5.91198308e-05]  gradient norm: 0.00043755775687794295\n",
            "iter: 6563  x: [-99.99978366  24.9999705 ]  f(x): 4.7672932318157914e-08  grad at x: [ 4.32678335e-04 -5.90015911e-05]  gradient norm: 0.0004366826413685706\n",
            "iter: 6564  x: [-99.99978409  24.99997056]  f(x): 4.7482431281096264e-08  grad at x: [ 4.31812978e-04 -5.88835879e-05]  gradient norm: 0.00043580927608804414\n",
            "iter: 6565  x: [-99.99978453  24.99997062]  f(x): 4.7292691485601384e-08  grad at x: [ 4.30949352e-04 -5.87658208e-05]  gradient norm: 0.00043493765753542833\n",
            "iter: 6566  x: [-99.99978496  24.99997068]  f(x): 4.710370988875609e-08  grad at x: [ 4.30087454e-04 -5.86482891e-05]  gradient norm: 0.00043406778221266823\n",
            "iter: 6567  x: [-99.99978539  24.99997073]  f(x): 4.691548346397713e-08  grad at x: [ 4.29227279e-04 -5.85309925e-05]  gradient norm: 0.00043319964664794976\n",
            "iter: 6568  x: [-99.99978582  24.99997079]  f(x): 4.672800918937548e-08  grad at x: [ 4.28368824e-04 -5.84139305e-05]  gradient norm: 0.00043233324734225786\n",
            "iter: 6569  x: [-99.99978624  24.99997085]  f(x): 4.65412840658038e-08  grad at x: [ 4.27512086e-04 -5.82971027e-05]  gradient norm: 0.00043146858085289964\n",
            "iter: 6570  x: [-99.99978667  24.99997091]  f(x): 4.6355305092451934e-08  grad at x: [ 4.26657062e-04 -5.81805085e-05]  gradient norm: 0.00043060564368085994\n",
            "iter: 6571  x: [-99.9997871   24.99997097]  f(x): 4.6170069291348835e-08  grad at x: [ 4.25803748e-04 -5.80641475e-05]  gradient norm: 0.00042974443238440603\n",
            "iter: 6572  x: [-99.99978752  24.99997103]  f(x): 4.5985573694513725e-08  grad at x: [ 4.24952141e-04 -5.79480192e-05]  gradient norm: 0.0004288849435198849\n",
            "iter: 6573  x: [-99.99978795  24.99997108]  f(x): 4.580181534495785e-08  grad at x: [ 4.24102236e-04 -5.78321231e-05]  gradient norm: 0.0004280271736465238\n",
            "iter: 6574  x: [-99.99978837  24.99997114]  f(x): 4.5618791289403645e-08  grad at x: [ 4.23254032e-04 -5.77164589e-05]  gradient norm: 0.00042717111929250855\n",
            "iter: 6575  x: [-99.9997888  24.9999712]  f(x): 4.543649859755756e-08  grad at x: [ 4.22407524e-04 -5.76010260e-05]  gradient norm: 0.0004263167770452275\n",
            "iter: 6576  x: [-99.99978922  24.99997126]  f(x): 4.525493434915573e-08  grad at x: [ 4.21562709e-04 -5.74858239e-05]  gradient norm: 0.00042546414349110893\n",
            "iter: 6577  x: [-99.99978964  24.99997131]  f(x): 4.50740956339368e-08  grad at x: [ 4.20719583e-04 -5.73708523e-05]  gradient norm: 0.0004246132152156209\n",
            "iter: 6578  x: [-99.99979006  24.99997137]  f(x): 4.4893979546258246e-08  grad at x: [ 4.19878144e-04 -5.72561106e-05]  gradient norm: 0.00042376398877799065\n",
            "iter: 6579  x: [-99.99979048  24.99997143]  f(x): 4.471458320237857e-08  grad at x: [ 4.19038388e-04 -5.71415983e-05]  gradient norm: 0.00042291646079280746\n",
            "iter: 6580  x: [-99.9997909   24.99997149]  f(x): 4.453590372862758e-08  grad at x: [ 4.18200311e-04 -5.70273151e-05]  gradient norm: 0.0004220706278746607\n",
            "iter: 6581  x: [-99.99979132  24.99997154]  f(x): 4.435793825564881e-08  grad at x: [ 4.17363911e-04 -5.69132605e-05]  gradient norm: 0.00042122648661093863\n",
            "iter: 6582  x: [-99.99979174  24.9999716 ]  f(x): 4.418068393578224e-08  grad at x: [ 4.16529183e-04 -5.67994340e-05]  gradient norm: 0.00042038403364439157\n",
            "iter: 6583  x: [-99.99979215  24.99997166]  f(x): 4.4004137919893645e-08  grad at x: [ 4.15696124e-04 -5.66858351e-05]  gradient norm: 0.0004195432655633678\n",
            "iter: 6584  x: [-99.99979257  24.99997171]  f(x): 4.382829738613447e-08  grad at x: [ 4.14864732e-04 -5.65724635e-05]  gradient norm: 0.00041870417903877897\n",
            "iter: 6585  x: [-99.99979298  24.99997177]  f(x): 4.365315951092294e-08  grad at x: [ 4.14035003e-04 -5.64593185e-05]  gradient norm: 0.0004178667706861743\n",
            "iter: 6586  x: [-99.9997934   24.99997183]  f(x): 4.347872148664792e-08  grad at x: [ 4.13206933e-04 -5.63463999e-05]  gradient norm: 0.0004170310371502242\n",
            "iter: 6587  x: [-99.99979381  24.99997188]  f(x): 4.330498051512018e-08  grad at x: [ 4.12380519e-04 -5.62337071e-05]  gradient norm: 0.00041619697507367916\n",
            "iter: 6588  x: [-99.99979422  24.99997194]  f(x): 4.31319338143938e-08  grad at x: [ 4.11555758e-04 -5.61212397e-05]  gradient norm: 0.00041536458113033086\n",
            "iter: 6589  x: [-99.99979463  24.999972  ]  f(x): 4.295957860580598e-08  grad at x: [ 4.10732646e-04 -5.60089972e-05]  gradient norm: 0.00041453385196292943\n",
            "iter: 6590  x: [-99.99979504  24.99997205]  f(x): 4.278791212685568e-08  grad at x: [ 4.09911181e-04 -5.58969792e-05]  gradient norm: 0.00041370478424526674\n",
            "iter: 6591  x: [-99.99979545  24.99997211]  f(x): 4.2616931630120434e-08  grad at x: [ 4.09091359e-04 -5.57851852e-05]  gradient norm: 0.0004128773746773753\n",
            "iter: 6592  x: [-99.99979586  24.99997216]  f(x): 4.244663437216419e-08  grad at x: [ 4.08273176e-04 -5.56736149e-05]  gradient norm: 0.00041205161993208665\n",
            "iter: 6593  x: [-99.99979627  24.99997222]  f(x): 4.227701761935631e-08  grad at x: [ 4.07456629e-04 -5.55622676e-05]  gradient norm: 0.00041122751668319234\n",
            "iter: 6594  x: [-99.99979668  24.99997227]  f(x): 4.210807865880853e-08  grad at x: [ 4.06641716e-04 -5.54511431e-05]  gradient norm: 0.00041040506165888614\n",
            "iter: 6595  x: [-99.99979709  24.99997233]  f(x): 4.1939814775953685e-08  grad at x: [ 4.05828433e-04 -5.53402408e-05]  gradient norm: 0.0004095842515329596\n",
            "iter: 6596  x: [-99.99979749  24.99997239]  f(x): 4.1772223277064014e-08  grad at x: [ 4.05016776e-04 -5.52295603e-05]  gradient norm: 0.00040876508303456653\n",
            "iter: 6597  x: [-99.9997979   24.99997244]  f(x): 4.160530147227002e-08  grad at x: [ 4.04206742e-04 -5.51191012e-05]  gradient norm: 0.00040794755286565953\n",
            "iter: 6598  x: [-99.9997983  24.9999725]  f(x): 4.1439046686663484e-08  grad at x: [ 4.03398329e-04 -5.50088630e-05]  gradient norm: 0.00040713165775539234\n",
            "iter: 6599  x: [-99.9997987   24.99997255]  f(x): 4.1273456255079096e-08  grad at x: [ 4.02591532e-04 -5.48988453e-05]  gradient norm: 0.0004063173944348388\n",
            "iter: 6600  x: [-99.99979911  24.99997261]  f(x): 4.1108527521285224e-08  grad at x: [ 4.01786349e-04 -5.47890476e-05]  gradient norm: 0.00040550475963315266\n",
            "iter: 6601  x: [-99.99979951  24.99997266]  f(x): 4.0944257844242257e-08  grad at x: [ 4.00982776e-04 -5.46794695e-05]  gradient norm: 0.0004046937501086087\n",
            "iter: 6602  x: [-99.99979991  24.99997271]  f(x): 4.0780644592330145e-08  grad at x: [ 4.00180811e-04 -5.45701106e-05]  gradient norm: 0.0004038843626204419\n",
            "iter: 6603  x: [-99.99980031  24.99997277]  f(x): 4.061768513706418e-08  grad at x: [ 3.99380449e-04 -5.44609704e-05]  gradient norm: 0.00040307659389780587\n",
            "iter: 6604  x: [-99.99980071  24.99997282]  f(x): 4.0455376865423205e-08  grad at x: [ 3.98581688e-04 -5.43520484e-05]  gradient norm: 0.0004022704407008957\n",
            "iter: 6605  x: [-99.99980111  24.99997288]  f(x): 4.0293717178604476e-08  grad at x: [ 3.97784525e-04 -5.42433443e-05]  gradient norm: 0.0004014658998151872\n",
            "iter: 6606  x: [-99.99980151  24.99997293]  f(x): 4.013270348745915e-08  grad at x: [ 3.96988956e-04 -5.41348576e-05]  gradient norm: 0.00040066296802903636\n",
            "iter: 6607  x: [-99.9998019   24.99997299]  f(x): 3.997233320587069e-08  grad at x: [ 3.96194978e-04 -5.40265879e-05]  gradient norm: 0.0003998616421007181\n",
            "iter: 6608  x: [-99.9998023   24.99997304]  f(x): 3.981260376260186e-08  grad at x: [ 3.95402588e-04 -5.39185347e-05]  gradient norm: 0.0003990619188176284\n",
            "iter: 6609  x: [-99.99980269  24.99997309]  f(x): 3.9653512595602826e-08  grad at x: [ 3.94611783e-04 -5.38106977e-05]  gradient norm: 0.0003982637949681232\n",
            "iter: 6610  x: [-99.99980309  24.99997315]  f(x): 3.9495057157007747e-08  grad at x: [ 3.93822559e-04 -5.37030763e-05]  gradient norm: 0.00039746726736679963\n",
            "iter: 6611  x: [-99.99980348  24.9999732 ]  f(x): 3.933723490823324e-08  grad at x: [ 3.93034914e-04 -5.35956701e-05]  gradient norm: 0.0003966723328301748\n",
            "iter: 6612  x: [-99.99980388  24.99997326]  f(x): 3.918004331937905e-08  grad at x: [ 3.92248844e-04 -5.34884788e-05]  gradient norm: 0.0003958789881738057\n",
            "iter: 6613  x: [-99.99980427  24.99997331]  f(x): 3.902347986402088e-08  grad at x: [ 3.91464347e-04 -5.33815018e-05]  gradient norm: 0.0003950872301860483\n",
            "iter: 6614  x: [-99.99980466  24.99997336]  f(x): 3.886754204106804e-08  grad at x: [ 3.90681418e-04 -5.32747388e-05]  gradient norm: 0.000394297055738782\n",
            "iter: 6615  x: [-99.99980505  24.99997342]  f(x): 3.8712227341706126e-08  grad at x: [ 3.89900055e-04 -5.31681893e-05]  gradient norm: 0.00039350846162036274\n",
            "iter: 6616  x: [-99.99980544  24.99997347]  f(x): 3.8557533282526536e-08  grad at x: [ 3.89120255e-04 -5.30618530e-05]  gradient norm: 0.0003927214447036298\n",
            "iter: 6617  x: [-99.99980583  24.99997352]  f(x): 3.840345737774189e-08  grad at x: [ 3.88342015e-04 -5.29557293e-05]  gradient norm: 0.0003919360018051003\n",
            "iter: 6618  x: [-99.99980622  24.99997358]  f(x): 3.824999716130226e-08  grad at x: [ 3.87565330e-04 -5.28498178e-05]  gradient norm: 0.00039115212979761344\n",
            "iter: 6619  x: [-99.9998066   24.99997363]  f(x): 3.809715017045754e-08  grad at x: [ 3.86790200e-04 -5.27441182e-05]  gradient norm: 0.00039036982552680756\n",
            "iter: 6620  x: [-99.99980699  24.99997368]  f(x): 3.794491395638324e-08  grad at x: [ 3.86016619e-04 -5.26386299e-05]  gradient norm: 0.00038958908586552186\n",
            "iter: 6621  x: [-99.99980738  24.99997373]  f(x): 3.779328607881476e-08  grad at x: [ 3.85244586e-04 -5.25333527e-05]  gradient norm: 0.00038880990768659564\n",
            "iter: 6622  x: [-99.99980776  24.99997379]  f(x): 3.7642264106209215e-08  grad at x: [ 3.84474097e-04 -5.24282860e-05]  gradient norm: 0.0003880322878638282\n",
            "iter: 6623  x: [-99.99980815  24.99997384]  f(x): 3.749184562080022e-08  grad at x: [ 3.83705149e-04 -5.23234294e-05]  gradient norm: 0.00038725622329822\n",
            "iter: 6624  x: [-99.99980853  24.99997389]  f(x): 3.7342028208193605e-08  grad at x: [ 3.82937739e-04 -5.22187825e-05]  gradient norm: 0.0003864817108645303\n",
            "iter: 6625  x: [-99.99980891  24.99997394]  f(x): 3.719280946208456e-08  grad at x: [ 3.82171863e-04 -5.21143450e-05]  gradient norm: 0.0003857087474355984\n",
            "iter: 6626  x: [-99.9998093   24.99997399]  f(x): 3.70441869958164e-08  grad at x: [ 3.81407519e-04 -5.20101163e-05]  gradient norm: 0.00038493732994250584\n",
            "iter: 6627  x: [-99.99980968  24.99997405]  f(x): 3.6896158425463206e-08  grad at x: [ 3.80644704e-04 -5.19060960e-05]  gradient norm: 0.000384167455287213\n",
            "iter: 6628  x: [-99.99981006  24.9999741 ]  f(x): 3.6748721375444555e-08  grad at x: [ 3.79883415e-04 -5.18022838e-05]  gradient norm: 0.0003833991203716803\n",
            "iter: 6629  x: [-99.99981044  24.99997415]  f(x): 3.660187348388936e-08  grad at x: [ 3.79123648e-04 -5.16986793e-05]  gradient norm: 0.0003826323221260293\n",
            "iter: 6630  x: [-99.99981082  24.9999742 ]  f(x): 3.645561239736451e-08  grad at x: [ 3.78365401e-04 -5.15952819e-05]  gradient norm: 0.00038186705748134134\n",
            "iter: 6631  x: [-99.9998112   24.99997425]  f(x): 3.6309935770666615e-08  grad at x: [ 3.77608670e-04 -5.14920914e-05]  gradient norm: 0.00038110332336869807\n",
            "iter: 6632  x: [-99.99981157  24.99997431]  f(x): 3.616484126679843e-08  grad at x: [ 3.76853453e-04 -5.13891072e-05]  gradient norm: 0.00038034111671918106\n",
            "iter: 6633  x: [-99.99981195  24.99997436]  f(x): 3.602032656210791e-08  grad at x: [ 3.76099746e-04 -5.12863290e-05]  gradient norm: 0.00037958043449107286\n",
            "iter: 6634  x: [-99.99981233  24.99997441]  f(x): 3.587638933609069e-08  grad at x: [ 3.75347546e-04 -5.11837563e-05]  gradient norm: 0.000378821273616415\n",
            "iter: 6635  x: [-99.9998127   24.99997446]  f(x): 3.57330272866607e-08  grad at x: [ 3.74596851e-04 -5.10813888e-05]  gradient norm: 0.0003780636310816511\n",
            "iter: 6636  x: [-99.99981308  24.99997451]  f(x): 3.559023810949775e-08  grad at x: [ 3.73847657e-04 -5.09792260e-05]  gradient norm: 0.0003773075038188228\n",
            "iter: 6637  x: [-99.99981345  24.99997456]  f(x): 3.544801951897404e-08  grad at x: [ 3.73099962e-04 -5.08772676e-05]  gradient norm: 0.00037655288881629384\n",
            "iter: 6638  x: [-99.99981382  24.99997461]  f(x): 3.5306369231789726e-08  grad at x: [ 3.72353762e-04 -5.07755130e-05]  gradient norm: 0.00037579978303234677\n",
            "iter: 6639  x: [-99.9998142   24.99997466]  f(x): 3.516528497846862e-08  grad at x: [ 3.71609055e-04 -5.06739620e-05]  gradient norm: 0.0003750481834563054\n",
            "iter: 6640  x: [-99.99981457  24.99997471]  f(x): 3.50247645023787e-08  grad at x: [ 3.70865837e-04 -5.05726141e-05]  gradient norm: 0.00037429808710373445\n",
            "iter: 6641  x: [-99.99981494  24.99997476]  f(x): 3.48848055444189e-08  grad at x: [ 3.70124105e-04 -5.04714688e-05]  gradient norm: 0.0003735494909348366\n",
            "iter: 6642  x: [-99.99981531  24.99997481]  f(x): 3.474540586373931e-08  grad at x: [ 3.69383857e-04 -5.03705259e-05]  gradient norm: 0.00037280239196517666\n",
            "iter: 6643  x: [-99.99981568  24.99997487]  f(x): 3.460656322243466e-08  grad at x: [ 3.68645089e-04 -5.02697849e-05]  gradient norm: 0.00037205678718407845\n",
            "iter: 6644  x: [-99.99981605  24.99997492]  f(x): 3.446827539549267e-08  grad at x: [ 3.67907799e-04 -5.01692453e-05]  gradient norm: 0.00037131267360806673\n",
            "iter: 6645  x: [-99.99981641  24.99997497]  f(x): 3.433054016585902e-08  grad at x: [ 3.67171983e-04 -5.00689068e-05]  gradient norm: 0.00037057004825462634\n",
            "iter: 6646  x: [-99.99981678  24.99997502]  f(x): 3.41933553292657e-08  grad at x: [ 3.66437639e-04 -4.99687690e-05]  gradient norm: 0.0003698289081684432\n",
            "iter: 6647  x: [-99.99981715  24.99997507]  f(x): 3.405671868394175e-08  grad at x: [ 3.65704764e-04 -4.98688314e-05]  gradient norm: 0.00036908925036604226\n",
            "iter: 6648  x: [-99.99981751  24.99997512]  f(x): 3.392062803600669e-08  grad at x: [ 3.64973354e-04 -4.97690938e-05]  gradient norm: 0.00036835107186490823\n",
            "iter: 6649  x: [-99.99981788  24.99997517]  f(x): 3.378508120426998e-08  grad at x: [ 3.64243408e-04 -4.96695556e-05]  gradient norm: 0.0003676143697097271\n",
            "iter: 6650  x: [-99.99981824  24.99997521]  f(x): 3.365007602068726e-08  grad at x: [ 3.63514921e-04 -4.95702165e-05]  gradient norm: 0.00036687914097526593\n",
            "iter: 6651  x: [-99.99981861  24.99997526]  f(x): 3.351561031926935e-08  grad at x: [ 3.62787891e-04 -4.94710760e-05]  gradient norm: 0.0003661453827062106\n",
            "iter: 6652  x: [-99.99981897  24.99997531]  f(x): 3.338168194196262e-08  grad at x: [ 3.62062315e-04 -4.93721339e-05]  gradient norm: 0.0003654130919491671\n",
            "iter: 6653  x: [-99.99981933  24.99997536]  f(x): 3.324828874323349e-08  grad at x: [ 3.61338191e-04 -4.92733896e-05]  gradient norm: 0.0003646822657779426\n",
            "iter: 6654  x: [-99.99981969  24.99997541]  f(x): 3.3115428579922616e-08  grad at x: [ 3.60615514e-04 -4.91748428e-05]  gradient norm: 0.00036395290123818284\n",
            "iter: 6655  x: [-99.99982005  24.99997546]  f(x): 3.298309932661797e-08  grad at x: [ 3.59894283e-04 -4.90764932e-05]  gradient norm: 0.0003632249954318561\n",
            "iter: 6656  x: [-99.99982041  24.99997551]  f(x): 3.285129886039239e-08  grad at x: [ 3.59174495e-04 -4.89783402e-05]  gradient norm: 0.0003624985454337294\n",
            "iter: 6657  x: [-99.99982077  24.99997556]  f(x): 3.27200250708461e-08  grad at x: [ 3.58456146e-04 -4.88803835e-05]  gradient norm: 0.00036177354834673084\n",
            "iter: 6658  x: [-99.99982113  24.99997561]  f(x): 3.2589275849693617e-08  grad at x: [ 3.57739233e-04 -4.87826227e-05]  gradient norm: 0.0003610500012446676\n",
            "iter: 6659  x: [-99.99982149  24.99997566]  f(x): 3.245904910128843e-08  grad at x: [ 3.57023755e-04 -4.86850575e-05]  gradient norm: 0.00036032790123046777\n",
            "iter: 6660  x: [-99.99982185  24.99997571]  f(x): 3.232934274254935e-08  grad at x: [ 3.56309707e-04 -4.85876874e-05]  gradient norm: 0.00035960724543618056\n",
            "iter: 6661  x: [-99.9998222   24.99997575]  f(x): 3.2200154687210397e-08  grad at x: [ 3.55597088e-04 -4.84905120e-05]  gradient norm: 0.0003588880309356131\n",
            "iter: 6662  x: [-99.99982256  24.9999758 ]  f(x): 3.20714828665933e-08  grad at x: [ 3.54885894e-04 -4.83935310e-05]  gradient norm: 0.00035817025485985457\n",
            "iter: 6663  x: [-99.99982291  24.99997585]  f(x): 3.1943325219417596e-08  grad at x: [ 3.54176122e-04 -4.82967439e-05]  gradient norm: 0.00035745391434095444\n",
            "iter: 6664  x: [-99.99982327  24.9999759 ]  f(x): 3.181567969143576e-08  grad at x: [ 3.53467770e-04 -4.82001504e-05]  gradient norm: 0.0003567390065100017\n",
            "iter: 6665  x: [-99.99982362  24.99997595]  f(x): 3.168854423575621e-08  grad at x: [ 3.52760834e-04 -4.81037501e-05]  gradient norm: 0.00035602552849904574\n",
            "iter: 6666  x: [-99.99982397  24.999976  ]  f(x): 3.1561916812479934e-08  grad at x: [ 3.52055313e-04 -4.80075426e-05]  gradient norm: 0.00035531347743917587\n",
            "iter: 6667  x: [-99.99982432  24.99997604]  f(x): 3.143579539401513e-08  grad at x: [ 3.51351202e-04 -4.79115275e-05]  gradient norm: 0.0003546028504906024\n",
            "iter: 6668  x: [-99.99982468  24.99997609]  f(x): 3.131017795486923e-08  grad at x: [ 3.50648500e-04 -4.78157045e-05]  gradient norm: 0.00035389364478537465\n",
            "iter: 6669  x: [-99.99982503  24.99997614]  f(x): 3.118506248162463e-08  grad at x: [ 3.49947203e-04 -4.77200731e-05]  gradient norm: 0.00035318585748370297\n",
            "iter: 6670  x: [-99.99982538  24.99997619]  f(x): 3.1060446972868603e-08  grad at x: [ 3.49247308e-04 -4.76246329e-05]  gradient norm: 0.00035247948577395876\n",
            "iter: 6671  x: [-99.99982573  24.99997624]  f(x): 3.093632942921718e-08  grad at x: [ 3.48548814e-04 -4.75293837e-05]  gradient norm: 0.00035177452681635254\n",
            "iter: 6672  x: [-99.99982607  24.99997628]  f(x): 3.081270785828809e-08  grad at x: [ 3.47851716e-04 -4.74343249e-05]  gradient norm: 0.00035107097777109454\n",
            "iter: 6673  x: [-99.99982642  24.99997633]  f(x): 3.068958027978227e-08  grad at x: [ 3.47156012e-04 -4.73394562e-05]  gradient norm: 0.00035036883582751635\n",
            "iter: 6674  x: [-99.99982677  24.99997638]  f(x): 3.056694471523015e-08  grad at x: [ 3.46461700e-04 -4.72447773e-05]  gradient norm: 0.00034966809814582824\n",
            "iter: 6675  x: [-99.99982712  24.99997642]  f(x): 3.0444799202931284e-08  grad at x: [ 3.45768777e-04 -4.71502878e-05]  gradient norm: 0.00034896876194256293\n",
            "iter: 6676  x: [-99.99982746  24.99997647]  f(x): 3.032314178345904e-08  grad at x: [ 3.45077239e-04 -4.70559872e-05]  gradient norm: 0.00034827082440801177\n",
            "iter: 6677  x: [-99.99982781  24.99997652]  f(x): 3.020197050880747e-08  grad at x: [ 3.44387085e-04 -4.69618752e-05]  gradient norm: 0.00034757428275870736\n",
            "iter: 6678  x: [-99.99982815  24.99997657]  f(x): 3.0081283433057126e-08  grad at x: [ 3.43698311e-04 -4.68679515e-05]  gradient norm: 0.0003468791341839813\n",
            "iter: 6679  x: [-99.99982849  24.99997661]  f(x): 2.996107862198561e-08  grad at x: [ 3.43010914e-04 -4.67742156e-05]  gradient norm: 0.00034618537590132606\n",
            "iter: 6680  x: [-99.99982884  24.99997666]  f(x): 2.98413541531651e-08  grad at x: [ 3.42324892e-04 -4.66806671e-05]  gradient norm: 0.0003454930051573554\n",
            "iter: 6681  x: [-99.99982918  24.99997671]  f(x): 2.9722108100831217e-08  grad at x: [ 3.41640243e-04 -4.65873058e-05]  gradient norm: 0.00034480201914044076\n",
            "iter: 6682  x: [-99.99982952  24.99997675]  f(x): 2.9603338556006203e-08  grad at x: [ 3.40956962e-04 -4.64941312e-05]  gradient norm: 0.00034411241509719586\n",
            "iter: 6683  x: [-99.99982986  24.9999768 ]  f(x): 2.9485043616378384e-08  grad at x: [ 3.40275048e-04 -4.64011429e-05]  gradient norm: 0.00034342419027423435\n",
            "iter: 6684  x: [-99.9998302   24.99997685]  f(x): 2.936722138112818e-08  grad at x: [ 3.39594498e-04 -4.63083406e-05]  gradient norm: 0.0003427373418880889\n",
            "iter: 6685  x: [-99.99983054  24.99997689]  f(x): 2.9249869966230496e-08  grad at x: [ 3.38915309e-04 -4.62157240e-05]  gradient norm: 0.00034205186721449423\n",
            "iter: 6686  x: [-99.99983088  24.99997694]  f(x): 2.9132987484288417e-08  grad at x: [ 3.38237478e-04 -4.61232925e-05]  gradient norm: 0.00034136776347094296\n",
            "iter: 6687  x: [-99.99983122  24.99997698]  f(x): 2.901657206429266e-08  grad at x: [ 3.37561003e-04 -4.60310459e-05]  gradient norm: 0.0003406850279322099\n",
            "iter: 6688  x: [-99.99983156  24.99997703]  f(x): 2.890062184176688e-08  grad at x: [ 3.36885881e-04 -4.59389838e-05]  gradient norm: 0.00034000365787306984\n",
            "iter: 6689  x: [-99.99983189  24.99997708]  f(x): 2.8785134958911907e-08  grad at x: [ 3.36212110e-04 -4.58471059e-05]  gradient norm: 0.0003393236505692576\n",
            "iter: 6690  x: [-99.99983223  24.99997712]  f(x): 2.8670109559329564e-08  grad at x: [ 3.35539685e-04 -4.57554117e-05]  gradient norm: 0.00033864500326642684\n",
            "iter: 6691  x: [-99.99983257  24.99997717]  f(x): 2.8555543802980283e-08  grad at x: [ 3.34868606e-04 -4.56639008e-05]  gradient norm: 0.0003379677132684735\n",
            "iter: 6692  x: [-99.9998329   24.99997721]  f(x): 2.8441435851332516e-08  grad at x: [ 3.34198869e-04 -4.55725730e-05]  gradient norm: 0.00033729177785017244\n",
            "iter: 6693  x: [-99.99983323  24.99997726]  f(x): 2.832778387229488e-08  grad at x: [ 3.33530471e-04 -4.54814279e-05]  gradient norm: 0.0003366171942862983\n",
            "iter: 6694  x: [-99.99983357  24.9999773 ]  f(x): 2.821458604981954e-08  grad at x: [ 3.3286341e-04 -4.5390465e-05]  gradient norm: 0.0003359439599089082\n",
            "iter: 6695  x: [-99.9998339   24.99997735]  f(x): 2.8101840564743393e-08  grad at x: [ 3.32197683e-04 -4.52996841e-05]  gradient norm: 0.000335272071993737\n",
            "iter: 6696  x: [-99.99983423  24.9999774 ]  f(x): 2.7989545608838013e-08  grad at x: [ 3.31533288e-04 -4.52090847e-05]  gradient norm: 0.00033460152784372046\n",
            "iter: 6697  x: [-99.99983456  24.99997744]  f(x): 2.78776993852267e-08  grad at x: [ 3.30870221e-04 -4.51186666e-05]  gradient norm: 0.00033393232479187574\n",
            "iter: 6698  x: [-99.9998349   24.99997749]  f(x): 2.7766300098291154e-08  grad at x: [ 3.30208481e-04 -4.50284292e-05]  gradient norm: 0.0003332644601411387\n",
            "iter: 6699  x: [-99.99983523  24.99997753]  f(x): 2.7655345963708684e-08  grad at x: [ 3.29548064e-04 -4.49383724e-05]  gradient norm: 0.0003325979312245263\n",
            "iter: 6700  x: [-99.99983556  24.99997758]  f(x): 2.754483520323235e-08  grad at x: [ 3.28888968e-04 -4.48484956e-05]  gradient norm: 0.00033193273537409566\n",
            "iter: 6701  x: [-99.99983588  24.99997762]  f(x): 2.743476604016934e-08  grad at x: [ 3.28231190e-04 -4.47587986e-05]  gradient norm: 0.0003312688698937426\n",
            "iter: 6702  x: [-99.99983621  24.99997767]  f(x): 2.7325136713375187e-08  grad at x: [ 3.27574728e-04 -4.46692810e-05]  gradient norm: 0.0003306063321436853\n",
            "iter: 6703  x: [-99.99983654  24.99997771]  f(x): 2.721594546800953e-08  grad at x: [ 3.26919578e-04 -4.45799425e-05]  gradient norm: 0.0003299451194851018\n",
            "iter: 6704  x: [-99.99983687  24.99997775]  f(x): 2.7107190550722426e-08  grad at x: [ 3.26265739e-04 -4.44907826e-05]  gradient norm: 0.00032928522925100925\n",
            "iter: 6705  x: [-99.99983719  24.9999778 ]  f(x): 2.699887021878002e-08  grad at x: [ 3.25613208e-04 -4.44018010e-05]  gradient norm: 0.00032862665880162566\n",
            "iter: 6706  x: [-99.99983752  24.99997784]  f(x): 2.6890982731081085e-08  grad at x: [ 3.24961981e-04 -4.43129974e-05]  gradient norm: 0.0003279694054699681\n",
            "iter: 6707  x: [-99.99983784  24.99997789]  f(x): 2.6783526361853383e-08  grad at x: [ 3.24312057e-04 -4.42243714e-05]  gradient norm: 0.0003273134666453758\n",
            "iter: 6708  x: [-99.99983817  24.99997793]  f(x): 2.6676499391344872e-08  grad at x: [ 3.23663433e-04 -4.41359227e-05]  gradient norm: 0.00032665883971718794\n",
            "iter: 6709  x: [-99.99983849  24.99997798]  f(x): 2.6569900101216232e-08  grad at x: [ 3.23016106e-04 -4.40476508e-05]  gradient norm: 0.0003260055220465827\n",
            "iter: 6710  x: [-99.99983881  24.99997802]  f(x): 2.6463726779306985e-08  grad at x: [ 3.22370074e-04 -4.39595555e-05]  gradient norm: 0.00032535351099569824\n",
            "iter: 6711  x: [-99.99983914  24.99997806]  f(x): 2.6357977728293076e-08  grad at x: [ 3.21725334e-04 -4.38716364e-05]  gradient norm: 0.0003247028039810748\n",
            "iter: 6712  x: [-99.99983946  24.99997811]  f(x): 2.6252651247952138e-08  grad at x: [ 3.21081883e-04 -4.37838932e-05]  gradient norm: 0.0003240533983648506\n",
            "iter: 6713  x: [-99.99983978  24.99997815]  f(x): 2.6147745653139236e-08  grad at x: [ 3.20439719e-04 -4.36963254e-05]  gradient norm: 0.0003234052915654859\n",
            "iter: 6714  x: [-99.9998401  24.9999782]  f(x): 2.6043259259889862e-08  grad at x: [ 3.19798840e-04 -4.36089327e-05]  gradient norm: 0.00032275848097232\n",
            "iter: 6715  x: [-99.99984042  24.99997824]  f(x): 2.593919039483046e-08  grad at x: [ 3.19159242e-04 -4.35217149e-05]  gradient norm: 0.0003221129640038132\n",
            "iter: 6716  x: [-99.99984074  24.99997828]  f(x): 2.5835537390279574e-08  grad at x: [ 3.18520924e-04 -4.34346714e-05]  gradient norm: 0.0003214687380774658\n",
            "iter: 6717  x: [-99.99984106  24.99997833]  f(x): 2.5732298584694688e-08  grad at x: [ 3.17883882e-04 -4.33478021e-05]  gradient norm: 0.00032082580061269816\n",
            "iter: 6718  x: [-99.99984138  24.99997837]  f(x): 2.5629472317529374e-08  grad at x: [ 3.17248114e-04 -4.32611065e-05]  gradient norm: 0.00032018414899884955\n",
            "iter: 6719  x: [-99.99984169  24.99997841]  f(x): 2.5527056947875765e-08  grad at x: [ 3.16613618e-04 -4.31745843e-05]  gradient norm: 0.0003195437807116625\n",
            "iter: 6720  x: [-99.99984201  24.99997846]  f(x): 2.542505082690841e-08  grad at x: [ 3.15980391e-04 -4.30882351e-05]  gradient norm: 0.0003189046931414363\n",
            "iter: 6721  x: [-99.99984233  24.9999785 ]  f(x): 2.5323452325045323e-08  grad at x: [ 3.15348430e-04 -4.30020586e-05]  gradient norm: 0.0003182668837629534\n",
            "iter: 6722  x: [-99.99984264  24.99997854]  f(x): 2.5222259809587274e-08  grad at x: [ 3.14717733e-04 -4.29160545e-05]  gradient norm: 0.0003176303499956342\n",
            "iter: 6723  x: [-99.99984296  24.99997858]  f(x): 2.512147165787566e-08  grad at x: [ 3.14088298e-04 -4.28302224e-05]  gradient norm: 0.0003169950892861002\n",
            "iter: 6724  x: [-99.99984327  24.99997863]  f(x): 2.5021086257384795e-08  grad at x: [ 3.13460121e-04 -4.27445620e-05]  gradient norm: 0.0003163610991091338\n",
            "iter: 6725  x: [-99.99984358  24.99997867]  f(x): 2.4921101996921457e-08  grad at x: [ 3.12833201e-04 -4.26590728e-05]  gradient norm: 0.00031572837691231655\n",
            "iter: 6726  x: [-99.9998439   24.99997871]  f(x): 2.4821517275373985e-08  grad at x: [ 3.12207534e-04 -4.25737547e-05]  gradient norm: 0.00031509692017139097\n",
            "iter: 6727  x: [-99.99984421  24.99997876]  f(x): 2.472233049279605e-08  grad at x: [ 3.11583119e-04 -4.24886072e-05]  gradient norm: 0.00031446672633393853\n",
            "iter: 6728  x: [-99.99984452  24.9999788 ]  f(x): 2.462354005927298e-08  grad at x: [ 3.10959953e-04 -4.24036300e-05]  gradient norm: 0.0003138377928757018\n",
            "iter: 6729  x: [-99.99984483  24.99997884]  f(x): 2.4525144394861464e-08  grad at x: [ 3.10338033e-04 -4.23188227e-05]  gradient norm: 0.0003132101173005844\n",
            "iter: 6730  x: [-99.99984514  24.99997888]  f(x): 2.4427141916325526e-08  grad at x: [ 3.09717357e-04 -4.22341850e-05]  gradient norm: 0.00031258369705616784\n",
            "iter: 6731  x: [-99.99984545  24.99997893]  f(x): 2.4329531059321337e-08  grad at x: [ 3.09097922e-04 -4.21497167e-05]  gradient norm: 0.0003119585296754768\n",
            "iter: 6732  x: [-99.99984576  24.99997897]  f(x): 2.423231025149703e-08  grad at x: [ 3.08479727e-04 -4.20654172e-05]  gradient norm: 0.0003113346126051328\n",
            "iter: 6733  x: [-99.99984607  24.99997901]  f(x): 2.4135477939607458e-08  grad at x: [ 3.07862767e-04 -4.19812864e-05]  gradient norm: 0.00031071194337912056\n",
            "iter: 6734  x: [-99.99984638  24.99997905]  f(x): 2.4039032571004472e-08  grad at x: [ 3.07247042e-04 -4.18973238e-05]  gradient norm: 0.0003100905195003838\n",
            "iter: 6735  x: [-99.99984668  24.99997909]  f(x): 2.3942972598918474e-08  grad at x: [ 3.06632547e-04 -4.18135292e-05]  gradient norm: 0.00030947033847474606\n",
            "iter: 6736  x: [-99.99984699  24.99997914]  f(x): 2.384729648169718e-08  grad at x: [ 3.06019282e-04 -4.17299021e-05]  gradient norm: 0.00030885139780611115\n",
            "iter: 6737  x: [-99.9998473   24.99997918]  f(x): 2.375200268323695e-08  grad at x: [ 3.05407244e-04 -4.16464423e-05]  gradient norm: 0.0003082336949993427\n",
            "iter: 6738  x: [-99.9998476   24.99997922]  f(x): 2.3657089681481033e-08  grad at x: [ 3.04796429e-04 -4.15631494e-05]  gradient norm: 0.0003076172276156264\n",
            "iter: 6739  x: [-99.99984791  24.99997926]  f(x): 2.3562555950879128e-08  grad at x: [ 3.04186836e-04 -4.14800231e-05]  gradient norm: 0.0003070019931588662\n",
            "iter: 6740  x: [-99.99984821  24.9999793 ]  f(x): 2.3468399975843796e-08  grad at x: [ 3.03578463e-04 -4.13970631e-05]  gradient norm: 0.00030638798916304664\n",
            "iter: 6741  x: [-99.99984851  24.99997934]  f(x): 2.3374620250395453e-08  grad at x: [ 3.02971306e-04 -4.13142690e-05]  gradient norm: 0.00030577521319031376\n",
            "iter: 6742  x: [-99.99984882  24.99997938]  f(x): 2.3281215269217446e-08  grad at x: [ 3.02365363e-04 -4.12316404e-05]  gradient norm: 0.00030516366277273216\n",
            "iter: 6743  x: [-99.99984912  24.99997943]  f(x): 2.318818353256006e-08  grad at x: [ 3.01760633e-04 -4.11491772e-05]  gradient norm: 0.0003045533354442867\n",
            "iter: 6744  x: [-99.99984942  24.99997947]  f(x): 2.3095523550064457e-08  grad at x: [ 3.01157111e-04 -4.10668788e-05]  gradient norm: 0.00030394422876616335\n",
            "iter: 6745  x: [-99.99984972  24.99997951]  f(x): 2.300323383672637e-08  grad at x: [ 3.00554797e-04 -4.09847450e-05]  gradient norm: 0.0003033363403005078\n",
            "iter: 6746  x: [-99.99985002  24.99997955]  f(x): 2.291131291273445e-08  grad at x: [ 2.99953687e-04 -4.09027755e-05]  gradient norm: 0.00030272966760946604\n",
            "iter: 6747  x: [-99.99985032  24.99997959]  f(x): 2.2819759307564493e-08  grad at x: [ 2.9935378e-04 -4.0820970e-05]  gradient norm: 0.000302124208282385\n",
            "iter: 6748  x: [-99.99985062  24.99997963]  f(x): 2.2728571547476308e-08  grad at x: [ 2.98755072e-04 -4.07393280e-05]  gradient norm: 0.00030151995985324955\n",
            "iter: 6749  x: [-99.99985092  24.99997967]  f(x): 2.2637748176767927e-08  grad at x: [ 2.98157562e-04 -4.06578494e-05]  gradient norm: 0.0003009169199414877\n",
            "iter: 6750  x: [-99.99985122  24.99997971]  f(x): 2.25472877360567e-08  grad at x: [ 2.97561247e-04 -4.05765337e-05]  gradient norm: 0.00030031508610828527\n",
            "iter: 6751  x: [-99.99985152  24.99997975]  f(x): 2.2457188775444387e-08  grad at x: [ 2.96966125e-04 -4.04953806e-05]  gradient norm: 0.00029971445594394934\n",
            "iter: 6752  x: [-99.99985181  24.99997979]  f(x): 2.2367449850247125e-08  grad at x: [ 2.96372193e-04 -4.04143899e-05]  gradient norm: 0.0002991150270397469\n",
            "iter: 6753  x: [-99.99985211  24.99997983]  f(x): 2.227806952069292e-08  grad at x: [ 2.95779448e-04 -4.03335611e-05]  gradient norm: 0.00029851679698598483\n",
            "iter: 6754  x: [-99.99985241  24.99997987]  f(x): 2.218904635624651e-08  grad at x: [ 2.95187889e-04 -4.02528940e-05]  gradient norm: 0.00029791976340113127\n",
            "iter: 6755  x: [-99.9998527   24.99997991]  f(x): 2.210037892732274e-08  grad at x: [ 2.94597513e-04 -4.01723882e-05]  gradient norm: 0.00029732392387645326\n",
            "iter: 6756  x: [-99.999853    24.99997995]  f(x): 2.201206581338412e-08  grad at x: [ 2.94008318e-04 -4.00920434e-05]  gradient norm: 0.0002967292760304188\n",
            "iter: 6757  x: [-99.99985329  24.99997999]  f(x): 2.1924105599000632e-08  grad at x: [ 2.93420302e-04 -4.00118593e-05]  gradient norm: 0.0002961358174824561\n",
            "iter: 6758  x: [-99.99985358  24.99998003]  f(x): 2.1836496873692284e-08  grad at x: [ 2.92833461e-04 -3.99318356e-05]  gradient norm: 0.00029554354585199305\n",
            "iter: 6759  x: [-99.99985388  24.99998007]  f(x): 2.1749238231914966e-08  grad at x: [ 2.92247794e-04 -3.98519719e-05]  gradient norm: 0.000294952458758458\n",
            "iter: 6760  x: [-99.99985417  24.99998011]  f(x): 2.1662328277049807e-08  grad at x: [ 2.91663299e-04 -3.97722680e-05]  gradient norm: 0.0002943625538484799\n",
            "iter: 6761  x: [-99.99985446  24.99998015]  f(x): 2.1575765613499146e-08  grad at x: [ 2.91079972e-04 -3.96927235e-05]  gradient norm: 0.000293773828742447\n",
            "iter: 6762  x: [-99.99985475  24.99998019]  f(x): 2.1489548854545382e-08  grad at x: [ 2.90497812e-04 -3.96133380e-05]  gradient norm: 0.00029318628108794846\n",
            "iter: 6763  x: [-99.99985504  24.99998023]  f(x): 2.1403676618317594e-08  grad at x: [ 2.89916817e-04 -3.95341113e-05]  gradient norm: 0.0002925999085325735\n",
            "iter: 6764  x: [-99.99985533  24.99998027]  f(x): 2.131814752777768e-08  grad at x: [ 2.89336983e-04 -3.94550431e-05]  gradient norm: 0.0002920147087239112\n",
            "iter: 6765  x: [-99.99985562  24.99998031]  f(x): 2.1232960210846465e-08  grad at x: [ 2.88758309e-04 -3.93761330e-05]  gradient norm: 0.0002914306793105109\n",
            "iter: 6766  x: [-99.99985591  24.99998035]  f(x): 2.1148113300109284e-08  grad at x: [ 2.88180792e-04 -3.92973808e-05]  gradient norm: 0.00029084781793996175\n",
            "iter: 6767  x: [-99.9998562   24.99998039]  f(x): 2.1063605441256512e-08  grad at x: [ 2.87604431e-04 -3.92187860e-05]  gradient norm: 0.0002902661223171351\n",
            "iter: 6768  x: [-99.99985649  24.99998043]  f(x): 2.097943527231394e-08  grad at x: [ 2.87029222e-04 -3.91403484e-05]  gradient norm: 0.00028968559006145914\n",
            "iter: 6769  x: [-99.99985677  24.99998047]  f(x): 2.0895601448455356e-08  grad at x: [ 2.86455163e-04 -3.90620677e-05]  gradient norm: 0.0002891062188778052\n",
            "iter: 6770  x: [-99.99985706  24.99998051]  f(x): 2.0812102625476175e-08  grad at x: [ 2.85882253e-04 -3.89839436e-05]  gradient norm: 0.00028852800644288363\n",
            "iter: 6771  x: [-99.99985734  24.99998055]  f(x): 2.0728937463875277e-08  grad at x: [ 2.85310489e-04 -3.89059757e-05]  gradient norm: 0.00028795095043340475\n",
            "iter: 6772  x: [-99.99985763  24.99998059]  f(x): 2.0646104628841533e-08  grad at x: [ 2.84739868e-04 -3.88281638e-05]  gradient norm: 0.0002873750485260788\n",
            "iter: 6773  x: [-99.99985791  24.99998062]  f(x): 2.056360279441639e-08  grad at x: [ 2.84170388e-04 -3.87505074e-05]  gradient norm: 0.0002868002984267373\n",
            "iter: 6774  x: [-99.9998582   24.99998066]  f(x): 2.0481430638996273e-08  grad at x: [ 2.83602047e-04 -3.86730064e-05]  gradient norm: 0.00028622669783929155\n",
            "iter: 6775  x: [-99.99985848  24.9999807 ]  f(x): 2.0399586841711355e-08  grad at x: [ 2.83034843e-04 -3.85956604e-05]  gradient norm: 0.00028565424444045186\n",
            "iter: 6776  x: [-99.99985877  24.99998074]  f(x): 2.031807009449358e-08  grad at x: [ 2.82468773e-04 -3.85184691e-05]  gradient norm: 0.0002850829359642108\n",
            "iter: 6777  x: [-99.99985905  24.99998078]  f(x): 2.0236879085820742e-08  grad at x: [ 2.81903836e-04 -3.84414321e-05]  gradient norm: 0.0002845127700882387\n",
            "iter: 6778  x: [-99.99985933  24.99998082]  f(x): 2.01560125166397e-08  grad at x: [ 2.81340028e-04 -3.83645493e-05]  gradient norm: 0.00028394374454556804\n",
            "iter: 6779  x: [-99.99985961  24.99998086]  f(x): 2.0075469092430356e-08  grad at x: [ 2.80777348e-04 -3.82878202e-05]  gradient norm: 0.00028337585706923133\n",
            "iter: 6780  x: [-99.99985989  24.99998089]  f(x): 1.999524751948215e-08  grad at x: [ 2.80215793e-04 -3.82112445e-05]  gradient norm: 0.00028280910536602\n",
            "iter: 6781  x: [-99.99986017  24.99998093]  f(x): 1.9915346512324647e-08  grad at x: [ 2.79655362e-04 -3.81348221e-05]  gradient norm: 0.00028224348716896657\n",
            "iter: 6782  x: [-99.99986045  24.99998097]  f(x): 1.983576478615073e-08  grad at x: [ 2.79096051e-04 -3.80585524e-05]  gradient norm: 0.00028167900018390244\n",
            "iter: 6783  x: [-99.99986073  24.99998101]  f(x): 1.9756501068581914e-08  grad at x: [ 2.78537859e-04 -3.79824353e-05]  gradient norm: 0.0002811156421729813\n",
            "iter: 6784  x: [-99.99986101  24.99998105]  f(x): 1.9677554091675537e-08  grad at x: [ 2.77980783e-04 -3.79064704e-05]  gradient norm: 0.00028055341089835664\n",
            "iter: 6785  x: [-99.99986129  24.99998108]  f(x): 1.9598922584027252e-08  grad at x: [ 2.77424822e-04 -3.78306575e-05]  gradient norm: 0.00027999230406586\n",
            "iter: 6786  x: [-99.99986157  24.99998112]  f(x): 1.9520605290644205e-08  grad at x: [ 2.76869972e-04 -3.77549962e-05]  gradient norm: 0.00027943231946676606\n",
            "iter: 6787  x: [-99.99986184  24.99998116]  f(x): 1.9442600952777727e-08  grad at x: [ 2.76316232e-04 -3.76794862e-05]  gradient norm: 0.00027887345483410735\n",
            "iter: 6788  x: [-99.99986212  24.9999812 ]  f(x): 1.9364908320416668e-08  grad at x: [ 2.75763600e-04 -3.76041272e-05]  gradient norm: 0.0002783157079319575\n",
            "iter: 6789  x: [-99.99986239  24.99998124]  f(x): 1.9287526147520527e-08  grad at x: [ 2.75212072e-04 -3.75289190e-05]  gradient norm: 0.0002777590765215101\n",
            "iter: 6790  x: [-99.99986267  24.99998127]  f(x): 1.92104531926763e-08  grad at x: [ 2.74661648e-04 -3.74538611e-05]  gradient norm: 0.0002772035583658788\n",
            "iter: 6791  x: [-99.99986294  24.99998131]  f(x): 1.91336882227131e-08  grad at x: [ 2.74112325e-04 -3.73789534e-05]  gradient norm: 0.0002766491512563384\n",
            "iter: 6792  x: [-99.99986322  24.99998135]  f(x): 1.905723000487563e-08  grad at x: [ 2.73564100e-04 -3.73041955e-05]  gradient norm: 0.00027609585295600245\n",
            "iter: 6793  x: [-99.99986349  24.99998139]  f(x): 1.898107731447813e-08  grad at x: [ 2.73016972e-04 -3.72295871e-05]  gradient norm: 0.0002755436612551857\n",
            "iter: 6794  x: [-99.99986376  24.99998142]  f(x): 1.8905228931378463e-08  grad at x: [ 2.72470938e-04 -3.71551279e-05]  gradient norm: 0.0002749925739461229\n",
            "iter: 6795  x: [-99.99986404  24.99998146]  f(x): 1.8829683635572038e-08  grad at x: [ 2.71925996e-04 -3.70808177e-05]  gradient norm: 0.00027444258879096765\n",
            "iter: 6796  x: [-99.99986431  24.9999815 ]  f(x): 1.875444021931649e-08  grad at x: [ 2.71382144e-04 -3.70066560e-05]  gradient norm: 0.0002738937036101158\n",
            "iter: 6797  x: [-99.99986458  24.99998153]  f(x): 1.8679497474968204e-08  grad at x: [ 2.70839380e-04 -3.69326427e-05]  gradient norm: 0.0002733459161938821\n",
            "iter: 6798  x: [-99.99986485  24.99998157]  f(x): 1.860485420321755e-08  grad at x: [ 2.70297701e-04 -3.68587774e-05]  gradient norm: 0.00027279922436266235\n",
            "iter: 6799  x: [-99.99986512  24.99998161]  f(x): 1.8530509204976733e-08  grad at x: [ 2.69757106e-04 -3.67850599e-05]  gradient norm: 0.0002722536259077314\n",
            "iter: 6800  x: [-99.99986539  24.99998164]  f(x): 1.845646128931997e-08  grad at x: [ 2.69217592e-04 -3.67114898e-05]  gradient norm: 0.00027170911864948495\n",
            "iter: 6801  x: [-99.99986566  24.99998168]  f(x): 1.838270926935338e-08  grad at x: [ 2.68679157e-04 -3.66380668e-05]  gradient norm: 0.00027116570040735887\n",
            "iter: 6802  x: [-99.99986593  24.99998172]  f(x): 1.8309251962464218e-08  grad at x: [ 2.68141798e-04 -3.65647906e-05]  gradient norm: 0.00027062336900174913\n",
            "iter: 6803  x: [-99.9998662   24.99998175]  f(x): 1.8236088190048412e-08  grad at x: [ 2.67605515e-04 -3.64916611e-05]  gradient norm: 0.0002700821222520914\n",
            "iter: 6804  x: [-99.99986646  24.99998179]  f(x): 1.8163216781683503e-08  grad at x: [ 2.67070304e-04 -3.64186777e-05]  gradient norm: 0.00026954195800790276\n",
            "iter: 6805  x: [-99.99986673  24.99998183]  f(x): 1.809063656698435e-08  grad at x: [ 2.66536163e-04 -3.63458404e-05]  gradient norm: 0.000269002874088619\n",
            "iter: 6806  x: [-99.999867    24.99998186]  f(x): 1.801834638344997e-08  grad at x: [ 2.66003091e-04 -3.62731487e-05]  gradient norm: 0.000268464868341837\n",
            "iter: 6807  x: [-99.99986726  24.9999819 ]  f(x): 1.7946345072898618e-08  grad at x: [ 2.65471084e-04 -3.62006024e-05]  gradient norm: 0.0002679279386170738\n",
            "iter: 6808  x: [-99.99986753  24.99998194]  f(x): 1.7874631477303533e-08  grad at x: [ 2.64940142e-04 -3.61282012e-05]  gradient norm: 0.0002673920827347252\n",
            "iter: 6809  x: [-99.99986779  24.99998197]  f(x): 1.7803204450220914e-08  grad at x: [ 2.64410262e-04 -3.60559448e-05]  gradient norm: 0.00026685729857150927\n",
            "iter: 6810  x: [-99.99986806  24.99998201]  f(x): 1.7732062845580954e-08  grad at x: [ 2.63881441e-04 -3.59838329e-05]  gradient norm: 0.00026632358397694303\n",
            "iter: 6811  x: [-99.99986832  24.99998204]  f(x): 1.766120552107244e-08  grad at x: [ 2.63353679e-04 -3.59118652e-05]  gradient norm: 0.0002657909367986233\n",
            "iter: 6812  x: [-99.99986859  24.99998208]  f(x): 1.759063134250545e-08  grad at x: [ 2.62826971e-04 -3.58400415e-05]  gradient norm: 0.0002652593549151882\n",
            "iter: 6813  x: [-99.99986885  24.99998212]  f(x): 1.752033917926873e-08  grad at x: [ 2.62301317e-04 -3.57683614e-05]  gradient norm: 0.00026472883620239584\n",
            "iter: 6814  x: [-99.99986911  24.99998215]  f(x): 1.7450327904956328e-08  grad at x: [ 2.61776715e-04 -3.56968247e-05]  gradient norm: 0.0002641993785379241\n",
            "iter: 6815  x: [-99.99986937  24.99998219]  f(x): 1.7380596393388023e-08  grad at x: [ 2.61253161e-04 -3.56254310e-05]  gradient norm: 0.0002636709797712901\n",
            "iter: 6816  x: [-99.99986963  24.99998222]  f(x): 1.7311143529750788e-08  grad at x: [ 2.60730655e-04 -3.55541802e-05]  gradient norm: 0.00026314363780833303\n",
            "iter: 6817  x: [-99.9998699   24.99998226]  f(x): 1.7241968199293895e-08  grad at x: [ 2.60209194e-04 -3.54830718e-05]  gradient norm: 0.0002626173505257708\n",
            "iter: 6818  x: [-99.99987016  24.99998229]  f(x): 1.7173069294988498e-08  grad at x: [ 2.59688775e-04 -3.54121057e-05]  gradient norm: 0.0002620921158294427\n",
            "iter: 6819  x: [-99.99987042  24.99998233]  f(x): 1.71044457099867e-08  grad at x: [ 2.59169398e-04 -3.53412815e-05]  gradient norm: 0.0002615679315970266\n",
            "iter: 6820  x: [-99.99987067  24.99998236]  f(x): 1.7036096345123987e-08  grad at x: [ 2.58651059e-04 -3.52705989e-05]  gradient norm: 0.0002610447957353219\n",
            "iter: 6821  x: [-99.99987093  24.9999824 ]  f(x): 1.6968020104826597e-08  grad at x: [ 2.58133757e-04 -3.52000577e-05]  gradient norm: 0.00026052270614920764\n",
            "iter: 6822  x: [-99.99987119  24.99998244]  f(x): 1.6900215897602308e-08  grad at x: [ 2.57617489e-04 -3.51296576e-05]  gradient norm: 0.000260001660745483\n",
            "iter: 6823  x: [-99.99987145  24.99998247]  f(x): 1.6832682635528745e-08  grad at x: [ 2.57102254e-04 -3.50593983e-05]  gradient norm: 0.0002594816574290271\n",
            "iter: 6824  x: [-99.99987171  24.99998251]  f(x): 1.6765419234866496e-08  grad at x: [ 2.56588050e-04 -3.49892795e-05]  gradient norm: 0.0002589626941075992\n",
            "iter: 6825  x: [-99.99987196  24.99998254]  f(x): 1.669842461893918e-08  grad at x: [ 2.56074874e-04 -3.49193009e-05]  gradient norm: 0.00025844476871423943\n",
            "iter: 6826  x: [-99.99987222  24.99998258]  f(x): 1.6631697715201e-08  grad at x: [ 2.55562724e-04 -3.48494623e-05]  gradient norm: 0.00025792787918486827\n",
            "iter: 6827  x: [-99.99987247  24.99998261]  f(x): 1.656523745085642e-08  grad at x: [ 2.55051598e-04 -3.47797634e-05]  gradient norm: 0.0002574120234243647\n",
            "iter: 6828  x: [-99.99987273  24.99998264]  f(x): 1.6499042760863853e-08  grad at x: [ 2.54541495e-04 -3.47102039e-05]  gradient norm: 0.00025689719936864905\n",
            "iter: 6829  x: [-99.99987298  24.99998268]  f(x): 1.6433112587269636e-08  grad at x: [ 2.54032412e-04 -3.46407835e-05]  gradient norm: 0.0002563834049798827\n",
            "iter: 6830  x: [-99.99987324  24.99998271]  f(x): 1.6367445868723797e-08  grad at x: [ 2.53524347e-04 -3.45715019e-05]  gradient norm: 0.0002558706381648648\n",
            "iter: 6831  x: [-99.99987349  24.99998275]  f(x): 1.6302041554800214e-08  grad at x: [ 2.53017299e-04 -3.45023589e-05]  gradient norm: 0.00025535889688671677\n",
            "iter: 6832  x: [-99.99987374  24.99998278]  f(x): 1.6236898595148764e-08  grad at x: [ 2.52511264e-04 -3.44333542e-05]  gradient norm: 0.00025484817908039885\n",
            "iter: 6833  x: [-99.999874    24.99998282]  f(x): 1.61720159466832e-08  grad at x: [ 2.52006242e-04 -3.43644875e-05]  gradient norm: 0.00025433848270903247\n",
            "iter: 6834  x: [-99.99987425  24.99998285]  f(x): 1.6107392570082577e-08  grad at x: [ 2.51502229e-04 -3.42957585e-05]  gradient norm: 0.0002538298057366989\n",
            "iter: 6835  x: [-99.9998745   24.99998289]  f(x): 1.6043027429414843e-08  grad at x: [ 2.50999225e-04 -3.42271670e-05]  gradient norm: 0.0002533221461255596\n",
            "iter: 6836  x: [-99.99987475  24.99998292]  f(x): 1.5978919492614322e-08  grad at x: [ 2.50497226e-04 -3.41587126e-05]  gradient norm: 0.0002528155018396959\n",
            "iter: 6837  x: [-99.999875    24.99998295]  f(x): 1.59150677311056e-08  grad at x: [ 2.49996232e-04 -3.40903952e-05]  gradient norm: 0.00025230987084222926\n",
            "iter: 6838  x: [-99.99987525  24.99998299]  f(x): 1.5851471120036145e-08  grad at x: [ 2.49496239e-04 -3.40222144e-05]  gradient norm: 0.000251805251097241\n",
            "iter: 6839  x: [-99.9998755   24.99998302]  f(x): 1.5788128641561994e-08  grad at x: [ 2.48997247e-04 -3.39541700e-05]  gradient norm: 0.00025130164059601357\n",
            "iter: 6840  x: [-99.99987575  24.99998306]  f(x): 1.5725039277981304e-08  grad at x: [ 2.48499252e-04 -3.38862616e-05]  gradient norm: 0.00025079903730262844\n",
            "iter: 6841  x: [-99.999876    24.99998309]  f(x): 1.5662202022090996e-08  grad at x: [ 2.48002254e-04 -3.38184891e-05]  gradient norm: 0.0002502974392365291\n",
            "iter: 6842  x: [-99.99987625  24.99998312]  f(x): 1.5599615863397866e-08  grad at x: [ 2.47506249e-04 -3.37508521e-05]  gradient norm: 0.00024979684436275706\n",
            "iter: 6843  x: [-99.99987649  24.99998316]  f(x): 1.5537279798228624e-08  grad at x: [ 2.47011237e-04 -3.36833504e-05]  gradient norm: 0.00024929725067259466\n",
            "iter: 6844  x: [-99.99987674  24.99998319]  f(x): 1.547519282654207e-08  grad at x: [ 2.46517214e-04 -3.36159837e-05]  gradient norm: 0.0002487986561582845\n",
            "iter: 6845  x: [-99.99987699  24.99998323]  f(x): 1.5413353955294853e-08  grad at x: [ 2.46024180e-04 -3.35487518e-05]  gradient norm: 0.0002483010588402301\n",
            "iter: 6846  x: [-99.99987723  24.99998326]  f(x): 1.5351762191418075e-08  grad at x: [ 2.45532131e-04 -3.34816543e-05]  gradient norm: 0.000247804456710674\n",
            "iter: 6847  x: [-99.99987748  24.99998329]  f(x): 1.5290416548806785e-08  grad at x: [ 2.45041067e-04 -3.34146910e-05]  gradient norm: 0.00024730884779001973\n",
            "iter: 6848  x: [-99.99987772  24.99998333]  f(x): 1.522931604491839e-08  grad at x: [ 2.44550985e-04 -3.33478616e-05]  gradient norm: 0.0002468142300996309\n",
            "iter: 6849  x: [-99.99987797  24.99998336]  f(x): 1.516845969693859e-08  grad at x: [ 2.44061883e-04 -3.32811659e-05]  gradient norm: 0.00024632060163079\n",
            "iter: 6850  x: [-99.99987821  24.99998339]  f(x): 1.5107846532782833e-08  grad at x: [ 2.43573759e-04 -3.32146035e-05]  gradient norm: 0.0002458279604339818\n",
            "iter: 6851  x: [-99.99987846  24.99998343]  f(x): 1.5047475576489133e-08  grad at x: [ 2.43086612e-04 -3.31481743e-05]  gradient norm: 0.00024533630450048876\n",
            "iter: 6852  x: [-99.9998787   24.99998346]  f(x): 1.4987345862658976e-08  grad at x: [ 2.42600439e-04 -3.30818780e-05]  gradient norm: 0.00024484563187983545\n",
            "iter: 6853  x: [-99.99987894  24.99998349]  f(x): 1.492745642926022e-08  grad at x: [ 2.42115238e-04 -3.30157142e-05]  gradient norm: 0.0002443559406215467\n",
            "iter: 6854  x: [-99.99987918  24.99998353]  f(x): 1.4867806314183716e-08  grad at x: [ 2.41631007e-04 -3.29496828e-05]  gradient norm: 0.00024386722874698613\n",
            "iter: 6855  x: [-99.99987943  24.99998356]  f(x): 1.4808394558578192e-08  grad at x: [ 2.41147745e-04 -3.28837834e-05]  gradient norm: 0.00024337949427655727\n",
            "iter: 6856  x: [-99.99987967  24.99998359]  f(x): 1.4749220213915042e-08  grad at x: [ 2.40665450e-04 -3.28180159e-05]  gradient norm: 0.0002428927352879459\n",
            "iter: 6857  x: [-99.99987991  24.99998362]  f(x): 1.4690282331553061e-08  grad at x: [ 2.40184119e-04 -3.27523798e-05]  gradient norm: 0.00024240694983067678\n",
            "iter: 6858  x: [-99.99988015  24.99998366]  f(x): 1.463157996276323e-08  grad at x: [ 2.39703751e-04 -3.26868751e-05]  gradient norm: 0.00024192213592611348\n",
            "iter: 6859  x: [-99.99988039  24.99998369]  f(x): 1.4573112168952026e-08  grad at x: [ 2.39224343e-04 -3.26215013e-05]  gradient norm: 0.0002414382916519418\n",
            "iter: 6860  x: [-99.99988063  24.99998372]  f(x): 1.4514878011405488e-08  grad at x: [ 2.38745894e-04 -3.25562583e-05]  gradient norm: 0.00024095541505768646\n",
            "iter: 6861  x: [-99.99988087  24.99998375]  f(x): 1.445687655808582e-08  grad at x: [ 2.38268403e-04 -3.24911458e-05]  gradient norm: 0.00024047350422103323\n",
            "iter: 6862  x: [-99.9998811   24.99998379]  f(x): 1.4399106880208895e-08  grad at x: [ 2.37791866e-04 -3.24261635e-05]  gradient norm: 0.0002399925572196679\n",
            "iter: 6863  x: [-99.99988134  24.99998382]  f(x): 1.4341568048977462e-08  grad at x: [ 2.37316282e-04 -3.23613112e-05]  gradient norm: 0.00023951257210407526\n",
            "iter: 6864  x: [-99.99988158  24.99998385]  f(x): 1.428425914222119e-08  grad at x: [ 2.36841649e-04 -3.22965886e-05]  gradient norm: 0.00023903354695290106\n",
            "iter: 6865  x: [-99.99988182  24.99998388]  f(x): 1.4227179244124613e-08  grad at x: [ 2.36367966e-04 -3.22319954e-05]  gradient norm: 0.0002385554798710322\n",
            "iter: 6866  x: [-99.99988205  24.99998392]  f(x): 1.4170327435701212e-08  grad at x: [ 2.35895230e-04 -3.21675314e-05]  gradient norm: 0.00023807836890991346\n",
            "iter: 6867  x: [-99.99988229  24.99998395]  f(x): 1.4113702807659962e-08  grad at x: [ 2.35423440e-04 -3.21031963e-05]  gradient norm: 0.00023760221217539168\n",
            "iter: 6868  x: [-99.99988252  24.99998398]  f(x): 1.4057304450656249e-08  grad at x: [ 2.34952593e-04 -3.20389899e-05]  gradient norm: 0.0002371270077461127\n",
            "iter: 6869  x: [-99.99988276  24.99998401]  f(x): 1.4001131461752293e-08  grad at x: [ 2.34482688e-04 -3.19749120e-05]  gradient norm: 0.00023665275372792343\n",
            "iter: 6870  x: [-99.99988299  24.99998404]  f(x): 1.3945182941277026e-08  grad at x: [ 2.34013722e-04 -3.19109621e-05]  gradient norm: 0.00023617944822763073\n",
            "iter: 6871  x: [-99.99988323  24.99998408]  f(x): 1.3889457989497086e-08  grad at x: [ 2.33545695e-04 -3.18471402e-05]  gradient norm: 0.0002357070893248405\n",
            "iter: 6872  x: [-99.99988346  24.99998411]  f(x): 1.3833955716237884e-08  grad at x: [ 2.33078604e-04 -3.17834459e-05]  gradient norm: 0.0002352356751535607\n",
            "iter: 6873  x: [-99.99988369  24.99998414]  f(x): 1.3778675227926138e-08  grad at x: [ 2.32612446e-04 -3.17198790e-05]  gradient norm: 0.00023476520379243717\n",
            "iter: 6874  x: [-99.99988393  24.99998417]  f(x): 1.3723615640842083e-08  grad at x: [ 2.32147221e-04 -3.16564393e-05]  gradient norm: 0.00023429567337739793\n",
            "iter: 6875  x: [-99.99988416  24.9999842 ]  f(x): 1.366877607412177e-08  grad at x: [ 2.31682927e-04 -3.15931264e-05]  gradient norm: 0.00023382708204245094\n",
            "iter: 6876  x: [-99.99988439  24.99998424]  f(x): 1.3614155643626973e-08  grad at x: [ 2.31219561e-04 -3.15299401e-05]  gradient norm: 0.00023335942786720208\n",
            "iter: 6877  x: [-99.99988462  24.99998427]  f(x): 1.3559753478172592e-08  grad at x: [ 2.30757122e-04 -3.14668803e-05]  gradient norm: 0.0002328927090157405\n",
            "iter: 6878  x: [-99.99988485  24.9999843 ]  f(x): 1.3505568702944239e-08  grad at x: [ 2.30295608e-04 -3.14039465e-05]  gradient norm: 0.00023242692359487304\n",
            "iter: 6879  x: [-99.99988508  24.99998433]  f(x): 1.345160044946603e-08  grad at x: [ 2.29835017e-04 -3.13411386e-05]  gradient norm: 0.0002319620697395678\n",
            "iter: 6880  x: [-99.99988531  24.99998436]  f(x): 1.3397847855670213e-08  grad at x: [ 2.29375347e-04 -3.12784563e-05]  gradient norm: 0.00023149814561391384\n",
            "iter: 6881  x: [-99.99988554  24.99998439]  f(x): 1.3344310055873696e-08  grad at x: [ 2.28916596e-04 -3.12158994e-05]  gradient norm: 0.00023103514932471808\n",
            "iter: 6882  x: [-99.99988577  24.99998442]  f(x): 1.3290986194043408e-08  grad at x: [ 2.28458763e-04 -3.11534676e-05]  gradient norm: 0.00023057307903606968\n",
            "iter: 6883  x: [-99.999886    24.99998445]  f(x): 1.3237875413890737e-08  grad at x: [ 2.28001845e-04 -3.10911607e-05]  gradient norm: 0.00023011193288389664\n",
            "iter: 6884  x: [-99.99988623  24.99998449]  f(x): 1.318497686212906e-08  grad at x: [ 2.27545841e-04 -3.10289784e-05]  gradient norm: 0.00022965170900412704\n",
            "iter: 6885  x: [-99.99988645  24.99998452]  f(x): 1.3132289694919436e-08  grad at x: [ 2.27090750e-04 -3.09669204e-05]  gradient norm: 0.00022919240558901105\n",
            "iter: 6886  x: [-99.99988668  24.99998455]  f(x): 1.3079813064914688e-08  grad at x: [ 2.26636568e-04 -3.09049866e-05]  gradient norm: 0.00022873402077447673\n",
            "iter: 6887  x: [-99.99988691  24.99998458]  f(x): 1.302754613095828e-08  grad at x: [ 2.26183295e-04 -3.08431766e-05]  gradient norm: 0.00022827655272461322\n",
            "iter: 6888  x: [-99.99988713  24.99998461]  f(x): 1.2975488058043611e-08  grad at x: [ 2.25730929e-04 -3.07814902e-05]  gradient norm: 0.00022781999963167072\n",
            "iter: 6889  x: [-99.99988736  24.99998464]  f(x): 1.29236380076693e-08  grad at x: [ 2.25279467e-04 -3.07199273e-05]  gradient norm: 0.00022736435963157726\n",
            "iter: 6890  x: [-99.99988759  24.99998467]  f(x): 1.287199515067499e-08  grad at x: [ 2.24828908e-04 -3.06584874e-05]  gradient norm: 0.00022690963091658308\n",
            "iter: 6891  x: [-99.99988781  24.9999847 ]  f(x): 1.282055865771178e-08  grad at x: [ 2.24379250e-04 -3.05971704e-05]  gradient norm: 0.0002264558116517373\n",
            "iter: 6892  x: [-99.99988803  24.99998473]  f(x): 1.2769327705412334e-08  grad at x: [ 2.23930491e-04 -3.05359761e-05]  gradient norm: 0.0002260029000292902\n",
            "iter: 6893  x: [-99.99988826  24.99998476]  f(x): 1.2718301473392437e-08  grad at x: [ 2.23482630e-04 -3.04749041e-05]  gradient norm: 0.00022555089424245196\n",
            "iter: 6894  x: [-99.99988848  24.99998479]  f(x): 1.2667479140856106e-08  grad at x: [ 2.23035665e-04 -3.04139543e-05]  gradient norm: 0.00022509979245531175\n",
            "iter: 6895  x: [-99.99988871  24.99998482]  f(x): 1.261685989316223e-08  grad at x: [ 2.22589594e-04 -3.03531264e-05]  gradient norm: 0.0002246495928610798\n",
            "iter: 6896  x: [-99.99988893  24.99998485]  f(x): 1.2566442921568331e-08  grad at x: [ 2.22144415e-04 -3.02924202e-05]  gradient norm: 0.00022420029368016743\n",
            "iter: 6897  x: [-99.99988915  24.99998488]  f(x): 1.251622741721305e-08  grad at x: [ 2.21700126e-04 -3.02318353e-05]  gradient norm: 0.00022375189310674492\n",
            "iter: 6898  x: [-99.99988937  24.99998491]  f(x): 1.2466212573852985e-08  grad at x: [ 2.21256726e-04 -3.01713717e-05]  gradient norm: 0.00022330438933306247\n",
            "iter: 6899  x: [-99.99988959  24.99998494]  f(x): 1.2416397588177656e-08  grad at x: [ 2.20814212e-04 -3.01110289e-05]  gradient norm: 0.00022285778055233034\n",
            "iter: 6900  x: [-99.99988981  24.99998497]  f(x): 1.2366781662825262e-08  grad at x: [ 2.20372584e-04 -3.00508069e-05]  gradient norm: 0.00022241206498591987\n",
            "iter: 6901  x: [-99.99989003  24.999985  ]  f(x): 1.231736400321786e-08  grad at x: [ 2.19931839e-04 -2.99907053e-05]  gradient norm: 0.00022196724085520243\n",
            "iter: 6902  x: [-99.99989025  24.99998503]  f(x): 1.2268143817553438e-08  grad at x: [ 2.19491975e-04 -2.99307238e-05]  gradient norm: 0.00022152330638154928\n",
            "iter: 6903  x: [-99.99989047  24.99998506]  f(x): 1.2219120313685062e-08  grad at x: [ 2.19052991e-04 -2.98708624e-05]  gradient norm: 0.00022108025975817075\n",
            "iter: 6904  x: [-99.99989069  24.99998509]  f(x): 1.217029270857012e-08  grad at x: [ 2.18614885e-04 -2.98111207e-05]  gradient norm: 0.00022063809923555922\n",
            "iter: 6905  x: [-99.99989091  24.99998512]  f(x): 1.2121660218686258e-08  grad at x: [ 2.18177655e-04 -2.97514984e-05]  gradient norm: 0.00022019682303508613\n",
            "iter: 6906  x: [-99.99989113  24.99998515]  f(x): 1.207322206325546e-08  grad at x: [ 2.17741300e-04 -2.96919954e-05]  gradient norm: 0.00021975642937812273\n",
            "iter: 6907  x: [-99.99989135  24.99998518]  f(x): 1.2024977467324304e-08  grad at x: [ 2.17305817e-04 -2.96326114e-05]  gradient norm: 0.00021931691651420147\n",
            "iter: 6908  x: [-99.99989156  24.99998521]  f(x): 1.1976925658748382e-08  grad at x: [ 2.16871206e-04 -2.95733462e-05]  gradient norm: 0.00021887828269381485\n",
            "iter: 6909  x: [-99.99989178  24.99998524]  f(x): 1.1929065865002918e-08  grad at x: [ 2.16437463e-04 -2.95141995e-05]  gradient norm: 0.0002184405261392942\n",
            "iter: 6910  x: [-99.999892    24.99998527]  f(x): 1.1881397319240433e-08  grad at x: [ 2.16004588e-04 -2.94551711e-05]  gradient norm: 0.00021800364510017196\n",
            "iter: 6911  x: [-99.99989221  24.9999853 ]  f(x): 1.1833919254335226e-08  grad at x: [ 2.15572579e-04 -2.93962608e-05]  gradient norm: 0.0002175676377987795\n",
            "iter: 6912  x: [-99.99989243  24.99998533]  f(x): 1.1786630911973125e-08  grad at x: [ 2.15141434e-04 -2.93374683e-05]  gradient norm: 0.00021713250251377038\n",
            "iter: 6913  x: [-99.99989264  24.99998536]  f(x): 1.1739531533428879e-08  grad at x: [ 2.14711151e-04 -2.92787933e-05]  gradient norm: 0.00021669823749563704\n",
            "iter: 6914  x: [-99.99989286  24.99998539]  f(x): 1.1692620365575629e-08  grad at x: [ 2.14281729e-04 -2.92202357e-05]  gradient norm: 0.000216264841022073\n",
            "iter: 6915  x: [-99.99989307  24.99998542]  f(x): 1.1645896655080621e-08  grad at x: [ 2.13853165e-04 -2.91617953e-05]  gradient norm: 0.0002158323113445308\n",
            "iter: 6916  x: [-99.99989329  24.99998545]  f(x): 1.1599359651045422e-08  grad at x: [ 2.13425459e-04 -2.91034717e-05]  gradient norm: 0.00021540064671254284\n",
            "iter: 6917  x: [-99.9998935   24.99998548]  f(x): 1.1553008608440075e-08  grad at x: [ 2.12998608e-04 -2.90452647e-05]  gradient norm: 0.00021496984540572266\n",
            "iter: 6918  x: [-99.99989371  24.99998551]  f(x): 1.1506842784733412e-08  grad at x: [ 2.12572611e-04 -2.89871742e-05]  gradient norm: 0.00021453990570272387\n",
            "iter: 6919  x: [-99.99989393  24.99998554]  f(x): 1.1460861439989441e-08  grad at x: [ 2.12147466e-04 -2.89291998e-05]  gradient norm: 0.0002141108258822\n",
            "iter: 6920  x: [-99.99989414  24.99998556]  f(x): 1.1415063836859936e-08  grad at x: [ 2.11723171e-04 -2.88713414e-05]  gradient norm: 0.00021368260422280458\n",
            "iter: 6921  x: [-99.99989435  24.99998559]  f(x): 1.1369449240679414e-08  grad at x: [ 2.11299724e-04 -2.88135988e-05]  gradient norm: 0.00021325523900415122\n",
            "iter: 6922  x: [-99.99989456  24.99998562]  f(x): 1.1324016922249156e-08  grad at x: [ 2.10877125e-04 -2.87559716e-05]  gradient norm: 0.00021282872853305455\n",
            "iter: 6923  x: [-99.99989477  24.99998565]  f(x): 1.1278766152023306e-08  grad at x: [ 2.10455371e-04 -2.86984596e-05]  gradient norm: 0.00021240307108912813\n",
            "iter: 6924  x: [-99.99989498  24.99998568]  f(x): 1.1233696202911797e-08  grad at x: [ 2.10034460e-04 -2.86410627e-05]  gradient norm: 0.00021197826495102557\n",
            "iter: 6925  x: [-99.99989519  24.99998571]  f(x): 1.1188806353455943e-08  grad at x: [ 2.09614391e-04 -2.85837806e-05]  gradient norm: 0.00021155430842652147\n",
            "iter: 6926  x: [-99.9998954   24.99998574]  f(x): 1.1144095884614773e-08  grad at x: [ 2.09195162e-04 -2.85266130e-05]  gradient norm: 0.00021113119982243053\n",
            "iter: 6927  x: [-99.99989561  24.99998577]  f(x): 1.1099564076994251e-08  grad at x: [ 2.08776772e-04 -2.84695598e-05]  gradient norm: 0.0002107089374183663\n",
            "iter: 6928  x: [-99.99989582  24.99998579]  f(x): 1.1055210219650724e-08  grad at x: [ 2.08359218e-04 -2.84126207e-05]  gradient norm: 0.00021028751955026455\n",
            "iter: 6929  x: [-99.99989603  24.99998582]  f(x): 1.101103359820425e-08  grad at x: [ 2.07942500e-04 -2.83557954e-05]  gradient norm: 0.00020986694449773885\n",
            "iter: 6930  x: [-99.99989624  24.99998585]  f(x): 1.0967033506687218e-08  grad at x: [ 2.07526615e-04 -2.82990838e-05]  gradient norm: 0.000209447210596725\n",
            "iter: 6931  x: [-99.99989644  24.99998588]  f(x): 1.0923209241591642e-08  grad at x: [ 2.07111562e-04 -2.82424857e-05]  gradient norm: 0.00020902831618315872\n",
            "iter: 6932  x: [-99.99989665  24.99998591]  f(x): 1.087956009892484e-08  grad at x: [ 2.06697339e-04 -2.81860007e-05]  gradient norm: 0.00020861025956481468\n",
            "iter: 6933  x: [-99.99989686  24.99998594]  f(x): 1.0836085377163286e-08  grad at x: [ 2.06283944e-04 -2.81296287e-05]  gradient norm: 0.00020819303904946762\n",
            "iter: 6934  x: [-99.99989706  24.99998596]  f(x): 1.0792784380171147e-08  grad at x: [ 2.05871376e-04 -2.80733694e-05]  gradient norm: 0.00020777665297305324\n",
            "iter: 6935  x: [-99.99989727  24.99998599]  f(x): 1.0749656414343732e-08  grad at x: [ 2.05459633e-04 -2.80172227e-05]  gradient norm: 0.0002073610996724673\n",
            "iter: 6936  x: [-99.99989748  24.99998602]  f(x): 1.0706700788400891e-08  grad at x: [ 2.05048714e-04 -2.79611883e-05]  gradient norm: 0.00020694637748364567\n",
            "iter: 6937  x: [-99.99989768  24.99998605]  f(x): 1.0663916813480232e-08  grad at x: [ 2.04638617e-04 -2.79052659e-05]  gradient norm: 0.00020653248474252405\n",
            "iter: 6938  x: [-99.99989789  24.99998608]  f(x): 1.0621303803229138e-08  grad at x: [ 2.04229339e-04 -2.78494553e-05]  gradient norm: 0.00020611941978599822\n",
            "iter: 6939  x: [-99.99989809  24.9999861 ]  f(x): 1.0578861073599418e-08  grad at x: [ 2.03820881e-04 -2.77937564e-05]  gradient norm: 0.00020570718095000396\n",
            "iter: 6940  x: [-99.99989829  24.99998613]  f(x): 1.053658794592917e-08  grad at x: [ 2.03413239e-04 -2.77381689e-05]  gradient norm: 0.00020529576659959815\n",
            "iter: 6941  x: [-99.9998985   24.99998616]  f(x): 1.0494483741037893e-08  grad at x: [ 2.03006412e-04 -2.76826926e-05]  gradient norm: 0.0002048851750716766\n",
            "iter: 6942  x: [-99.9998987   24.99998619]  f(x): 1.0452547784908693e-08  grad at x: [ 2.02600400e-04 -2.76273272e-05]  gradient norm: 0.00020447540473033613\n",
            "iter: 6943  x: [-99.9998989   24.99998621]  f(x): 1.0410779403102824e-08  grad at x: [ 2.02195199e-04 -2.75720726e-05]  gradient norm: 0.00020406645391247258\n",
            "iter: 6944  x: [-99.9998991   24.99998624]  f(x): 1.0369177929285827e-08  grad at x: [ 2.01790808e-04 -2.75169284e-05]  gradient norm: 0.0002036583210113039\n",
            "iter: 6945  x: [-99.99989931  24.99998627]  f(x): 1.0327742693618264e-08  grad at x: [ 2.01387227e-04 -2.74618946e-05]  gradient norm: 0.00020325100436276584\n",
            "iter: 6946  x: [-99.99989951  24.9999863 ]  f(x): 1.0286473034524127e-08  grad at x: [ 2.00984452e-04 -2.74069708e-05]  gradient norm: 0.0002028445023610364\n",
            "iter: 6947  x: [-99.99989971  24.99998632]  f(x): 1.0245368286931655e-08  grad at x: [ 2.00582483e-04 -2.73521568e-05]  gradient norm: 0.00020243881334301142\n",
            "iter: 6948  x: [-99.99989991  24.99998635]  f(x): 1.0204427796548528e-08  grad at x: [ 2.00181318e-04 -2.72974525e-05]  gradient norm: 0.0002020339357291099\n",
            "iter: 6949  x: [-99.99990011  24.99998638]  f(x): 1.016365090302674e-08  grad at x: [ 1.99780956e-04 -2.72428576e-05]  gradient norm: 0.00020162986785718766\n",
            "iter: 6950  x: [-99.99990031  24.99998641]  f(x): 1.0123036953916565e-08  grad at x: [ 1.99381394e-04 -2.71883719e-05]  gradient norm: 0.0002012266081204627\n",
            "iter: 6951  x: [-99.99990051  24.99998643]  f(x): 1.0082585299141501e-08  grad at x: [ 1.98982631e-04 -2.71339951e-05]  gradient norm: 0.000200824154913113\n",
            "iter: 6952  x: [-99.99990071  24.99998646]  f(x): 1.004229528788014e-08  grad at x: [ 1.98584666e-04 -2.70797272e-05]  gradient norm: 0.0002004225065992354\n",
            "iter: 6953  x: [-99.99990091  24.99998649]  f(x): 1.0002166274606732e-08  grad at x: [ 1.98187497e-04 -2.70255677e-05]  gradient norm: 0.00020002166157300795\n",
            "iter: 6954  x: [-99.9999011   24.99998651]  f(x): 9.962197618862875e-09  grad at x: [ 1.97791122e-04 -2.69715166e-05]  gradient norm: 0.00019962161825676973\n",
            "iter: 6955  x: [-99.9999013   24.99998654]  f(x): 9.92238867671186e-09  grad at x: [ 1.97395539e-04 -2.69175735e-05]  gradient norm: 0.0001992223750155776\n",
            "iter: 6956  x: [-99.9999015   24.99998657]  f(x): 9.882738812178484e-09  grad at x: [ 1.97000748e-04 -2.68637384e-05]  gradient norm: 0.00019882393027177068\n",
            "iter: 6957  x: [-99.9999017   24.99998659]  f(x): 9.843247388613222e-09  grad at x: [ 1.96606747e-04 -2.68100109e-05]  gradient norm: 0.00019842628241856694\n",
            "iter: 6958  x: [-99.99990189  24.99998662]  f(x): 9.803913771693316e-09  grad at x: [ 1.96213533e-04 -2.67563909e-05]  gradient norm: 0.0001980294298501444\n",
            "iter: 6959  x: [-99.99990209  24.99998665]  f(x): 9.76473733200846e-09  grad at x: [ 1.95821106e-04 -2.67028781e-05]  gradient norm: 0.0001976333709878821\n",
            "iter: 6960  x: [-99.99990229  24.99998668]  f(x): 9.72571744253437e-09  grad at x: [ 1.95429464e-04 -2.66494723e-05]  gradient norm: 0.00019723810425507918\n",
            "iter: 6961  x: [-99.99990248  24.9999867 ]  f(x): 9.686853475475009e-09  grad at x: [ 1.95038605e-04 -2.65961734e-05]  gradient norm: 0.00019684362804495357\n",
            "iter: 6962  x: [-99.99990268  24.99998673]  f(x): 9.648144808195397e-09  grad at x: [ 1.94648528e-04 -2.65429811e-05]  gradient norm: 0.00019644994078080449\n",
            "iter: 6963  x: [-99.99990287  24.99998676]  f(x): 9.609590822809272e-09  grad at x: [ 1.94259231e-04 -2.64898951e-05]  gradient norm: 0.000196057040912172\n",
            "iter: 6964  x: [-99.99990306  24.99998678]  f(x): 9.571190898163722e-09  grad at x: [ 1.93870712e-04 -2.64369153e-05]  gradient norm: 0.00019566492683323417\n",
            "iter: 6965  x: [-99.99990326  24.99998681]  f(x): 9.532944418135916e-09  grad at x: [ 1.93482971e-04 -2.63840415e-05]  gradient norm: 0.00019527359696729012\n",
            "iter: 6966  x: [-99.99990345  24.99998683]  f(x): 9.494850771411277e-09  grad at x: [ 1.93096005e-04 -2.63312734e-05]  gradient norm: 0.00019488304976484\n",
            "iter: 6967  x: [-99.99990365  24.99998686]  f(x): 9.456909348712239e-09  grad at x: [ 1.92709813e-04 -2.62786108e-05]  gradient norm: 0.000194493283675424\n",
            "iter: 6968  x: [-99.99990384  24.99998689]  f(x): 9.41911953769961e-09  grad at x: [ 1.92324393e-04 -2.62260536e-05]  gradient norm: 0.00019410429709514017\n",
            "iter: 6969  x: [-99.99990403  24.99998691]  f(x): 9.381480736098624e-09  grad at x: [ 1.91939745e-04 -2.61736015e-05]  gradient norm: 0.00019371608850168975\n",
            "iter: 6970  x: [-99.99990422  24.99998694]  f(x): 9.34399233846679e-09  grad at x: [ 1.91555865e-04 -2.61212543e-05]  gradient norm: 0.0001933286563183719\n",
            "iter: 6971  x: [-99.99990441  24.99998697]  f(x): 9.306653744205514e-09  grad at x: [ 1.91172753e-04 -2.60690118e-05]  gradient norm: 0.0001929419989966468\n",
            "iter: 6972  x: [-99.9999046   24.99998699]  f(x): 9.269464354723102e-09  grad at x: [ 1.90790408e-04 -2.60168738e-05]  gradient norm: 0.00019255611498701465\n",
            "iter: 6973  x: [-99.9999048   24.99998702]  f(x): 9.232423576227804e-09  grad at x: [ 1.90408827e-04 -2.59648400e-05]  gradient norm: 0.00019217100276813673\n",
            "iter: 6974  x: [-99.99990499  24.99998704]  f(x): 9.195530811685448e-09  grad at x: [ 1.90028009e-04 -2.59129104e-05]  gradient norm: 0.0001917866607633122\n",
            "iter: 6975  x: [-99.99990518  24.99998707]  f(x): 9.15878547155502e-09  grad at x: [ 1.89647953e-04 -2.58610845e-05]  gradient norm: 0.00019140308745216228\n",
            "iter: 6976  x: [-99.99990537  24.9999871 ]  f(x): 9.122186965659703e-09  grad at x: [ 1.89268657e-04 -2.58093624e-05]  gradient norm: 0.00019102028128614723\n",
            "iter: 6977  x: [-99.99990555  24.99998712]  f(x): 9.085734705800708e-09  grad at x: [ 1.88890120e-04 -2.57577436e-05]  gradient norm: 0.00019063824071576728\n",
            "iter: 6978  x: [-99.99990574  24.99998715]  f(x): 9.04942810870515e-09  grad at x: [ 1.88512340e-04 -2.57062282e-05]  gradient norm: 0.00019025696422160374\n",
            "iter: 6979  x: [-99.99990593  24.99998717]  f(x): 9.013266592953382e-09  grad at x: [ 1.88135315e-04 -2.56548157e-05]  gradient norm: 0.00018987645028231787\n",
            "iter: 6980  x: [-99.99990612  24.9999872 ]  f(x): 8.977249579247891e-09  grad at x: [ 1.87759045e-04 -2.56035061e-05]  gradient norm: 0.000189496697377531\n",
            "iter: 6981  x: [-99.99990631  24.99998722]  f(x): 8.941376490406599e-09  grad at x: [ 1.87383526e-04 -2.55522990e-05]  gradient norm: 0.00018911770398782445\n",
            "iter: 6982  x: [-99.9999065   24.99998725]  f(x): 8.90564675108438e-09  grad at x: [ 1.87008759e-04 -2.55011945e-05]  gradient norm: 0.0001887394685918595\n",
            "iter: 6983  x: [-99.99990668  24.99998727]  f(x): 8.870059785478543e-09  grad at x: [ 1.86634742e-04 -2.54501921e-05]  gradient norm: 0.00018836198964205644\n",
            "iter: 6984  x: [-99.99990687  24.9999873 ]  f(x): 8.834615027664803e-09  grad at x: [ 1.86261472e-04 -2.53992917e-05]  gradient norm: 0.00018798526567435868\n",
            "iter: 6985  x: [-99.99990706  24.99998733]  f(x): 8.799311905753152e-09  grad at x: [ 1.85888949e-04 -2.53484931e-05]  gradient norm: 0.00018760929514022648\n",
            "iter: 6986  x: [-99.99990724  24.99998735]  f(x): 8.764149855143367e-09  grad at x: [ 1.85517172e-04 -2.52977961e-05]  gradient norm: 0.00018723407654744226\n",
            "iter: 6987  x: [-99.99990743  24.99998738]  f(x): 8.729128313293981e-09  grad at x: [ 1.85146137e-04 -2.52472005e-05]  gradient norm: 0.00018685960840474842\n",
            "iter: 6988  x: [-99.99990761  24.9999874 ]  f(x): 8.694246717000448e-09  grad at x: [ 1.84775845e-04 -2.51967061e-05]  gradient norm: 0.0001864858891927263\n",
            "iter: 6989  x: [-99.9999078   24.99998743]  f(x): 8.659504507567611e-09  grad at x: [ 1.84406293e-04 -2.51463127e-05]  gradient norm: 0.00018611291741915832\n",
            "iter: 6990  x: [-99.99990798  24.99998745]  f(x): 8.624901128252535e-09  grad at x: [ 1.84037481e-04 -2.50960201e-05]  gradient norm: 0.00018574069159182685\n",
            "iter: 6991  x: [-99.99990817  24.99998748]  f(x): 8.590436024347919e-09  grad at x: [ 1.83669406e-04 -2.50458280e-05]  gradient norm: 0.00018536921021947435\n",
            "iter: 6992  x: [-99.99990835  24.9999875 ]  f(x): 8.556108643086838e-09  grad at x: [ 1.83302067e-04 -2.49957364e-05]  gradient norm: 0.0001849984718108432\n",
            "iter: 6993  x: [-99.99990853  24.99998753]  f(x): 8.521918433637206e-09  grad at x: [ 1.82935463e-04 -2.49457449e-05]  gradient norm: 0.00018462847487467587\n",
            "iter: 6994  x: [-99.99990872  24.99998755]  f(x): 8.487864847007792e-09  grad at x: [ 1.82569592e-04 -2.48958534e-05]  gradient norm: 0.0001842592179187548\n",
            "iter: 6995  x: [-99.9999089   24.99998758]  f(x): 8.453947338809391e-09  grad at x: [ 1.82204453e-04 -2.48460617e-05]  gradient norm: 0.00018389069947998338\n",
            "iter: 6996  x: [-99.99990908  24.9999876 ]  f(x): 8.420165363966257e-09  grad at x: [ 1.81840044e-04 -2.47963696e-05]  gradient norm: 0.00018352291806710417\n",
            "iter: 6997  x: [-99.99990926  24.99998763]  f(x): 8.386518381894397e-09  grad at x: [ 1.81476364e-04 -2.47467768e-05]  gradient norm: 0.00018315587221702062\n",
            "iter: 6998  x: [-99.99990944  24.99998765]  f(x): 8.353005853984262e-09  grad at x: [ 1.81113411e-04 -2.46972833e-05]  gradient norm: 0.0001827895604675963\n",
            "iter: 6999  x: [-99.99990962  24.99998768]  f(x): 8.319627243331798e-09  grad at x: [ 1.80751184e-04 -2.46478887e-05]  gradient norm: 0.0001824239813547747\n",
            "iter: 7000  x: [-99.99990981  24.9999877 ]  f(x): 8.286382012433515e-09  grad at x: [ 1.80389682e-04 -2.45985929e-05]  gradient norm: 0.00018205913338729826\n",
            "iter: 7001  x: [-99.99990999  24.99998773]  f(x): 8.253269630880986e-09  grad at x: [ 1.80028902e-04 -2.45493958e-05]  gradient norm: 0.0001816950151311916\n",
            "iter: 7002  x: [-99.99991017  24.99998775]  f(x): 8.22028956482924e-09  grad at x: [ 1.79668845e-04 -2.45002970e-05]  gradient norm: 0.0001813316250942371\n",
            "iter: 7003  x: [-99.99991035  24.99998777]  f(x): 8.187441287584385e-09  grad at x: [ 1.79309507e-04 -2.44512964e-05]  gradient norm: 0.00018096896184245943\n",
            "iter: 7004  x: [-99.99991052  24.9999878 ]  f(x): 8.154724271742953e-09  grad at x: [ 1.78950888e-04 -2.44023938e-05]  gradient norm: 0.00018060702391372219\n",
            "iter: 7005  x: [-99.9999107   24.99998782]  f(x): 8.122137994201518e-09  grad at x: [ 1.78592986e-04 -2.43535890e-05]  gradient norm: 0.0001802458098730899\n",
            "iter: 7006  x: [-99.99991088  24.99998785]  f(x): 8.089681931147467e-09  grad at x: [ 1.78235800e-04 -2.43048818e-05]  gradient norm: 0.0001798853182574661\n",
            "iter: 7007  x: [-99.99991106  24.99998787]  f(x): 8.05735556322096e-09  grad at x: [ 1.77879329e-04 -2.42562720e-05]  gradient norm: 0.00017952554763287548\n",
            "iter: 7008  x: [-99.99991124  24.9999879 ]  f(x): 8.025158370265969e-09  grad at x: [ 1.77523570e-04 -2.42077595e-05]  gradient norm: 0.0001791664965362215\n",
            "iter: 7009  x: [-99.99991142  24.99998792]  f(x): 7.9930898365583e-09  grad at x: [ 1.77168523e-04 -2.41593440e-05]  gradient norm: 0.00017880816353352887\n",
            "iter: 7010  x: [-99.99991159  24.99998794]  f(x): 7.961149450688643e-09  grad at x: [ 1.76814186e-04 -2.41110253e-05]  gradient norm: 0.00017845054721898326\n",
            "iter: 7011  x: [-99.99991177  24.99998797]  f(x): 7.92933669800933e-09  grad at x: [ 1.76460557e-04 -2.40628032e-05]  gradient norm: 0.00017809364613044824\n",
            "iter: 7012  x: [-99.99991195  24.99998799]  f(x): 7.897651068102063e-09  grad at x: [ 1.76107636e-04 -2.40146776e-05]  gradient norm: 0.00017773745883298842\n",
            "iter: 7013  x: [-99.99991212  24.99998802]  f(x): 7.866092054918809e-09  grad at x: [ 1.75755421e-04 -2.39666483e-05]  gradient norm: 0.00017738198392078954\n",
            "iter: 7014  x: [-99.9999123   24.99998804]  f(x): 7.8346591516806e-09  grad at x: [ 1.7540391e-04 -2.3918715e-05]  gradient norm: 0.00017702721995987622\n",
            "iter: 7015  x: [-99.99991247  24.99998806]  f(x): 7.80335185338511e-09  grad at x: [ 1.75053102e-04 -2.38708775e-05]  gradient norm: 0.00017667316551627313\n",
            "iter: 7016  x: [-99.99991265  24.99998809]  f(x): 7.772169659284246e-09  grad at x: [ 1.74702996e-04 -2.38231358e-05]  gradient norm: 0.00017631981918416598\n",
            "iter: 7017  x: [-99.99991282  24.99998811]  f(x): 7.741112070376522e-09  grad at x: [ 1.74353590e-04 -2.37754895e-05]  gradient norm: 0.00017596717955774052\n",
            "iter: 7018  x: [-99.999913    24.99998814]  f(x): 7.710178587013627e-09  grad at x: [ 1.74004883e-04 -2.37279385e-05]  gradient norm: 0.0001756152452039814\n",
            "iter: 7019  x: [-99.99991317  24.99998816]  f(x): 7.679368713602585e-09  grad at x: [ 1.73656873e-04 -2.36804827e-05]  gradient norm: 0.00017526401471611432\n",
            "iter: 7020  x: [-99.99991335  24.99998818]  f(x): 7.648681956450677e-09  grad at x: [ 1.73309559e-04 -2.36331217e-05]  gradient norm: 0.00017491348668928508\n",
            "iter: 7021  x: [-99.99991352  24.99998821]  f(x): 7.61811782350744e-09  grad at x: [ 1.72962940e-04 -2.35858555e-05]  gradient norm: 0.00017456365971767939\n",
            "iter: 7022  x: [-99.99991369  24.99998823]  f(x): 7.587675824444195e-09  grad at x: [ 1.72617014e-04 -2.35386837e-05]  gradient norm: 0.00017421453239548296\n",
            "iter: 7023  x: [-99.99991386  24.99998825]  f(x): 7.55735547073259e-09  grad at x: [ 1.72271780e-04 -2.34916064e-05]  gradient norm: 0.00017386610331784157\n",
            "iter: 7024  x: [-99.99991404  24.99998828]  f(x): 7.527156277915496e-09  grad at x: [ 1.71927237e-04 -2.34446232e-05]  gradient norm: 0.00017351837110710203\n",
            "iter: 7025  x: [-99.99991421  24.9999883 ]  f(x): 7.497077760784946e-09  grad at x: [ 1.71583382e-04 -2.33977339e-05]  gradient norm: 0.00017317133435745013\n",
            "iter: 7026  x: [-99.99991438  24.99998832]  f(x): 7.467119438351614e-09  grad at x: [ 1.71240216e-04 -2.33509385e-05]  gradient norm: 0.00017282499169219264\n",
            "iter: 7027  x: [-99.99991455  24.99998835]  f(x): 7.437280828874772e-09  grad at x: [ 1.70897735e-04 -2.33042366e-05]  gradient norm: 0.00017247934170647536\n",
            "iter: 7028  x: [-99.99991472  24.99998837]  f(x): 7.407561454729301e-09  grad at x: [ 1.70555940e-04 -2.32576281e-05]  gradient norm: 0.00017213438302360516\n",
            "iter: 7029  x: [-99.99991489  24.99998839]  f(x): 7.37796083987522e-09  grad at x: [ 1.70214828e-04 -2.32111129e-05]  gradient norm: 0.0001717901142659288\n",
            "iter: 7030  x: [-99.99991506  24.99998842]  f(x): 7.3484785076043005e-09  grad at x: [ 1.69874398e-04 -2.31646906e-05]  gradient norm: 0.00017144653402859214\n",
            "iter: 7031  x: [-99.99991523  24.99998844]  f(x): 7.319113987622064e-09  grad at x: [ 1.69534649e-04 -2.31183612e-05]  gradient norm: 0.000171103640962103\n",
            "iter: 7032  x: [-99.9999154   24.99998846]  f(x): 7.289866809028253e-09  grad at x: [ 1.69195580e-04 -2.30721245e-05]  gradient norm: 0.00017076143369072834\n",
            "iter: 7033  x: [-99.99991557  24.99998849]  f(x): 7.260736502406986e-09  grad at x: [ 1.68857189e-04 -2.30259803e-05]  gradient norm: 0.0001704199108368149\n",
            "iter: 7034  x: [-99.99991574  24.99998851]  f(x): 7.231722600150255e-09  grad at x: [ 1.68519475e-04 -2.29799283e-05]  gradient norm: 0.00017007907102462965\n",
            "iter: 7035  x: [-99.99991591  24.99998853]  f(x): 7.202824636125658e-09  grad at x: [ 1.68182436e-04 -2.29339685e-05]  gradient norm: 0.00016973891287651937\n",
            "iter: 7036  x: [-99.99991608  24.99998856]  f(x): 7.174042148383842e-09  grad at x: [ 1.67846071e-04 -2.28881005e-05]  gradient norm: 0.00016939943504491202\n",
            "iter: 7037  x: [-99.99991624  24.99998858]  f(x): 7.145374676423252e-09  grad at x: [ 1.67510379e-04 -2.28423243e-05]  gradient norm: 0.00016906063618031552\n",
            "iter: 7038  x: [-99.99991641  24.9999886 ]  f(x): 7.116821759135438e-09  grad at x: [ 1.67175358e-04 -2.27966397e-05]  gradient norm: 0.0001687225149069968\n",
            "iter: 7039  x: [-99.99991658  24.99998862]  f(x): 7.088382939322024e-09  grad at x: [ 1.66841007e-04 -2.27510464e-05]  gradient norm: 0.00016838506987642372\n",
            "iter: 7040  x: [-99.99991675  24.99998865]  f(x): 7.060057761462288e-09  grad at x: [ 1.66507325e-04 -2.27055443e-05]  gradient norm: 0.0001680482997410243\n",
            "iter: 7041  x: [-99.99991691  24.99998867]  f(x): 7.0318457714662734e-09  grad at x: [ 1.66174310e-04 -2.26601332e-05]  gradient norm: 0.00016771220315130647\n",
            "iter: 7042  x: [-99.99991708  24.99998869]  f(x): 7.003746514716827e-09  grad at x: [ 1.65841962e-04 -2.26148129e-05]  gradient norm: 0.00016737677873249714\n",
            "iter: 7043  x: [-99.99991724  24.99998872]  f(x): 6.975759542661239e-09  grad at x: [ 1.65510278e-04 -2.25695833e-05]  gradient norm: 0.0001670420251632653\n",
            "iter: 7044  x: [-99.99991741  24.99998874]  f(x): 6.9478844084674335e-09  grad at x: [ 1.65179257e-04 -2.25244441e-05]  gradient norm: 0.0001667079411242\n",
            "iter: 7045  x: [-99.99991758  24.99998876]  f(x): 6.920120662172924e-09  grad at x: [ 1.64848899e-04 -2.24793953e-05]  gradient norm: 0.00016637452523956815\n",
            "iter: 7046  x: [-99.99991774  24.99998878]  f(x): 6.89246785999939e-09  grad at x: [ 1.64519201e-04 -2.24344365e-05]  gradient norm: 0.00016604177618899878\n",
            "iter: 7047  x: [-99.9999179   24.99998881]  f(x): 6.864925557461353e-09  grad at x: [ 1.64190163e-04 -2.23895676e-05]  gradient norm: 0.00016570969262491984\n",
            "iter: 7048  x: [-99.99991807  24.99998883]  f(x): 6.837493313882791e-09  grad at x: [ 1.63861782e-04 -2.23447885e-05]  gradient norm: 0.00016537827322696039\n",
            "iter: 7049  x: [-99.99991823  24.99998885]  f(x): 6.8101706902046505e-09  grad at x: [ 1.63534059e-04 -2.23000989e-05]  gradient norm: 0.0001650475166757095\n",
            "iter: 7050  x: [-99.9999184   24.99998887]  f(x): 6.78295724890061e-09  grad at x: [ 1.63206991e-04 -2.22554987e-05]  gradient norm: 0.0001647174216517562\n",
            "iter: 7051  x: [-99.99991856  24.99998889]  f(x): 6.755852551658039e-09  grad at x: [ 1.62880577e-04 -2.22109877e-05]  gradient norm: 0.00016438798680752848\n",
            "iter: 7052  x: [-99.99991872  24.99998892]  f(x): 6.728856164016873e-09  grad at x: [ 1.62554815e-04 -2.21665657e-05]  gradient norm: 0.00016405921082361544\n",
            "iter: 7053  x: [-99.99991889  24.99998894]  f(x): 6.701967655263471e-09  grad at x: [ 1.62229706e-04 -2.21222326e-05]  gradient norm: 0.00016373109240780715\n",
            "iter: 7054  x: [-99.99991905  24.99998896]  f(x): 6.675186591736879e-09  grad at x: [ 1.61905246e-04 -2.20779881e-05]  gradient norm: 0.00016340363021349164\n",
            "iter: 7055  x: [-99.99991921  24.99998898]  f(x): 6.648512545819044e-09  grad at x: [ 1.61581436e-04 -2.20338321e-05]  gradient norm: 0.00016307682294941907\n",
            "iter: 7056  x: [-99.99991937  24.99998901]  f(x): 6.621945089088515e-09  grad at x: [ 1.61258273e-04 -2.19897645e-05]  gradient norm: 0.0001627506692961785\n",
            "iter: 7057  x: [-99.99991953  24.99998903]  f(x): 6.5954837969132176e-09  grad at x: [ 1.60935756e-04 -2.19457850e-05]  gradient norm: 0.00016242516796252012\n",
            "iter: 7058  x: [-99.99991969  24.99998905]  f(x): 6.569128243858341e-09  grad at x: [ 1.60613885e-04 -2.19018934e-05]  gradient norm: 0.00016210031762903292\n",
            "iter: 7059  x: [-99.99991985  24.99998907]  f(x): 6.542878008260758e-09  grad at x: [ 1.60292657e-04 -2.18580896e-05]  gradient norm: 0.0001617761170044671\n",
            "iter: 7060  x: [-99.99992001  24.99998909]  f(x): 6.5167326677327734e-09  grad at x: [ 1.59972072e-04 -2.18143734e-05]  gradient norm: 0.00016145256477037177\n",
            "iter: 7061  x: [-99.99992017  24.99998911]  f(x): 6.490691803562783e-09  grad at x: [ 1.59652128e-04 -2.17707447e-05]  gradient norm: 0.00016112965963549706\n",
            "iter: 7062  x: [-99.99992033  24.99998914]  f(x): 6.46475499850203e-09  grad at x: [ 1.59332823e-04 -2.17272032e-05]  gradient norm: 0.00016080740030859314\n",
            "iter: 7063  x: [-99.99992049  24.99998916]  f(x): 6.438921836837463e-09  grad at x: [ 1.59014158e-04 -2.16837488e-05]  gradient norm: 0.00016048578549937017\n",
            "iter: 7064  x: [-99.99992065  24.99998918]  f(x): 6.413191904233041e-09  grad at x: [ 1.58696129e-04 -2.16403813e-05]  gradient norm: 0.00016016481391657833\n",
            "iter: 7065  x: [-99.99992081  24.9999892 ]  f(x): 6.387564790130497e-09  grad at x: [ 1.58378737e-04 -2.15971005e-05]  gradient norm: 0.0001598444842980889\n",
            "iter: 7066  x: [-99.99992097  24.99998922]  f(x): 6.362040080906756e-09  grad at x: [ 1.58061980e-04 -2.15539063e-05]  gradient norm: 0.00015952479532545098\n",
            "iter: 7067  x: [-99.99992113  24.99998924]  f(x): 6.336617368805201e-09  grad at x: [ 1.57745856e-04 -2.15107985e-05]  gradient norm: 0.00015920574573557577\n",
            "iter: 7068  x: [-99.99992128  24.99998927]  f(x): 6.311296245251998e-09  grad at x: [ 1.57430364e-04 -2.14677769e-05]  gradient norm: 0.00015888733423721348\n",
            "iter: 7069  x: [-99.99992144  24.99998929]  f(x): 6.286076305492151e-09  grad at x: [ 1.57115503e-04 -2.14248414e-05]  gradient norm: 0.0001585695595691954\n",
            "iter: 7070  x: [-99.9999216   24.99998931]  f(x): 6.260957143801128e-09  grad at x: [ 1.56801272e-04 -2.13819917e-05]  gradient norm: 0.00015825242044027166\n",
            "iter: 7071  x: [-99.99992176  24.99998933]  f(x): 6.2359383581797274e-09  grad at x: [ 1.56487670e-04 -2.13392277e-05]  gradient norm: 0.00015793591558831358\n",
            "iter: 7072  x: [-99.99992191  24.99998935]  f(x): 6.211019548031951e-09  grad at x: [ 1.56174694e-04 -2.12965492e-05]  gradient norm: 0.0001576200437511924\n",
            "iter: 7073  x: [-99.99992207  24.99998937]  f(x): 6.186200314161004e-09  grad at x: [ 1.55862345e-04 -2.12539561e-05]  gradient norm: 0.0001573048036667794\n",
            "iter: 7074  x: [-99.99992222  24.99998939]  f(x): 6.1614802587653035e-09  grad at x: [ 1.55550620e-04 -2.12114482e-05]  gradient norm: 0.0001569901940729459\n",
            "iter: 7075  x: [-99.99992238  24.99998942]  f(x): 6.136858983228411e-09  grad at x: [ 1.55239519e-04 -2.11690253e-05]  gradient norm: 0.00015667621367940204\n",
            "iter: 7076  x: [-99.99992254  24.99998944]  f(x): 6.112336094742139e-09  grad at x: [ 1.54929040e-04 -2.11266873e-05]  gradient norm: 0.0001563628612521802\n",
            "iter: 7077  x: [-99.99992269  24.99998946]  f(x): 6.08791119966665e-09  grad at x: [ 1.54619182e-04 -2.10844339e-05]  gradient norm: 0.00015605013552915165\n",
            "iter: 7078  x: [-99.99992285  24.99998948]  f(x): 6.063583905741438e-09  grad at x: [ 1.54309944e-04 -2.10422650e-05]  gradient norm: 0.0001557380352481877\n",
            "iter: 7079  x: [-99.999923   24.9999895]  f(x): 6.039353824344471e-09  grad at x: [ 1.54001324e-04 -2.10001805e-05]  gradient norm: 0.0001554265591762807\n",
            "iter: 7080  x: [-99.99992315  24.99998952]  f(x): 6.0152205659484786e-09  grad at x: [ 1.53693321e-04 -2.09581801e-05]  gradient norm: 0.00015511570605130195\n",
            "iter: 7081  x: [-99.99992331  24.99998954]  f(x): 5.991183744573488e-09  grad at x: [ 1.53385934e-04 -2.09162638e-05]  gradient norm: 0.00015480547463928384\n",
            "iter: 7082  x: [-99.99992346  24.99998956]  f(x): 5.96724297341033e-09  grad at x: [ 1.53079163e-04 -2.08744312e-05]  gradient norm: 0.00015449586367809762\n",
            "iter: 7083  x: [-99.99992361  24.99998958]  f(x): 5.9433978714256795e-09  grad at x: [ 1.52773004e-04 -2.08326824e-05]  gradient norm: 0.00015418687196289675\n",
            "iter: 7084  x: [-99.99992377  24.9999896 ]  f(x): 5.9196480544994826e-09  grad at x: [ 1.52467458e-04 -2.07910170e-05]  gradient norm: 0.0001538784982315526\n",
            "iter: 7085  x: [-99.99992392  24.99998963]  f(x): 5.89599313993745e-09  grad at x: [ 1.52162523e-04 -2.07494350e-05]  gradient norm: 0.00015357074122289636\n",
            "iter: 7086  x: [-99.99992407  24.99998965]  f(x): 5.872432750635362e-09  grad at x: [ 1.51858198e-04 -2.07079361e-05]  gradient norm: 0.00015326359973112156\n",
            "iter: 7087  x: [-99.99992422  24.99998967]  f(x): 5.8489665087184415e-09  grad at x: [ 1.51554482e-04 -2.06665202e-05]  gradient norm: 0.0001529570725232206\n",
            "iter: 7088  x: [-99.99992437  24.99998969]  f(x): 5.825594037634589e-09  grad at x: [ 1.51251373e-04 -2.06251872e-05]  gradient norm: 0.00015265115836618585\n",
            "iter: 7089  x: [-99.99992453  24.99998971]  f(x): 5.802314964222578e-09  grad at x: [ 1.50948870e-04 -2.05839368e-05]  gradient norm: 0.00015234585605421077\n",
            "iter: 7090  x: [-99.99992468  24.99998973]  f(x): 5.7791289145518885e-09  grad at x: [ 1.50646972e-04 -2.05427690e-05]  gradient norm: 0.0001520411643542878\n",
            "iter: 7091  x: [-99.99992483  24.99998975]  f(x): 5.756035516003395e-09  grad at x: [ 1.50345678e-04 -2.05016834e-05]  gradient norm: 0.0001517370820334093\n",
            "iter: 7092  x: [-99.99992498  24.99998977]  f(x): 5.733034397192919e-09  grad at x: [ 1.50044987e-04 -2.04606800e-05]  gradient norm: 0.00015143360785760762\n",
            "iter: 7093  x: [-99.99992513  24.99998979]  f(x): 5.7101251924418495e-09  grad at x: [ 1.49744897e-04 -2.04197587e-05]  gradient norm: 0.0001511307406511574\n",
            "iter: 7094  x: [-99.99992528  24.99998981]  f(x): 5.687307532944785e-09  grad at x: [ 1.49445407e-04 -2.03789192e-05]  gradient norm: 0.000150828479180091\n",
            "iter: 7095  x: [-99.99992543  24.99998983]  f(x): 5.664581051265667e-09  grad at x: [ 1.49146517e-04 -2.03381613e-05]  gradient norm: 0.0001505268222114008\n",
            "iter: 7096  x: [-99.99992558  24.99998985]  f(x): 5.641945385563837e-09  grad at x: [ 1.48848224e-04 -2.02974850e-05]  gradient norm: 0.00015022576856936145\n",
            "iter: 7097  x: [-99.99992572  24.99998987]  f(x): 5.619400170887273e-09  grad at x: [ 1.48550527e-04 -2.02568900e-05]  gradient norm: 0.0001499253170200053\n",
            "iter: 7098  x: [-99.99992587  24.99998989]  f(x): 5.596945047855402e-09  grad at x: [ 1.48253426e-04 -2.02163763e-05]  gradient norm: 0.00014962546638664693\n",
            "iter: 7099  x: [-99.99992602  24.99998991]  f(x): 5.574579656304754e-09  grad at x: [ 1.47956919e-04 -2.01759435e-05]  gradient norm: 0.00014932621546539983\n",
            "iter: 7100  x: [-99.99992617  24.99998993]  f(x): 5.5523036351626685e-09  grad at x: [ 1.47661005e-04 -2.01355916e-05]  gradient norm: 0.00014902756302325644\n",
            "iter: 7101  x: [-99.99992632  24.99998995]  f(x): 5.530116628819352e-09  grad at x: [ 1.47365683e-04 -2.00953204e-05]  gradient norm: 0.00014872950788353132\n",
            "iter: 7102  x: [-99.99992646  24.99998997]  f(x): 5.508018282902307e-09  grad at x: [ 1.47070952e-04 -2.00551298e-05]  gradient norm: 0.00014843204886953905\n",
            "iter: 7103  x: [-99.99992661  24.99998999]  f(x): 5.486008242258102e-09  grad at x: [ 1.46776810e-04 -2.00150195e-05]  gradient norm: 0.00014813518477739315\n",
            "iter: 7104  x: [-99.99992676  24.99999001]  f(x): 5.464086152908736e-09  grad at x: [ 1.46483256e-04 -1.99749895e-05]  gradient norm: 0.0001478389144022471\n",
            "iter: 7105  x: [-99.9999269   24.99999003]  f(x): 5.442251664197083e-09  grad at x: [ 1.46190290e-04 -1.99350395e-05]  gradient norm: 0.00014754323656741548\n",
            "iter: 7106  x: [-99.99992705  24.99999005]  f(x): 5.420504426759893e-09  grad at x: [ 1.45897909e-04 -1.98951694e-05]  gradient norm: 0.00014724815009717294\n",
            "iter: 7107  x: [-99.9999272   24.99999007]  f(x): 5.3988440903838715e-09  grad at x: [ 1.45606113e-04 -1.98553791e-05]  gradient norm: 0.00014695365378763293\n",
            "iter: 7108  x: [-99.99992734  24.99999009]  f(x): 5.377270310147654e-09  grad at x: [ 1.45314901e-04 -1.98156684e-05]  gradient norm: 0.0001466597464902712\n",
            "iter: 7109  x: [-99.99992749  24.99999011]  f(x): 5.355782738204232e-09  grad at x: [ 1.45024271e-04 -1.97760370e-05]  gradient norm: 0.00014636642700024117\n",
            "iter: 7110  x: [-99.99992763  24.99999013]  f(x): 5.33438103005931e-09  grad at x: [ 1.44734223e-04 -1.97364849e-05]  gradient norm: 0.00014607369414181747\n",
            "iter: 7111  x: [-99.99992778  24.99999015]  f(x): 5.3130648424238356e-09  grad at x: [ 1.44444754e-04 -1.96970120e-05]  gradient norm: 0.00014578154673927472\n",
            "iter: 7112  x: [-99.99992792  24.99999017]  f(x): 5.29183383525913e-09  grad at x: [ 1.44155865e-04 -1.96576179e-05]  gradient norm: 0.00014548998364504864\n",
            "iter: 7113  x: [-99.99992807  24.99999019]  f(x): 5.270687667594305e-09  grad at x: [ 1.43867553e-04 -1.96183027e-05]  gradient norm: 0.00014519900368245375\n",
            "iter: 7114  x: [-99.99992821  24.99999021]  f(x): 5.249625999793127e-09  grad at x: [ 1.43579818e-04 -1.95790661e-05]  gradient norm: 0.00014490860567672477\n",
            "iter: 7115  x: [-99.99992835  24.99999023]  f(x): 5.228648495307825e-09  grad at x: [ 1.43292658e-04 -1.95399080e-05]  gradient norm: 0.0001446187884793373\n",
            "iter: 7116  x: [-99.9999285   24.99999025]  f(x): 5.207754816800072e-09  grad at x: [ 1.43006073e-04 -1.95008282e-05]  gradient norm: 0.00014432955091456595\n",
            "iter: 7117  x: [-99.99992864  24.99999027]  f(x): 5.186944628116312e-09  grad at x: [ 1.42720061e-04 -1.94618265e-05]  gradient norm: 0.0001440408918066854\n",
            "iter: 7118  x: [-99.99992878  24.99999029]  f(x): 5.1662175983325904e-09  grad at x: [ 1.42434621e-04 -1.94229028e-05]  gradient norm: 0.00014375281003629238\n",
            "iter: 7119  x: [-99.99992893  24.99999031]  f(x): 5.145573393630398e-09  grad at x: [ 1.42149752e-04 -1.93840570e-05]  gradient norm: 0.0001434653044276615\n",
            "iter: 7120  x: [-99.99992907  24.99999033]  f(x): 5.125011681365904e-09  grad at x: [ 1.41865452e-04 -1.93452889e-05]  gradient norm: 0.00014317837380506743\n",
            "iter: 7121  x: [-99.99992921  24.99999035]  f(x): 5.104532134090563e-09  grad at x: [ 1.41581721e-04 -1.93065983e-05]  gradient norm: 0.00014289201704910688\n",
            "iter: 7122  x: [-99.99992935  24.99999037]  f(x): 5.0841344235520365e-09  grad at x: [ 1.41298558e-04 -1.92679852e-05]  gradient norm: 0.00014260623301317563\n",
            "iter: 7123  x: [-99.99992949  24.99999039]  f(x): 5.0638182225093834e-09  grad at x: [ 1.41015961e-04 -1.92294492e-05]  gradient norm: 0.00014232102054874936\n",
            "iter: 7124  x: [-99.99992963  24.9999904 ]  f(x): 5.043583205003869e-09  grad at x: [ 1.40733929e-04 -1.91909903e-05]  gradient norm: 0.00014203637850922375\n",
            "iter: 7125  x: [-99.99992977  24.99999042]  f(x): 5.0234290461501906e-09  grad at x: [ 1.40452461e-04 -1.91526083e-05]  gradient norm: 0.00014175230574703455\n",
            "iter: 7126  x: [-99.99992991  24.99999044]  f(x): 5.00335542426166e-09  grad at x: [ 1.40171556e-04 -1.91143031e-05]  gradient norm: 0.00014146880114373854\n",
            "iter: 7127  x: [-99.99993005  24.99999046]  f(x): 4.983362016714857e-09  grad at x: [ 1.39891213e-04 -1.90760745e-05]  gradient norm: 0.00014118586355177146\n",
            "iter: 7128  x: [-99.99993019  24.99999048]  f(x): 4.963448502018644e-09  grad at x: [ 1.39611430e-04 -1.90379223e-05]  gradient norm: 0.00014090349182356902\n",
            "iter: 7129  x: [-99.99993033  24.9999905 ]  f(x): 4.943614561858443e-09  grad at x: [ 1.39332208e-04 -1.89998465e-05]  gradient norm: 0.00014062168484068796\n",
            "iter: 7130  x: [-99.99993047  24.99999052]  f(x): 4.923859879029088e-09  grad at x: [ 1.39053543e-04 -1.89618468e-05]  gradient norm: 0.0001403404414846852\n",
            "iter: 7131  x: [-99.99993061  24.99999054]  f(x): 4.904184135392315e-09  grad at x: [ 1.38775436e-04 -1.89239231e-05]  gradient norm: 0.0001400597606079964\n",
            "iter: 7132  x: [-99.99993075  24.99999056]  f(x): 4.884587015897453e-09  grad at x: [ 1.38497885e-04 -1.88860752e-05]  gradient norm: 0.00013977964109121834\n",
            "iter: 7133  x: [-99.99993089  24.99999058]  f(x): 4.865068206661246e-09  grad at x: [ 1.38220890e-04 -1.88483031e-05]  gradient norm: 0.00013950008181590784\n",
            "iter: 7134  x: [-99.99993103  24.99999059]  f(x): 4.845627394897209e-09  grad at x: [ 1.37944448e-04 -1.88106065e-05]  gradient norm: 0.00013922108166362175\n",
            "iter: 7135  x: [-99.99993117  24.99999061]  f(x): 4.826264266956118e-09  grad at x: [ 1.37668559e-04 -1.87729853e-05]  gradient norm: 0.00013894263948775577\n",
            "iter: 7136  x: [-99.9999313   24.99999063]  f(x): 4.806978514133309e-09  grad at x: [ 1.37393222e-04 -1.87354393e-05]  gradient norm: 0.00013866475419706782\n",
            "iter: 7137  x: [-99.99993144  24.99999065]  f(x): 4.787769828929246e-09  grad at x: [ 1.37118435e-04 -1.86979684e-05]  gradient norm: 0.0001383874247022358\n",
            "iter: 7138  x: [-99.99993158  24.99999067]  f(x): 4.768637900890627e-09  grad at x: [ 1.36844198e-04 -1.86605725e-05]  gradient norm: 0.00013811064985569545\n",
            "iter: 7139  x: [-99.99993171  24.99999069]  f(x): 4.7495824246092035e-09  grad at x: [ 1.36570510e-04 -1.86232513e-05]  gradient norm: 0.00013783442856716465\n",
            "iter: 7140  x: [-99.99993185  24.99999071]  f(x): 4.730603093802387e-09  grad at x: [ 1.36297369e-04 -1.85860048e-05]  gradient norm: 0.00013755875971820024\n",
            "iter: 7141  x: [-99.99993199  24.99999073]  f(x): 4.711699603262631e-09  grad at x: [ 1.36024774e-04 -1.85488328e-05]  gradient norm: 0.00013728364219035903\n",
            "iter: 7142  x: [-99.99993212  24.99999074]  f(x): 4.69287165078351e-09  grad at x: [ 1.35752725e-04 -1.85117352e-05]  gradient norm: 0.00013700907489335896\n",
            "iter: 7143  x: [-99.99993226  24.99999076]  f(x): 4.674118935212038e-09  grad at x: [ 1.35481219e-04 -1.84747117e-05]  gradient norm: 0.0001367350567369179\n",
            "iter: 7144  x: [-99.99993239  24.99999078]  f(x): 4.655441156445679e-09  grad at x: [ 1.35210257e-04 -1.84377623e-05]  gradient norm: 0.0001364615866307538\n",
            "iter: 7145  x: [-99.99993253  24.9999908 ]  f(x): 4.636838013511741e-09  grad at x: [ 1.34939836e-04 -1.84008868e-05]  gradient norm: 0.00013618866345642343\n",
            "iter: 7146  x: [-99.99993267  24.99999082]  f(x): 4.618309208476359e-09  grad at x: [ 1.34669957e-04 -1.83640850e-05]  gradient norm: 0.00013591628612460478\n",
            "iter: 7147  x: [-99.9999328   24.99999084]  f(x): 4.5998544443814855e-09  grad at x: [ 1.34400617e-04 -1.83273568e-05]  gradient norm: 0.00013564445354501578\n",
            "iter: 7148  x: [-99.99993293  24.99999085]  f(x): 4.5814734253075455e-09  grad at x: [ 1.34131815e-04 -1.82907021e-05]  gradient norm: 0.00013537316462737428\n",
            "iter: 7149  x: [-99.99993307  24.99999087]  f(x): 4.56316585827279e-09  grad at x: [ 1.33863552e-04 -1.82541207e-05]  gradient norm: 0.00013510241830955936\n",
            "iter: 7150  x: [-99.9999332   24.99999089]  f(x): 4.544931447580495e-09  grad at x: [ 1.33595825e-04 -1.82176125e-05]  gradient norm: 0.00013483221347408778\n",
            "iter: 7151  x: [-99.99993334  24.99999091]  f(x): 4.5267699023030884e-09  grad at x: [ 1.33328633e-04 -1.81811772e-05]  gradient norm: 0.00013456254905883864\n",
            "iter: 7152  x: [-99.99993347  24.99999093]  f(x): 4.5086809306980184e-09  grad at x: [ 1.33061976e-04 -1.81448149e-05]  gradient norm: 0.0001342934239744898\n",
            "iter: 7153  x: [-99.9999336   24.99999095]  f(x): 4.4906642419816346e-09  grad at x: [ 1.32795852e-04 -1.81085253e-05]  gradient norm: 0.00013402483713075922\n",
            "iter: 7154  x: [-99.99993373  24.99999096]  f(x): 4.472719548274466e-09  grad at x: [ 1.32530260e-04 -1.80723082e-05]  gradient norm: 0.0001337567874655259\n",
            "iter: 7155  x: [-99.99993387  24.99999098]  f(x): 4.454846560884334e-09  grad at x: [ 1.32265200e-04 -1.80361636e-05]  gradient norm: 0.00013348927388946776\n",
            "iter: 7156  x: [-99.999934  24.999991]  f(x): 4.437044994009391e-09  grad at x: [ 1.32000669e-04 -1.80000913e-05]  gradient norm: 0.00013322229534142385\n",
            "iter: 7157  x: [-99.99993413  24.99999102]  f(x): 4.419314562844352e-09  grad at x: [ 1.31736668e-04 -1.79640911e-05]  gradient norm: 0.0001329558507602332\n",
            "iter: 7158  x: [-99.99993426  24.99999104]  f(x): 4.401654981645623e-09  grad at x: [ 1.31473195e-04 -1.79281629e-05]  gradient norm: 0.00013268993905561375\n",
            "iter: 7159  x: [-99.99993439  24.99999105]  f(x): 4.384065967604141e-09  grad at x: [ 1.31210248e-04 -1.78923066e-05]  gradient norm: 0.0001324245591664045\n",
            "iter: 7160  x: [-99.99993453  24.99999107]  f(x): 4.366547240696313e-09  grad at x: [ 1.30947828e-04 -1.78565219e-05]  gradient norm: 0.00013215971005864553\n",
            "iter: 7161  x: [-99.99993466  24.99999109]  f(x): 4.349098518281682e-09  grad at x: [ 1.30685932e-04 -1.78208089e-05]  gradient norm: 0.00013189539064397486\n",
            "iter: 7162  x: [-99.99993479  24.99999111]  f(x): 4.331719520443653e-09  grad at x: [ 1.30424560e-04 -1.77851673e-05]  gradient norm: 0.00013163159986027145\n",
            "iter: 7163  x: [-99.99993492  24.99999113]  f(x): 4.314409968308786e-09  grad at x: [ 1.30163711e-04 -1.77495969e-05]  gradient norm: 0.00013136833664637436\n",
            "iter: 7164  x: [-99.99993505  24.99999114]  f(x): 4.297169585889394e-09  grad at x: [ 1.29903384e-04 -1.77140978e-05]  gradient norm: 0.00013110559997024375\n",
            "iter: 7165  x: [-99.99993518  24.99999116]  f(x): 4.279998096188547e-09  grad at x: [ 1.29643577e-04 -1.76786696e-05]  gradient norm: 0.00013084338876975858\n",
            "iter: 7166  x: [-99.99993531  24.99999118]  f(x): 4.262895223243743e-09  grad at x: [ 1.29384290e-04 -1.76433122e-05]  gradient norm: 0.00013058170198375794\n",
            "iter: 7167  x: [-99.99993544  24.9999912 ]  f(x): 4.2458606938959215e-09  grad at x: [ 1.29125521e-04 -1.76080256e-05]  gradient norm: 0.00013032053857924193\n",
            "iter: 7168  x: [-99.99993557  24.99999121]  f(x): 4.2288942341057e-09  grad at x: [ 1.28867270e-04 -1.75728095e-05]  gradient norm: 0.00013005989749504957\n",
            "iter: 7169  x: [-99.9999357   24.99999123]  f(x): 4.211995572686608e-09  grad at x: [ 1.28609536e-04 -1.75376639e-05]  gradient norm: 0.00012979977769914103\n",
            "iter: 7170  x: [-99.99993582  24.99999125]  f(x): 4.195164437448868e-09  grad at x: [ 1.28352316e-04 -1.75025886e-05]  gradient norm: 0.00012954017812939533\n",
            "iter: 7171  x: [-99.99993595  24.99999127]  f(x): 4.178400560925474e-09  grad at x: [ 1.28095612e-04 -1.74675834e-05]  gradient norm: 0.00012928109778193367\n",
            "iter: 7172  x: [-99.99993608  24.99999128]  f(x): 4.1617036728791525e-09  grad at x: [ 1.27839421e-04 -1.74326482e-05]  gradient norm: 0.00012902253559559512\n",
            "iter: 7173  x: [-99.99993621  24.9999913 ]  f(x): 4.145073505899552e-09  grad at x: [ 1.27583742e-04 -1.73977830e-05]  gradient norm: 0.00012876449053833984\n",
            "iter: 7174  x: [-99.99993634  24.99999132]  f(x): 4.1285097915777944e-09  grad at x: [ 1.27328574e-04 -1.73629874e-05]  gradient norm: 0.0001285069615480468\n",
            "iter: 7175  x: [-99.99993646  24.99999134]  f(x): 4.112012266248438e-09  grad at x: [ 1.27073917e-04 -1.73282614e-05]  gradient norm: 0.00012824994762179731\n",
            "iter: 7176  x: [-99.99993659  24.99999135]  f(x): 4.095580665173486e-09  grad at x: [ 1.26819769e-04 -1.72936049e-05]  gradient norm: 0.00012799344772563144\n",
            "iter: 7177  x: [-99.99993672  24.99999137]  f(x): 4.079214724664871e-09  grad at x: [ 1.26566130e-04 -1.72590177e-05]  gradient norm: 0.00012773746082750934\n",
            "iter: 7178  x: [-99.99993684  24.99999139]  f(x): 4.0629141818969865e-09  grad at x: [ 1.26312998e-04 -1.72244996e-05]  gradient norm: 0.00012748198589443115\n",
            "iter: 7179  x: [-99.99993697  24.9999914 ]  f(x): 4.046678776818228e-09  grad at x: [ 1.26060372e-04 -1.71900506e-05]  gradient norm: 0.00012722702192251813\n",
            "iter: 7180  x: [-99.9999371   24.99999142]  f(x): 4.030508248493187e-09  grad at x: [ 1.25808251e-04 -1.71556705e-05]  gradient norm: 0.00012697256787973043\n",
            "iter: 7181  x: [-99.99993722  24.99999144]  f(x): 4.014402336841414e-09  grad at x: [ 1.25556634e-04 -1.71213592e-05]  gradient norm: 0.00012671862273306816\n",
            "iter: 7182  x: [-99.99993735  24.99999146]  f(x): 3.998360784537497e-09  grad at x: [ 1.25305521e-04 -1.70871165e-05]  gradient norm: 0.00012646518547865254\n",
            "iter: 7183  x: [-99.99993747  24.99999147]  f(x): 3.982383335091729e-09  grad at x: [ 1.25054910e-04 -1.70529422e-05]  gradient norm: 0.00012621225511164484\n",
            "iter: 7184  x: [-99.9999376   24.99999149]  f(x): 3.9664697311954994e-09  grad at x: [ 1.24804800e-04 -1.70188364e-05]  gradient norm: 0.00012595983060000518\n",
            "iter: 7185  x: [-99.99993772  24.99999151]  f(x): 3.950619718215616e-09  grad at x: [ 1.24555191e-04 -1.69847987e-05]  gradient norm: 0.0001257079109398548\n",
            "iter: 7186  x: [-99.99993785  24.99999152]  f(x): 3.9348330424075e-09  grad at x: [ 1.24306080e-04 -1.69508291e-05]  gradient norm: 0.00012545649512731496\n",
            "iter: 7187  x: [-99.99993797  24.99999154]  f(x): 3.9191094491496934e-09  grad at x: [ 1.24057468e-04 -1.69169274e-05]  gradient norm: 0.00012520558213034583\n",
            "iter: 7188  x: [-99.9999381   24.99999156]  f(x): 3.903448688177288e-09  grad at x: [ 1.23809353e-04 -1.68830936e-05]  gradient norm: 0.0001249551709722697\n",
            "iter: 7189  x: [-99.99993822  24.99999158]  f(x): 3.887850506700657e-09  grad at x: [ 1.23561734e-04 -1.68493274e-05]  gradient norm: 0.00012470526062200674\n",
            "iter: 7190  x: [-99.99993834  24.99999159]  f(x): 3.872314656207591e-09  grad at x: [ 1.23314611e-04 -1.68156287e-05]  gradient norm: 0.0001244558501028793\n",
            "iter: 7191  x: [-99.99993847  24.99999161]  f(x): 3.85684088735902e-09  grad at x: [ 1.23067982e-04 -1.67819975e-05]  gradient norm: 0.0001242069384110086\n",
            "iter: 7192  x: [-99.99993859  24.99999163]  f(x): 3.841428951748879e-09  grad at x: [ 1.22821846e-04 -1.67484335e-05]  gradient norm: 0.0001239585245434759\n",
            "iter: 7193  x: [-99.99993871  24.99999164]  f(x): 3.8260786017228535e-09  grad at x: [ 1.22576202e-04 -1.67149366e-05]  gradient norm: 0.00012371060749544243\n",
            "iter: 7194  x: [-99.99993883  24.99999166]  f(x): 3.810789592352567e-09  grad at x: [ 1.22331050e-04 -1.66815067e-05]  gradient norm: 0.00012346318629215055\n",
            "iter: 7195  x: [-99.99993896  24.99999168]  f(x): 3.795561677708028e-09  grad at x: [ 1.22086388e-04 -1.66481437e-05]  gradient norm: 0.00012321625992876148\n",
            "iter: 7196  x: [-99.99993908  24.99999169]  f(x): 3.780394612841338e-09  grad at x: [ 1.21842215e-04 -1.66148474e-05]  gradient norm: 0.00012296982740235652\n",
            "iter: 7197  x: [-99.9999392   24.99999171]  f(x): 3.765288155334319e-09  grad at x: [ 1.21598530e-04 -1.65816177e-05]  gradient norm: 0.00012272388773721795\n",
            "iter: 7198  x: [-99.99993932  24.99999173]  f(x): 3.750242063672373e-09  grad at x: [ 1.21355333e-04 -1.65484545e-05]  gradient norm: 0.0001224784399585882\n",
            "iter: 7199  x: [-99.99993944  24.99999174]  f(x): 3.735256097065466e-09  grad at x: [ 1.21112623e-04 -1.65153576e-05]  gradient norm: 0.00012223348308978952\n",
            "iter: 7200  x: [-99.99993956  24.99999176]  f(x): 3.720330013963459e-09  grad at x: [ 1.20870397e-04 -1.64823269e-05]  gradient norm: 0.00012198901612790324\n",
            "iter: 7201  x: [-99.99993969  24.99999178]  f(x): 3.705463575322879e-09  grad at x: [ 1.20628657e-04 -1.64493622e-05]  gradient norm: 0.00012174503809721165\n",
            "iter: 7202  x: [-99.99993981  24.99999179]  f(x): 3.69065654299374e-09  grad at x: [ 1.20387399e-04 -1.64164635e-05]  gradient norm: 0.00012150154802295713\n",
            "iter: 7203  x: [-99.99993993  24.99999181]  f(x): 3.675908679600166e-09  grad at x: [ 1.20146624e-04 -1.63836306e-05]  gradient norm: 0.00012125854492942204\n",
            "iter: 7204  x: [-99.99994005  24.99999182]  f(x): 3.6612197486547767e-09  grad at x: [ 1.19906331e-04 -1.63508633e-05]  gradient norm: 0.00012101602784184873\n",
            "iter: 7205  x: [-99.99994017  24.99999184]  f(x): 3.6465895144397852e-09  grad at x: [ 1.19666519e-04 -1.63181616e-05]  gradient norm: 0.00012077399578451953\n",
            "iter: 7206  x: [-99.99994029  24.99999186]  f(x): 3.6320177421209282e-09  grad at x: [ 1.19427185e-04 -1.62855253e-05]  gradient norm: 0.00012053244778267681\n",
            "iter: 7207  x: [-99.99994041  24.99999187]  f(x): 3.6175041993228063e-09  grad at x: [ 1.19188331e-04 -1.62529542e-05]  gradient norm: 0.000120291382888764\n",
            "iter: 7208  x: [-99.99994053  24.99999189]  f(x): 3.60304865284466e-09  grad at x: [ 1.18949954e-04 -1.62204483e-05]  gradient norm: 0.00012005080012802347\n",
            "iter: 7209  x: [-99.99994064  24.99999191]  f(x): 3.588650870303898e-09  grad at x: [ 1.18712055e-04 -1.61880074e-05]  gradient norm: 0.00011981069852569758\n",
            "iter: 7210  x: [-99.99994076  24.99999192]  f(x): 3.5743106217599807e-09  grad at x: [ 1.18474630e-04 -1.61556314e-05]  gradient norm: 0.00011957107713422976\n",
            "iter: 7211  x: [-99.99994088  24.99999194]  f(x): 3.5600276764498597e-09  grad at x: [ 1.18237681e-04 -1.61233201e-05]  gradient norm: 0.00011933193497886239\n",
            "iter: 7212  x: [-99.999941    24.99999195]  f(x): 3.545801806098564e-09  grad at x: [ 1.18001206e-04 -1.60910735e-05]  gradient norm: 0.00011909327111299889\n",
            "iter: 7213  x: [-99.99994112  24.99999197]  f(x): 3.531632781495958e-09  grad at x: [ 1.17765203e-04 -1.60588913e-05]  gradient norm: 0.00011885508456092163\n",
            "iter: 7214  x: [-99.99994124  24.99999199]  f(x): 3.517520377693191e-09  grad at x: [ 1.17529673e-04 -1.60267736e-05]  gradient norm: 0.00011861737440515519\n",
            "iter: 7215  x: [-99.99994135  24.999992  ]  f(x): 3.5034643670711045e-09  grad at x: [ 1.17294614e-04 -1.59947200e-05]  gradient norm: 0.00011838013966998188\n",
            "iter: 7216  x: [-99.99994147  24.99999202]  f(x): 3.489464522926411e-09  grad at x: [ 1.17060024e-04 -1.59627306e-05]  gradient norm: 0.00011814337938160413\n",
            "iter: 7217  x: [-99.99994159  24.99999203]  f(x): 3.475520622562245e-09  grad at x: [ 1.16825904e-04 -1.59308051e-05]  gradient norm: 0.00011790709262062644\n",
            "iter: 7218  x: [-99.9999417   24.99999205]  f(x): 3.4616324424526115e-09  grad at x: [ 1.16592253e-04 -1.58989435e-05]  gradient norm: 0.00011767127844045226\n",
            "iter: 7219  x: [-99.99994182  24.99999207]  f(x): 3.447799759853401e-09  grad at x: [ 1.16359068e-04 -1.58671456e-05]  gradient norm: 0.00011743593589448506\n",
            "iter: 7220  x: [-99.99994194  24.99999208]  f(x): 3.4340223528564193e-09  grad at x: [ 1.16126350e-04 -1.58354113e-05]  gradient norm: 0.00011720106403708831\n",
            "iter: 7221  x: [-99.99994205  24.9999921 ]  f(x): 3.420300000218153e-09  grad at x: [ 1.15894097e-04 -1.58037405e-05]  gradient norm: 0.00011696666192070548\n",
            "iter: 7222  x: [-99.99994217  24.99999211]  f(x): 3.4066324815828057e-09  grad at x: [ 1.15662309e-04 -1.57721330e-05]  gradient norm: 0.00011673272859970002\n",
            "iter: 7223  x: [-99.99994228  24.99999213]  f(x): 3.393019578895638e-09  grad at x: [ 1.15430984e-04 -1.57405888e-05]  gradient norm: 0.00011649926315467645\n",
            "iter: 7224  x: [-99.9999424   24.99999215]  f(x): 3.379461073334528e-09  grad at x: [ 1.15200123e-04 -1.57091076e-05]  gradient norm: 0.00011626626463999827\n",
            "iter: 7225  x: [-99.99994252  24.99999216]  f(x): 3.365956746789793e-09  grad at x: [ 1.14969722e-04 -1.56776894e-05]  gradient norm: 0.00011603373210906893\n",
            "iter: 7226  x: [-99.99994263  24.99999218]  f(x): 3.3525063836042733e-09  grad at x: [ 1.14739783e-04 -1.56463340e-05]  gradient norm: 0.000115801664644413\n",
            "iter: 7227  x: [-99.99994274  24.99999219]  f(x): 3.339109768816021e-09  grad at x: [ 1.14510303e-04 -1.56150413e-05]  gradient norm: 0.00011557006132759507\n",
            "iter: 7228  x: [-99.99994286  24.99999221]  f(x): 3.325766686588055e-09  grad at x: [ 1.14281283e-04 -1.55838112e-05]  gradient norm: 0.00011533892121201855\n",
            "iter: 7229  x: [-99.99994297  24.99999222]  f(x): 3.3124769234640476e-09  grad at x: [ 1.14052720e-04 -1.55526436e-05]  gradient norm: 0.000115108243379248\n",
            "iter: 7230  x: [-99.99994309  24.99999224]  f(x): 3.2992402651699762e-09  grad at x: [ 1.13824615e-04 -1.55215383e-05]  gradient norm: 0.00011487802688364693\n",
            "iter: 7231  x: [-99.9999432   24.99999225]  f(x): 3.2860565013604927e-09  grad at x: [ 1.13596965e-04 -1.54904953e-05]  gradient norm: 0.00011464827083494094\n",
            "iter: 7232  x: [-99.99994332  24.99999227]  f(x): 3.272925419250528e-09  grad at x: [ 1.13369772e-04 -1.54595143e-05]  gradient norm: 0.00011441897428749357\n",
            "iter: 7233  x: [-99.99994343  24.99999229]  f(x): 3.259846810021542e-09  grad at x: [ 1.13143032e-04 -1.54285952e-05]  gradient norm: 0.00011419013635199044\n",
            "iter: 7234  x: [-99.99994354  24.9999923 ]  f(x): 3.2468204623139163e-09  grad at x: [ 1.12916746e-04 -1.53977380e-05]  gradient norm: 0.00011396175608183504\n",
            "iter: 7235  x: [-99.99994365  24.99999232]  f(x): 3.2338461671164546e-09  grad at x: [ 1.12690912e-04 -1.53669426e-05]  gradient norm: 0.0001137338325585919\n",
            "iter: 7236  x: [-99.99994377  24.99999233]  f(x): 3.2209237178026904e-09  grad at x: [ 1.12465531e-04 -1.53362087e-05]  gradient norm: 0.00011350636489294669\n",
            "iter: 7237  x: [-99.99994388  24.99999235]  f(x): 3.208052906867817e-09  grad at x: [ 1.12240600e-04 -1.53055363e-05]  gradient norm: 0.000113279352167424\n",
            "iter: 7238  x: [-99.99994399  24.99999236]  f(x): 3.19523352753446e-09  grad at x: [ 1.12016118e-04 -1.52749252e-05]  gradient norm: 0.00011305279346454842\n",
            "iter: 7239  x: [-99.9999441   24.99999238]  f(x): 3.18246537369643e-09  grad at x: [ 1.11792086e-04 -1.52443753e-05]  gradient norm: 0.00011282668786588447\n",
            "iter: 7240  x: [-99.99994422  24.99999239]  f(x): 3.169748241610772e-09  grad at x: [ 1.11568502e-04 -1.52138866e-05]  gradient norm: 0.00011260103448211782\n",
            "iter: 7241  x: [-99.99994433  24.99999241]  f(x): 3.1570819282430334e-09  grad at x: [ 1.11345365e-04 -1.51834588e-05]  gradient norm: 0.00011237583242393417\n",
            "iter: 7242  x: [-99.99994444  24.99999242]  f(x): 3.144466228053126e-09  grad at x: [ 1.11122674e-04 -1.51530919e-05]  gradient norm: 0.00011215108074473694\n",
            "iter: 7243  x: [-99.99994455  24.99999244]  f(x): 3.1319009410665733e-09  grad at x: [ 1.10900429e-04 -1.51227857e-05]  gradient norm: 0.00011192677858433295\n",
            "iter: 7244  x: [-99.99994466  24.99999245]  f(x): 3.1193858647453558e-09  grad at x: [ 1.10678628e-04 -1.50925402e-05]  gradient norm: 0.00011170292502428674\n",
            "iter: 7245  x: [-99.99994477  24.99999247]  f(x): 3.1069207989416382e-09  grad at x: [ 1.10457271e-04 -1.50623551e-05]  gradient norm: 0.000111479519176244\n",
            "iter: 7246  x: [-99.99994488  24.99999248]  f(x): 3.0945055440987687e-09  grad at x: [ 1.10236356e-04 -1.50322304e-05]  gradient norm: 0.00011125656014993037\n",
            "iter: 7247  x: [-99.99994499  24.9999925 ]  f(x): 3.082139899846836e-09  grad at x: [ 1.10015883e-04 -1.50021659e-05]  gradient norm: 0.00011103404702787044\n",
            "iter: 7248  x: [-99.9999451   24.99999251]  f(x): 3.069823668136371e-09  grad at x: [ 1.09795852e-04 -1.49721616e-05]  gradient norm: 0.00011081197892170992\n",
            "iter: 7249  x: [-99.99994521  24.99999253]  f(x): 3.0575566530609075e-09  grad at x: [ 1.09576260e-04 -1.49422172e-05]  gradient norm: 0.00011059035496933551\n",
            "iter: 7250  x: [-99.99994532  24.99999254]  f(x): 3.045338656390248e-09  grad at x: [ 1.09357107e-04 -1.49123328e-05]  gradient norm: 0.00011036917425423184\n",
            "iter: 7251  x: [-99.99994543  24.99999256]  f(x): 3.033169483590872e-09  grad at x: [ 1.09138393e-04 -1.48825081e-05]  gradient norm: 0.00011014843591428563\n",
            "iter: 7252  x: [-99.99994554  24.99999257]  f(x): 3.021048937813627e-09  grad at x: [ 1.08920116e-04 -1.48527431e-05]  gradient norm: 0.00010992813903298149\n",
            "iter: 7253  x: [-99.99994565  24.99999259]  f(x): 3.008976825942749e-09  grad at x: [ 1.08702276e-04 -1.48230376e-05]  gradient norm: 0.00010970828274916619\n",
            "iter: 7254  x: [-99.99994576  24.9999926 ]  f(x): 2.996952954043594e-09  grad at x: [ 1.08484872e-04 -1.47933916e-05]  gradient norm: 0.00010948886617448541\n",
            "iter: 7255  x: [-99.99994587  24.99999262]  f(x): 2.9849771302954445e-09  grad at x: [ 1.08267902e-04 -1.47638048e-05]  gradient norm: 0.0001092698884468259\n",
            "iter: 7256  x: [-99.99994597  24.99999263]  f(x): 2.9730491621135968e-09  grad at x: [ 1.08051366e-04 -1.47342772e-05]  gradient norm: 0.00010905134867783336\n",
            "iter: 7257  x: [-99.99994608  24.99999265]  f(x): 2.9611688575897317e-09  grad at x: [ 1.07835263e-04 -1.47048086e-05]  gradient norm: 0.00010883324597915347\n",
            "iter: 7258  x: [-99.99994619  24.99999266]  f(x): 2.9493360269150605e-09  grad at x: [ 1.07619593e-04 -1.46753990e-05]  gradient norm: 0.00010861557948867299\n",
            "iter: 7259  x: [-99.9999463   24.99999268]  f(x): 2.937550479519651e-09  grad at x: [ 1.07404354e-04 -1.46460482e-05]  gradient norm: 0.0001083983483180376\n",
            "iter: 7260  x: [-99.99994641  24.99999269]  f(x): 2.925812028550682e-09  grad at x: [ 1.07189545e-04 -1.46167561e-05]  gradient norm: 0.00010818155163521518\n",
            "iter: 7261  x: [-99.99994651  24.99999271]  f(x): 2.914120483187174e-09  grad at x: [ 1.06975166e-04 -1.45875226e-05]  gradient norm: 0.0001079651885227303\n",
            "iter: 7262  x: [-99.99994662  24.99999272]  f(x): 2.902475657839043e-09  grad at x: [ 1.06761216e-04 -1.45583475e-05]  gradient norm: 0.00010774925814759084\n",
            "iter: 7263  x: [-99.99994673  24.99999274]  f(x): 2.8908773645805433e-09  grad at x: [ 1.06547693e-04 -1.45292308e-05]  gradient norm: 0.00010753375962144248\n",
            "iter: 7264  x: [-99.99994683  24.99999275]  f(x): 2.879325419171144e-09  grad at x: [ 1.06334598e-04 -1.45001724e-05]  gradient norm: 0.00010731869211225311\n",
            "iter: 7265  x: [-99.99994694  24.99999276]  f(x): 2.8678196349396438e-09  grad at x: [ 1.06121929e-04 -1.44711720e-05]  gradient norm: 0.00010710405473070838\n",
            "iter: 7266  x: [-99.99994705  24.99999278]  f(x): 2.856359827430876e-09  grad at x: [ 1.05909685e-04 -1.44422297e-05]  gradient norm: 0.00010688984661661511\n",
            "iter: 7267  x: [-99.99994715  24.99999279]  f(x): 2.8449458128349683e-09  grad at x: [ 1.05697865e-04 -1.44133452e-05]  gradient norm: 0.00010667606690978006\n",
            "iter: 7268  x: [-99.99994726  24.99999281]  f(x): 2.8335774094334515e-09  grad at x: [ 1.05486470e-04 -1.43845185e-05]  gradient norm: 0.00010646271477721112\n",
            "iter: 7269  x: [-99.99994736  24.99999282]  f(x): 2.822254434692829e-09  grad at x: [ 1.05275497e-04 -1.43557495e-05]  gradient norm: 0.00010624978935871503\n",
            "iter: 7270  x: [-99.99994747  24.99999284]  f(x): 2.8109767067194134e-09  grad at x: [ 1.05064946e-04 -1.43270380e-05]  gradient norm: 0.00010603728979409863\n",
            "iter: 7271  x: [-99.99994757  24.99999285]  f(x): 2.7997440442066956e-09  grad at x: [ 1.04854816e-04 -1.42983839e-05]  gradient norm: 0.00010582521522220865\n",
            "iter: 7272  x: [-99.99994768  24.99999287]  f(x): 2.7885562665861097e-09  grad at x: [ 1.04645106e-04 -1.42697872e-05]  gradient norm: 0.00010561356478381193\n",
            "iter: 7273  x: [-99.99994778  24.99999288]  f(x): 2.7774131953059366e-09  grad at x: [ 1.04435816e-04 -1.42412476e-05]  gradient norm: 0.0001054023376459163\n",
            "iter: 7274  x: [-99.99994789  24.99999289]  f(x): 2.766314652485978e-09  grad at x: [ 1.04226944e-04 -1.42127651e-05]  gradient norm: 0.00010519153297648966\n",
            "iter: 7275  x: [-99.99994799  24.99999291]  f(x): 2.755260459386709e-09  grad at x: [ 1.04018490e-04 -1.41843396e-05]  gradient norm: 0.00010498114991533879\n",
            "iter: 7276  x: [-99.99994809  24.99999292]  f(x): 2.7442504378975387e-09  grad at x: [ 1.03810453e-04 -1.41559709e-05]  gradient norm: 0.00010477118760227047\n",
            "iter: 7277  x: [-99.9999482   24.99999294]  f(x): 2.7332844134795666e-09  grad at x: [ 1.03602833e-04 -1.41276589e-05]  gradient norm: 0.0001045616452334137\n",
            "iter: 7278  x: [-99.9999483   24.99999295]  f(x): 2.722362209307061e-09  grad at x: [ 1.03395627e-04 -1.40994036e-05]  gradient norm: 0.00010435252194953529\n",
            "iter: 7279  x: [-99.99994841  24.99999296]  f(x): 2.7114836505438904e-09  grad at x: [ 1.03188836e-04 -1.40712048e-05]  gradient norm: 0.00010414381691764308\n",
            "iter: 7280  x: [-99.99994851  24.99999298]  f(x): 2.700648561550795e-09  grad at x: [ 1.02982458e-04 -1.40430624e-05]  gradient norm: 0.00010393552927754387\n",
            "iter: 7281  x: [-99.99994861  24.99999299]  f(x): 2.6898567702296174e-09  grad at x: [ 1.02776493e-04 -1.40149763e-05]  gradient norm: 0.00010372765822536663\n",
            "iter: 7282  x: [-99.99994871  24.99999301]  f(x): 2.679108102211507e-09  grad at x: [ 1.02570940e-04 -1.39869463e-05]  gradient norm: 0.00010352020290187819\n",
            "iter: 7283  x: [-99.99994882  24.99999302]  f(x): 2.6684023866035403e-09  grad at x: [ 1.02365798e-04 -1.39589724e-05]  gradient norm: 0.00010331316250320751\n",
            "iter: 7284  x: [-99.99994892  24.99999303]  f(x): 2.657739450200646e-09  grad at x: [ 1.02161067e-04 -1.39310545e-05]  gradient norm: 0.00010310653616916138\n",
            "iter: 7285  x: [-99.99994902  24.99999305]  f(x): 2.6471191233085463e-09  grad at x: [ 1.01956744e-04 -1.39031924e-05]  gradient norm: 0.00010290032309586878\n",
            "iter: 7286  x: [-99.99994912  24.99999306]  f(x): 2.636541235375016e-09  grad at x: [ 1.01752831e-04 -1.38753860e-05]  gradient norm: 0.00010269452245129759\n",
            "iter: 7287  x: [-99.99994923  24.99999308]  f(x): 2.6260056164949806e-09  grad at x: [ 1.01549325e-04 -1.38476352e-05]  gradient norm: 0.00010248913340437572\n",
            "iter: 7288  x: [-99.99994933  24.99999309]  f(x): 2.615512098750331e-09  grad at x: [ 1.01346227e-04 -1.38199400e-05]  gradient norm: 0.00010228415515123212\n",
            "iter: 7289  x: [-99.99994943  24.9999931 ]  f(x): 2.605060511980299e-09  grad at x: [ 1.01143534e-04 -1.37923001e-05]  gradient norm: 0.00010207958683263366\n",
            "iter: 7290  x: [-99.99994953  24.99999312]  f(x): 2.5946506908825607e-09  grad at x: [ 1.00941247e-04 -1.37647155e-05]  gradient norm: 0.00010187542767287037\n",
            "iter: 7291  x: [-99.99994963  24.99999313]  f(x): 2.5842824664281875e-09  grad at x: [ 1.00739365e-04 -1.37371860e-05]  gradient norm: 0.00010167167681174906\n",
            "iter: 7292  x: [-99.99994973  24.99999315]  f(x): 2.5739556730953587e-09  grad at x: [ 1.00537886e-04 -1.37097117e-05]  gradient norm: 0.0001014683334463587\n",
            "iter: 7293  x: [-99.99994983  24.99999316]  f(x): 2.563670145889585e-09  grad at x: [ 1.00336810e-04 -1.36822922e-05]  gradient norm: 0.00010126539677282828\n",
            "iter: 7294  x: [-99.99994993  24.99999317]  f(x): 2.5534257204395835e-09  grad at x: [ 1.00136136e-04 -1.36549277e-05]  gradient norm: 0.00010106286598824681\n",
            "iter: 7295  x: [-99.99995003  24.99999319]  f(x): 2.54322223152657e-09  grad at x: [ 9.99358642e-05 -1.36276178e-05]  gradient norm: 0.0001008607402615422\n",
            "iter: 7296  x: [-99.99995013  24.9999932 ]  f(x): 2.5330595158831836e-09  grad at x: [ 9.97359925e-05 -1.36003626e-05]  gradient norm: 0.00010065901878884343\n",
            "iter: 7297  x: [-99.99995023  24.99999321]  f(x): 2.522937409397336e-09  grad at x: [ 9.95365205e-05 -1.35731619e-05]  gradient norm: 0.00010045770073811835\n",
            "iter: 7298  x: [-99.99995033  24.99999323]  f(x): 2.5128557514559974e-09  grad at x: [ 9.93374475e-05 -1.35460155e-05]  gradient norm: 0.00010025678533557711\n",
            "iter: 7299  x: [-99.99995043  24.99999324]  f(x): 2.502814380495665e-09  grad at x: [ 9.91387726e-05 -1.35189235e-05]  gradient norm: 0.00010005627177734866\n",
            "iter: 7300  x: [-99.99995053  24.99999325]  f(x): 2.492813134111724e-09  grad at x: [ 9.89404950e-05 -1.34918857e-05]  gradient norm: 9.985615923140092e-05\n",
            "iter: 7301  x: [-99.99995063  24.99999327]  f(x): 2.4828518533762164e-09  grad at x: [ 9.87426141e-05 -1.34649019e-05]  gradient norm: 9.965644692394399e-05\n",
            "iter: 7302  x: [-99.99995073  24.99999328]  f(x): 2.4729303770145477e-09  grad at x: [ 9.85451288e-05 -1.34379721e-05]  gradient norm: 9.945713402294574e-05\n",
            "iter: 7303  x: [-99.99995083  24.99999329]  f(x): 2.4630485471662807e-09  grad at x: [ 9.83480386e-05 -1.34110961e-05]  gradient norm: 9.925821975365629e-05\n",
            "iter: 7304  x: [-99.99995092  24.99999331]  f(x): 2.4532062051230493e-09  grad at x: [ 9.81513425e-05 -1.33842739e-05]  gradient norm: 9.905970331316461e-05\n",
            "iter: 7305  x: [-99.99995102  24.99999332]  f(x): 2.4434031927329915e-09  grad at x: [ 9.79550398e-05 -1.33575054e-05]  gradient norm: 9.886158389855974e-05\n",
            "iter: 7306  x: [-99.99995112  24.99999333]  f(x): 2.433639353741034e-09  grad at x: [ 9.77591297e-05 -1.33307904e-05]  gradient norm: 9.86638607341317e-05\n",
            "iter: 7307  x: [-99.99995122  24.99999335]  f(x): 2.4239145311427536e-09  grad at x: [ 9.75636115e-05 -1.33041288e-05]  gradient norm: 9.846653301792957e-05\n",
            "iter: 7308  x: [-99.99995132  24.99999336]  f(x): 2.414228568437935e-09  grad at x: [ 9.73684842e-05 -1.32775205e-05]  gradient norm: 9.826959994704233e-05\n",
            "iter: 7309  x: [-99.99995141  24.99999337]  f(x): 2.404581311010384e-09  grad at x: [ 9.71737473e-05 -1.32509655e-05]  gradient norm: 9.807306074576002e-05\n",
            "iter: 7310  x: [-99.99995151  24.99999339]  f(x): 2.3949726034976307e-09  grad at x: [ 9.69793998e-05 -1.32244636e-05]  gradient norm: 9.787691461213171e-05\n",
            "iter: 7311  x: [-99.99995161  24.9999934 ]  f(x): 2.3854022923655005e-09  grad at x: [ 9.67854409e-05 -1.31980146e-05]  gradient norm: 9.768116077044745e-05\n",
            "iter: 7312  x: [-99.9999517   24.99999341]  f(x): 2.3758702247082485e-09  grad at x: [ 9.65918701e-05 -1.31716186e-05]  gradient norm: 9.748579844691735e-05\n",
            "iter: 7313  x: [-99.9999518   24.99999343]  f(x): 2.3663762467360933e-09  grad at x: [ 9.63986863e-05 -1.31452754e-05]  gradient norm: 9.729082683863044e-05\n",
            "iter: 7314  x: [-99.9999519   24.99999344]  f(x): 2.3569202079359797e-09  grad at x: [ 9.62058890e-05 -1.31189848e-05]  gradient norm: 9.709624519899789e-05\n",
            "iter: 7315  x: [-99.99995199  24.99999345]  f(x): 2.3475019542205024e-09  grad at x: [ 9.60134772e-05 -1.30927469e-05]  gradient norm: 9.69020526969476e-05\n",
            "iter: 7316  x: [-99.99995209  24.99999347]  f(x): 2.3381213361376108e-09  grad at x: [ 9.58214502e-05 -1.30665614e-05]  gradient norm: 9.670824858589076e-05\n",
            "iter: 7317  x: [-99.99995219  24.99999348]  f(x): 2.3287782034385486e-09  grad at x: [ 9.56298073e-05 -1.30404282e-05]  gradient norm: 9.651483209203752e-05\n",
            "iter: 7318  x: [-99.99995228  24.99999349]  f(x): 2.3194724063533207e-09  grad at x: [ 9.54385477e-05 -1.30143474e-05]  gradient norm: 9.632180244063793e-05\n",
            "iter: 7319  x: [-99.99995238  24.99999351]  f(x): 2.3102037942822523e-09  grad at x: [ 9.52476706e-05 -1.29883187e-05]  gradient norm: 9.612915882878102e-05\n",
            "iter: 7320  x: [-99.99995247  24.99999352]  f(x): 2.300972219860556e-09  grad at x: [ 9.50571753e-05 -1.29623420e-05]  gradient norm: 9.593690050987797e-05\n",
            "iter: 7321  x: [-99.99995257  24.99999353]  f(x): 2.2917775349313035e-09  grad at x: [ 9.48670609e-05 -1.29364174e-05]  gradient norm: 9.574502671013891e-05\n",
            "iter: 7322  x: [-99.99995266  24.99999354]  f(x): 2.2826195918107167e-09  grad at x: [ 9.46773268e-05 -1.29105445e-05]  gradient norm: 9.555353665481392e-05\n",
            "iter: 7323  x: [-99.99995276  24.99999356]  f(x): 2.2734982433786966e-09  grad at x: [ 9.44879721e-05 -1.28847234e-05]  gradient norm: 9.536242957011312e-05\n",
            "iter: 7324  x: [-99.99995285  24.99999357]  f(x): 2.264413344325579e-09  grad at x: [ 9.42989962e-05 -1.28589540e-05]  gradient norm: 9.517170470944774e-05\n",
            "iter: 7325  x: [-99.99995294  24.99999358]  f(x): 2.2553647485085224e-09  grad at x: [ 9.41103982e-05 -1.28332361e-05]  gradient norm: 9.498136129806779e-05\n",
            "iter: 7326  x: [-99.99995304  24.9999936 ]  f(x): 2.2463523103436794e-09  grad at x: [ 9.39221774e-05 -1.28075696e-05]  gradient norm: 9.479139856218347e-05\n",
            "iter: 7327  x: [-99.99995313  24.99999361]  f(x): 2.237375886090892e-09  grad at x: [ 9.37343330e-05 -1.27819545e-05]  gradient norm: 9.460181575616596e-05\n",
            "iter: 7328  x: [-99.99995323  24.99999362]  f(x): 2.2284353324642057e-09  grad at x: [ 9.35468643e-05 -1.27563906e-05]  gradient norm: 9.441261213342644e-05\n",
            "iter: 7329  x: [-99.99995332  24.99999363]  f(x): 2.2195305053494087e-09  grad at x: [ 9.33597706e-05 -1.27308778e-05]  gradient norm: 9.422378691921502e-05\n",
            "iter: 7330  x: [-99.99995341  24.99999365]  f(x): 2.2106612611850985e-09  grad at x: [ 9.31730511e-05 -1.27054160e-05]  gradient norm: 9.403533933974182e-05\n",
            "iter: 7331  x: [-99.99995351  24.99999366]  f(x): 2.2018274582371332e-09  grad at x: [ 9.29867050e-05 -1.26800052e-05]  gradient norm: 9.384726864937804e-05\n",
            "iter: 7332  x: [-99.9999536   24.99999367]  f(x): 2.193028955265184e-09  grad at x: [ 9.28007316e-05 -1.26546452e-05]  gradient norm: 9.365957410249492e-05\n",
            "iter: 7333  x: [-99.99995369  24.99999369]  f(x): 2.184265611476459e-09  grad at x: [ 9.26151301e-05 -1.26293359e-05]  gradient norm: 9.347225495250361e-05\n",
            "iter: 7334  x: [-99.99995379  24.9999937 ]  f(x): 2.1755372866143124e-09  grad at x: [ 9.24298998e-05 -1.26040772e-05]  gradient norm: 9.328531045377536e-05\n",
            "iter: 7335  x: [-99.99995388  24.99999371]  f(x): 2.1668438396008242e-09  grad at x: [ 9.22450400e-05 -1.25788691e-05]  gradient norm: 9.309873983252027e-05\n",
            "iter: 7336  x: [-99.99995397  24.99999372]  f(x): 2.1581851311204327e-09  grad at x: [ 9.20605499e-05 -1.25537113e-05]  gradient norm: 9.291254234214954e-05\n",
            "iter: 7337  x: [-99.99995406  24.99999374]  f(x): 2.1495610236948745e-09  grad at x: [ 9.18764289e-05 -1.25286039e-05]  gradient norm: 9.272671726519546e-05\n",
            "iter: 7338  x: [-99.99995415  24.99999375]  f(x): 2.140971377759196e-09  grad at x: [ 9.16926760e-05 -1.25035467e-05]  gradient norm: 9.254126382882819e-05\n",
            "iter: 7339  x: [-99.99995425  24.99999376]  f(x): 2.132416056710409e-09  grad at x: [ 9.15092907e-05 -1.24785396e-05]  gradient norm: 9.235618131365998e-05\n",
            "iter: 7340  x: [-99.99995434  24.99999377]  f(x): 2.1238949219555395e-09  grad at x: [ 9.13262721e-05 -1.24535825e-05]  gradient norm: 9.217146894686098e-05\n",
            "iter: 7341  x: [-99.99995443  24.99999379]  f(x): 2.115407837939219e-09  grad at x: [ 9.11436195e-05 -1.24286754e-05]  gradient norm: 9.198712601096349e-05\n",
            "iter: 7342  x: [-99.99995452  24.9999938 ]  f(x): 2.1069546682829467e-09  grad at x: [ 9.09613323e-05 -1.24038180e-05]  gradient norm: 9.180315176033874e-05\n",
            "iter: 7343  x: [-99.99995461  24.99999381]  f(x): 2.098535277086741e-09  grad at x: [ 9.07794096e-05 -1.23790104e-05]  gradient norm: 9.161954544935794e-05\n",
            "iter: 7344  x: [-99.9999547   24.99999382]  f(x): 2.0901495301713526e-09  grad at x: [ 9.05978508e-05 -1.23542523e-05]  gradient norm: 9.143630635959334e-05\n",
            "iter: 7345  x: [-99.99995479  24.99999384]  f(x): 2.0817972926260686e-09  grad at x: [ 9.04166551e-05 -1.23295438e-05]  gradient norm: 9.125343374637622e-05\n",
            "iter: 7346  x: [-99.99995488  24.99999385]  f(x): 2.0734784312528788e-09  grad at x: [ 9.02358218e-05 -1.23048848e-05]  gradient norm: 9.107092689223886e-05\n",
            "iter: 7347  x: [-99.99995497  24.99999386]  f(x): 2.0651928120368275e-09  grad at x: [ 9.00553502e-05 -1.22802750e-05]  gradient norm: 9.088878505155248e-05\n",
            "iter: 7348  x: [-99.99995506  24.99999387]  f(x): 2.0569403014347045e-09  grad at x: [ 8.98752395e-05 -1.22557144e-05]  gradient norm: 9.070700747868832e-05\n",
            "iter: 7349  x: [-99.99995515  24.99999388]  f(x): 2.0487207676483336e-09  grad at x: [ 8.9695489e-05 -1.2231203e-05]  gradient norm: 9.052559345617865e-05\n",
            "iter: 7350  x: [-99.99995524  24.9999939 ]  f(x): 2.040534079338371e-09  grad at x: [ 8.95160980e-05 -1.22067406e-05]  gradient norm: 9.034454226655578e-05\n",
            "iter: 7351  x: [-99.99995533  24.99999391]  f(x): 2.0323801056662827e-09  grad at x: [ 8.93370658e-05 -1.21823271e-05]  gradient norm: 9.016385319331207e-05\n",
            "iter: 7352  x: [-99.99995542  24.99999392]  f(x): 2.0242587149392055e-09  grad at x: [ 8.91583917e-05 -1.21579625e-05]  gradient norm: 8.998352549081871e-05\n",
            "iter: 7353  x: [-99.99995551  24.99999393]  f(x): 2.016169777193827e-09  grad at x: [ 8.89800749e-05 -1.21336465e-05]  gradient norm: 8.980355844160803e-05\n",
            "iter: 7354  x: [-99.9999556   24.99999395]  f(x): 2.0081131629204422e-09  grad at x: [ 8.88021148e-05 -1.21093792e-05]  gradient norm: 8.962395132821231e-05\n",
            "iter: 7355  x: [-99.99995569  24.99999396]  f(x): 2.0000887431045938e-09  grad at x: [ 8.86245105e-05 -1.20851605e-05]  gradient norm: 8.944470343412389e-05\n",
            "iter: 7356  x: [-99.99995578  24.99999397]  f(x): 1.992096387882745e-09  grad at x: [ 8.84472615e-05 -1.20609902e-05]  gradient norm: 8.9265814013714e-05\n",
            "iter: 7357  x: [-99.99995586  24.99999398]  f(x): 1.9841359703599466e-09  grad at x: [ 8.82703670e-05 -1.20368682e-05]  gradient norm: 8.908728237767603e-05\n",
            "iter: 7358  x: [-99.99995595  24.99999399]  f(x): 1.9762073628704255e-09  grad at x: [ 8.80938262e-05 -1.20127945e-05]  gradient norm: 8.890910780950229e-05\n",
            "iter: 7359  x: [-99.99995604  24.99999401]  f(x): 1.9683104381526473e-09  grad at x: [ 8.79176386e-05 -1.19887689e-05]  gradient norm: 8.873128959172514e-05\n",
            "iter: 7360  x: [-99.99995613  24.99999402]  f(x): 1.9604450694334743e-09  grad at x: [ 8.77418033e-05 -1.19647913e-05]  gradient norm: 8.855382700783686e-05\n",
            "iter: 7361  x: [-99.99995622  24.99999403]  f(x): 1.952611130341623e-09  grad at x: [ 8.75663197e-05 -1.19408618e-05]  gradient norm: 8.837671934036979e-05\n",
            "iter: 7362  x: [-99.9999563   24.99999404]  f(x): 1.9448084961910537e-09  grad at x: [ 8.7391187e-05 -1.1916980e-05]  gradient norm: 8.81999659000173e-05\n",
            "iter: 7363  x: [-99.99995639  24.99999405]  f(x): 1.9370370415306955e-09  grad at x: [ 8.72164047e-05 -1.18931461e-05]  gradient norm: 8.802356597027174e-05\n",
            "iter: 7364  x: [-99.99995648  24.99999407]  f(x): 1.929296641349969e-09  grad at x: [ 8.70419719e-05 -1.18693598e-05]  gradient norm: 8.784751883462546e-05\n",
            "iter: 7365  x: [-99.99995657  24.99999408]  f(x): 1.9215871722699033e-09  grad at x: [ 8.68678879e-05 -1.18456211e-05]  gradient norm: 8.767182380377183e-05\n",
            "iter: 7366  x: [-99.99995665  24.99999409]  f(x): 1.9139085101499565e-09  grad at x: [ 8.66941521e-05 -1.18219298e-05]  gradient norm: 8.74964801612032e-05\n",
            "iter: 7367  x: [-99.99995674  24.9999941 ]  f(x): 1.906260531286318e-09  grad at x: [ 8.65207638e-05 -1.17982860e-05]  gradient norm: 8.732148719041191e-05\n",
            "iter: 7368  x: [-99.99995683  24.99999411]  f(x): 1.898643114822971e-09  grad at x: [ 8.63477223e-05 -1.17746894e-05]  gradient norm: 8.714684423025245e-05\n",
            "iter: 7369  x: [-99.99995691  24.99999412]  f(x): 1.8910561366867054e-09  grad at x: [ 8.61750269e-05 -1.17511400e-05]  gradient norm: 8.697255053605604e-05\n",
            "iter: 7370  x: [-99.999957    24.99999414]  f(x): 1.883499476871984e-09  grad at x: [ 8.60026768e-05 -1.17276377e-05]  gradient norm: 8.679860544667717e-05\n",
            "iter: 7371  x: [-99.99995708  24.99999415]  f(x): 1.875973013387922e-09  grad at x: [ 8.58306715e-05 -1.17041824e-05]  gradient norm: 8.66250082456082e-05\n",
            "iter: 7372  x: [-99.99995717  24.99999416]  f(x): 1.868476624715689e-09  grad at x: [ 8.56590101e-05 -1.16807741e-05]  gradient norm: 8.645175821730149e-05\n",
            "iter: 7373  x: [-99.99995726  24.99999417]  f(x): 1.8610101920708928e-09  grad at x: [ 8.54876921e-05 -1.16574125e-05]  gradient norm: 8.627885469965147e-05\n",
            "iter: 7374  x: [-99.99995734  24.99999418]  f(x): 1.8535735947775892e-09  grad at x: [ 8.53167167e-05 -1.16340977e-05]  gradient norm: 8.610629697711055e-05\n",
            "iter: 7375  x: [-99.99995743  24.99999419]  f(x): 1.846166714965135e-09  grad at x: [ 8.51460833e-05 -1.16108295e-05]  gradient norm: 8.59340843894932e-05\n",
            "iter: 7376  x: [-99.99995751  24.99999421]  f(x): 1.8387894327128533e-09  grad at x: [ 8.49757911e-05 -1.15876079e-05]  gradient norm: 8.576221621933177e-05\n",
            "iter: 7377  x: [-99.9999576   24.99999422]  f(x): 1.8314416297711171e-09  grad at x: [ 8.48058395e-05 -1.15644326e-05]  gradient norm: 8.559069177827965e-05\n",
            "iter: 7378  x: [-99.99995768  24.99999423]  f(x): 1.8241231895476538e-09  grad at x: [ 8.46362279e-05 -1.15413038e-05]  gradient norm: 8.541951040711141e-05\n",
            "iter: 7379  x: [-99.99995777  24.99999424]  f(x): 1.8168339933701193e-09  grad at x: [ 8.44669554e-05 -1.15182212e-05]  gradient norm: 8.524867138835934e-05\n",
            "iter: 7380  x: [-99.99995785  24.99999425]  f(x): 1.809573924226153e-09  grad at x: [ 8.42980215e-05 -1.14951847e-05]  gradient norm: 8.507817403367689e-05\n",
            "iter: 7381  x: [-99.99995794  24.99999426]  f(x): 1.8023428667090042e-09  grad at x: [ 8.41294254e-05 -1.14721944e-05]  gradient norm: 8.490801768287855e-05\n",
            "iter: 7382  x: [-99.99995802  24.99999428]  f(x): 1.7951407046180822e-09  grad at x: [ 8.39611666e-05 -1.14492500e-05]  gradient norm: 8.473820164761775e-05\n",
            "iter: 7383  x: [-99.9999581   24.99999429]  f(x): 1.7879673222011328e-09  grad at x: [ 8.37932442e-05 -1.14263515e-05]  gradient norm: 8.456872524050798e-05\n",
            "iter: 7384  x: [-99.99995819  24.9999943 ]  f(x): 1.7808226052195124e-09  grad at x: [ 8.36256578e-05 -1.14034988e-05]  gradient norm: 8.439958780040368e-05\n",
            "iter: 7385  x: [-99.99995827  24.99999431]  f(x): 1.7737064374992013e-09  grad at x: [ 8.34584064e-05 -1.13806918e-05]  gradient norm: 8.423078861079721e-05\n",
            "iter: 7386  x: [-99.99995835  24.99999432]  f(x): 1.766618706871215e-09  grad at x: [ 8.32914896e-05 -1.13579304e-05]  gradient norm: 8.40623270406242e-05\n",
            "iter: 7387  x: [-99.99995844  24.99999433]  f(x): 1.7595592979662843e-09  grad at x: [ 8.31249066e-05 -1.13352145e-05]  gradient norm: 8.389420237337701e-05\n",
            "iter: 7388  x: [-99.99995852  24.99999434]  f(x): 1.7525280993633023e-09  grad at x: [ 8.29586568e-05 -1.13125441e-05]  gradient norm: 8.372641397703122e-05\n",
            "iter: 7389  x: [-99.9999586   24.99999436]  f(x): 1.745524997669974e-09  grad at x: [ 8.27927395e-05 -1.12899190e-05]  gradient norm: 8.355896116324027e-05\n",
            "iter: 7390  x: [-99.99995869  24.99999437]  f(x): 1.7385498798936893e-09  grad at x: [ 8.26271541e-05 -1.12673392e-05]  gradient norm: 8.339184324365757e-05\n",
            "iter: 7391  x: [-99.99995877  24.99999438]  f(x): 1.731602634652175e-09  grad at x: [ 8.24618998e-05 -1.12448045e-05]  gradient norm: 8.32250595590577e-05\n",
            "iter: 7392  x: [-99.99995885  24.99999439]  f(x): 1.7246831509109737e-09  grad at x: [ 8.22969760e-05 -1.12223149e-05]  gradient norm: 8.305860944925513e-05\n",
            "iter: 7393  x: [-99.99995893  24.9999944 ]  f(x): 1.717791316855361e-09  grad at x: [ 8.21323820e-05 -1.11998703e-05]  gradient norm: 8.289249222590333e-05\n",
            "iter: 7394  x: [-99.99995902  24.99999441]  f(x): 1.7109270222703026e-09  grad at x: [ 8.19681172e-05 -1.11774705e-05]  gradient norm: 8.272670722977684e-05\n",
            "iter: 7395  x: [-99.9999591   24.99999442]  f(x): 1.704090158448001e-09  grad at x: [ 8.18041810e-05 -1.11551156e-05]  gradient norm: 8.256125382885125e-05\n",
            "iter: 7396  x: [-99.99995918  24.99999443]  f(x): 1.6972806147345781e-09  grad at x: [ 8.16405727e-05 -1.11328053e-05]  gradient norm: 8.239613133477998e-05\n",
            "iter: 7397  x: [-99.99995926  24.99999444]  f(x): 1.6904982809074284e-09  grad at x: [ 8.14772915e-05 -1.11105397e-05]  gradient norm: 8.223133906017652e-05\n",
            "iter: 7398  x: [-99.99995934  24.99999446]  f(x): 1.683743049405992e-09  grad at x: [ 8.13143369e-05 -1.10883187e-05]  gradient norm: 8.206687637301647e-05\n",
            "iter: 7399  x: [-99.99995942  24.99999447]  f(x): 1.6770148119268276e-09  grad at x: [ 8.11517082e-05 -1.10661420e-05]  gradient norm: 8.190274261407435e-05\n",
            "iter: 7400  x: [-99.99995951  24.99999448]  f(x): 1.6703134605061683e-09  grad at x: [ 8.09894048e-05 -1.10440097e-05]  gradient norm: 8.173893712316471e-05\n",
            "iter: 7401  x: [-99.99995959  24.99999449]  f(x): 1.6636388875975448e-09  grad at x: [ 8.08274260e-05 -1.10219217e-05]  gradient norm: 8.157545924106207e-05\n",
            "iter: 7402  x: [-99.99995967  24.9999945 ]  f(x): 1.6569909871384838e-09  grad at x: [ 8.06657712e-05 -1.09998779e-05]  gradient norm: 8.141230833574206e-05\n",
            "iter: 7403  x: [-99.99995975  24.99999451]  f(x): 1.6503696511842472e-09  grad at x: [ 8.05044396e-05 -1.09778781e-05]  gradient norm: 8.124948371981811e-05\n",
            "iter: 7404  x: [-99.99995983  24.99999452]  f(x): 1.6437747744184406e-09  grad at x: [ 8.03434307e-05 -1.09559224e-05]  gradient norm: 8.108698476126586e-05\n",
            "iter: 7405  x: [-99.99995991  24.99999453]  f(x): 1.6372062507887407e-09  grad at x: [ 8.01827439e-05 -1.09340105e-05]  gradient norm: 8.092481080085986e-05\n",
            "iter: 7406  x: [-99.99995999  24.99999454]  f(x): 1.6306639746152926e-09  grad at x: [ 8.00223784e-05 -1.09121425e-05]  gradient norm: 8.076296117937461e-05\n",
            "iter: 7407  x: [-99.99996007  24.99999455]  f(x): 1.6241478416858622e-09  grad at x: [ 7.98623336e-05 -1.08903182e-05]  gradient norm: 8.060143526478576e-05\n",
            "iter: 7408  x: [-99.99996015  24.99999457]  f(x): 1.617657747055747e-09  grad at x: [ 7.97026090e-05 -1.08685376e-05]  gradient norm: 8.044023239786785e-05\n",
            "iter: 7409  x: [-99.99996023  24.99999458]  f(x): 1.6111935861495346e-09  grad at x: [ 7.95432038e-05 -1.08468005e-05]  gradient norm: 8.027935191939543e-05\n",
            "iter: 7410  x: [-99.99996031  24.99999459]  f(x): 1.6047552569778137e-09  grad at x: [ 7.93841174e-05 -1.08251069e-05]  gradient norm: 8.011879322550519e-05\n",
            "iter: 7411  x: [-99.99996039  24.9999946 ]  f(x): 1.5983426545613987e-09  grad at x: [ 7.92253491e-05 -1.08034567e-05]  gradient norm: 7.995855562881058e-05\n",
            "iter: 7412  x: [-99.99996047  24.99999461]  f(x): 1.591955677667095e-09  grad at x: [ 7.90668984e-05 -1.07818498e-05]  gradient norm: 7.979863852640833e-05\n",
            "iter: 7413  x: [-99.99996055  24.99999462]  f(x): 1.5855942231660833e-09  grad at x: [ 7.89087646e-05 -1.07602861e-05]  gradient norm: 7.963904125907301e-05\n",
            "iter: 7414  x: [-99.99996062  24.99999463]  f(x): 1.5792581882936182e-09  grad at x: [ 7.87509471e-05 -1.07387655e-05]  gradient norm: 7.947976316757916e-05\n",
            "iter: 7415  x: [-99.9999607   24.99999464]  f(x): 1.5729474728436595e-09  grad at x: [ 7.85934452e-05 -1.07172880e-05]  gradient norm: 7.932080364806347e-05\n",
            "iter: 7416  x: [-99.99996078  24.99999465]  f(x): 1.5666619748012778e-09  grad at x: [ 7.84362583e-05 -1.06958534e-05]  gradient norm: 7.916216204226051e-05\n",
            "iter: 7417  x: [-99.99996086  24.99999466]  f(x): 1.5604015935487839e-09  grad at x: [ 7.82793858e-05 -1.06744617e-05]  gradient norm: 7.900383771814591e-05\n",
            "iter: 7418  x: [-99.99996094  24.99999467]  f(x): 1.554166228857667e-09  grad at x: [ 7.81228270e-05 -1.06531128e-05]  gradient norm: 7.88458300446553e-05\n",
            "iter: 7419  x: [-99.99996102  24.99999468]  f(x): 1.547955780849445e-09  grad at x: [ 7.79665814e-05 -1.06318065e-05]  gradient norm: 7.86881383907243e-05\n",
            "iter: 7420  x: [-99.99996109  24.99999469]  f(x): 1.5417701500323607e-09  grad at x: [ 7.78106482e-05 -1.06105429e-05]  gradient norm: 7.853076212624861e-05\n",
            "iter: 7421  x: [-99.99996117  24.99999471]  f(x): 1.535609236083603e-09  grad at x: [ 7.76550269e-05 -1.05893218e-05]  gradient norm: 7.837370059104273e-05\n",
            "iter: 7422  x: [-99.99996125  24.99999472]  f(x): 1.5294729412767493e-09  grad at x: [ 7.74997169e-05 -1.05681432e-05]  gradient norm: 7.821695318220339e-05\n",
            "iter: 7423  x: [-99.99996133  24.99999473]  f(x): 1.5233611671234528e-09  grad at x: [ 7.73447174e-05 -1.05470069e-05]  gradient norm: 7.806051926866623e-05\n",
            "iter: 7424  x: [-99.9999614   24.99999474]  f(x): 1.5172738154804232e-09  grad at x: [ 7.71900280e-05 -1.05259129e-05]  gradient norm: 7.790439821936687e-05\n",
            "iter: 7425  x: [-99.99996148  24.99999475]  f(x): 1.5112107896805021e-09  grad at x: [ 7.70356479e-05 -1.05048611e-05]  gradient norm: 7.77485894323621e-05\n",
            "iter: 7426  x: [-99.99996156  24.99999476]  f(x): 1.5051719911309588e-09  grad at x: [ 7.68815766e-05 -1.04838513e-05]  gradient norm: 7.759309224746643e-05\n",
            "iter: 7427  x: [-99.99996164  24.99999477]  f(x): 1.4991573238453049e-09  grad at x: [ 7.67278135e-05 -1.04628836e-05]  gradient norm: 7.743790606273661e-05\n",
            "iter: 7428  x: [-99.99996171  24.99999478]  f(x): 1.4931666910438633e-09  grad at x: [ 7.65743578e-05 -1.04419579e-05]  gradient norm: 7.728303024710828e-05\n",
            "iter: 7429  x: [-99.99996179  24.99999479]  f(x): 1.4871999973731308e-09  grad at x: [ 7.64212091e-05 -1.04210740e-05]  gradient norm: 7.712846419767816e-05\n",
            "iter: 7430  x: [-99.99996187  24.9999948 ]  f(x): 1.4812571456424227e-09  grad at x: [ 7.62683667e-05 -1.04002318e-05]  gradient norm: 7.697420725522082e-05\n",
            "iter: 7431  x: [-99.99996194  24.99999481]  f(x): 1.4753380422898478e-09  grad at x: [ 7.61158300e-05 -1.03794313e-05]  gradient norm: 7.682025884595411e-05\n",
            "iter: 7432  x: [-99.99996202  24.99999482]  f(x): 1.4694425918773294e-09  grad at x: [ 7.59635983e-05 -1.03586725e-05]  gradient norm: 7.666661833881365e-05\n",
            "iter: 7433  x: [-99.99996209  24.99999483]  f(x): 1.4635706993031057e-09  grad at x: [ 7.58116711e-05 -1.03379551e-05]  gradient norm: 7.651328510273508e-05\n",
            "iter: 7434  x: [-99.99996217  24.99999484]  f(x): 1.4577222708759524e-09  grad at x: [ 7.56600478e-05 -1.03172792e-05]  gradient norm: 7.636025853481515e-05\n",
            "iter: 7435  x: [-99.99996225  24.99999485]  f(x): 1.4518972121939556e-09  grad at x: [ 7.55087277e-05 -1.02966447e-05]  gradient norm: 7.620753800494951e-05\n",
            "iter: 7436  x: [-99.99996232  24.99999486]  f(x): 1.446095431293631e-09  grad at x: [ 7.53577102e-05 -1.02760514e-05]  gradient norm: 7.605512293839597e-05\n",
            "iter: 7437  x: [-99.9999624   24.99999487]  f(x): 1.4403168344257538e-09  grad at x: [ 7.52069948e-05 -1.02554993e-05]  gradient norm: 7.590301270505022e-05\n",
            "iter: 7438  x: [-99.99996247  24.99999488]  f(x): 1.4345613281360043e-09  grad at x: [ 7.50565808e-05 -1.02349883e-05]  gradient norm: 7.575120667384789e-05\n",
            "iter: 7439  x: [-99.99996255  24.99999489]  f(x): 1.4288288214296335e-09  grad at x: [ 7.49064677e-05 -1.02145183e-05]  gradient norm: 7.559970427004681e-05\n",
            "iter: 7440  x: [-99.99996262  24.9999949 ]  f(x): 1.423119221536004e-09  grad at x: [ 7.47566548e-05 -1.01940893e-05]  gradient norm: 7.544850486354263e-05\n",
            "iter: 7441  x: [-99.9999627   24.99999491]  f(x): 1.417432437037012e-09  grad at x: [ 7.46071414e-05 -1.01737011e-05]  gradient norm: 7.529760785143209e-05\n",
            "iter: 7442  x: [-99.99996277  24.99999492]  f(x): 1.4117683768699235e-09  grad at x: [ 7.44579272e-05 -1.01533537e-05]  gradient norm: 7.514701263177196e-05\n",
            "iter: 7443  x: [-99.99996285  24.99999493]  f(x): 1.4061269502541043e-09  grad at x: [ 7.43090113e-05 -1.01330470e-05]  gradient norm: 7.499671860165894e-05\n",
            "iter: 7444  x: [-99.99996292  24.99999494]  f(x): 1.4005080667623279e-09  grad at x: [ 7.41603933e-05 -1.01127809e-05]  gradient norm: 7.484672515914983e-05\n",
            "iter: 7445  x: [-99.99996299  24.99999495]  f(x): 1.3949116362477974e-09  grad at x: [ 7.40120725e-05 -1.00925553e-05]  gradient norm: 7.469703170134132e-05\n",
            "iter: 7446  x: [-99.99996307  24.99999496]  f(x): 1.389337568915169e-09  grad at x: [ 7.38640483e-05 -1.00723702e-05]  gradient norm: 7.454763762629019e-05\n",
            "iter: 7447  x: [-99.99996314  24.99999497]  f(x): 1.3837857763311496e-09  grad at x: [ 7.37163202e-05 -1.00522255e-05]  gradient norm: 7.439854236021428e-05\n",
            "iter: 7448  x: [-99.99996322  24.99999498]  f(x): 1.3782561682410495e-09  grad at x: [ 7.35688876e-05 -1.00321210e-05]  gradient norm: 7.424974527204923e-05\n",
            "iter: 7449  x: [-99.99996329  24.99999499]  f(x): 1.372748656833875e-09  grad at x: [ 7.34217498e-05 -1.00120568e-05]  gradient norm: 7.410124578801291e-05\n",
            "iter: 7450  x: [-99.99996336  24.999995  ]  f(x): 1.36726315356077e-09  grad at x: [ 7.32749063e-05 -9.99203267e-06]  gradient norm: 7.395304330616206e-05\n",
            "iter: 7451  x: [-99.99996344  24.99999501]  f(x): 1.3617995701483635e-09  grad at x: [ 7.31283565e-05 -9.97204861e-06]  gradient norm: 7.380513722359341e-05\n",
            "iter: 7452  x: [-99.99996351  24.99999502]  f(x): 1.3563578186689447e-09  grad at x: [ 7.29820998e-05 -9.95210451e-06]  gradient norm: 7.365752693836373e-05\n",
            "iter: 7453  x: [-99.99996358  24.99999503]  f(x): 1.3509378125389954e-09  grad at x: [ 7.28361356e-05 -9.93220031e-06]  gradient norm: 7.351021187669085e-05\n",
            "iter: 7454  x: [-99.99996365  24.99999504]  f(x): 1.3455394654397407e-09  grad at x: [ 7.26904633e-05 -9.91233590e-06]  gradient norm: 7.336319146383262e-05\n",
            "iter: 7455  x: [-99.99996373  24.99999505]  f(x): 1.3401626893250893e-09  grad at x: [ 7.25450824e-05 -9.89251123e-06]  gradient norm: 7.321646506968468e-05\n",
            "iter: 7456  x: [-99.9999638   24.99999506]  f(x): 1.3348073995853836e-09  grad at x: [ 7.23999922e-05 -9.87272621e-06]  gradient norm: 7.307003214958602e-05\n",
            "iter: 7457  x: [-99.99996387  24.99999507]  f(x): 1.329473508749396e-09  grad at x: [ 7.22551922e-05 -9.85298076e-06]  gradient norm: 7.292389207247227e-05\n",
            "iter: 7458  x: [-99.99996394  24.99999508]  f(x): 1.3241609327334302e-09  grad at x: [ 7.21106819e-05 -9.83327480e-06]  gradient norm: 7.277804429176234e-05\n",
            "iter: 7459  x: [-99.99996402  24.99999509]  f(x): 1.3188695857658606e-09  grad at x: [ 7.19664605e-05 -9.81360824e-06]  gradient norm: 7.263248820647302e-05\n",
            "iter: 7460  x: [-99.99996409  24.9999951 ]  f(x): 1.3135993833636847e-09  grad at x: [ 7.18225276e-05 -9.79398103e-06]  gradient norm: 7.248722324282218e-05\n",
            "iter: 7461  x: [-99.99996416  24.99999511]  f(x): 1.3083502402845511e-09  grad at x: [ 7.16788825e-05 -9.77439306e-06]  gradient norm: 7.23422487979065e-05\n",
            "iter: 7462  x: [-99.99996423  24.99999512]  f(x): 1.3031220726735913e-09  grad at x: [ 7.15355248e-05 -9.75484428e-06]  gradient norm: 7.219756429890391e-05\n",
            "iter: 7463  x: [-99.9999643   24.99999513]  f(x): 1.297914796898788e-09  grad at x: [ 7.13924537e-05 -9.73533459e-06]  gradient norm: 7.205316917107222e-05\n",
            "iter: 7464  x: [-99.99996438  24.99999514]  f(x): 1.2927283296889095e-09  grad at x: [ 7.12496688e-05 -9.71586392e-06]  gradient norm: 7.190906284158929e-05\n",
            "iter: 7465  x: [-99.99996445  24.99999515]  f(x): 1.2875625869837006e-09  grad at x: [ 7.11071695e-05 -9.69643219e-06]  gradient norm: 7.176524470755189e-05\n",
            "iter: 7466  x: [-99.99996452  24.99999516]  f(x): 1.2824174871067859e-09  grad at x: [ 7.09649551e-05 -9.67703932e-06]  gradient norm: 7.162171422429892e-05\n",
            "iter: 7467  x: [-99.99996459  24.99999517]  f(x): 1.2772929466151901e-09  grad at x: [ 7.08230252e-05 -9.65768525e-06]  gradient norm: 7.147847078988721e-05\n",
            "iter: 7468  x: [-99.99996466  24.99999518]  f(x): 1.2721888843367987e-09  grad at x: [ 7.06813792e-05 -9.63836987e-06]  gradient norm: 7.133551385773565e-05\n",
            "iter: 7469  x: [-99.99996473  24.99999519]  f(x): 1.2671052174425341e-09  grad at x: [ 7.05400164e-05 -9.61909313e-06]  gradient norm: 7.119284282686102e-05\n",
            "iter: 7470  x: [-99.9999648  24.9999952]  f(x): 1.2620418653643382e-09  grad at x: [ 7.03989364e-05 -9.59985495e-06]  gradient norm: 7.105045715164226e-05\n",
            "iter: 7471  x: [-99.99996487  24.99999521]  f(x): 1.2569987458149716e-09  grad at x: [ 7.02581385e-05 -9.58065524e-06]  gradient norm: 7.090835623013613e-05\n",
            "iter: 7472  x: [-99.99996494  24.99999522]  f(x): 1.2519757787929295e-09  grad at x: [ 7.01176222e-05 -9.56149393e-06]  gradient norm: 7.076653951672159e-05\n",
            "iter: 7473  x: [-99.99996501  24.99999523]  f(x): 1.246972883612243e-09  grad at x: [ 6.99773870e-05 -9.54237094e-06]  gradient norm: 7.06250064385765e-05\n",
            "iter: 7474  x: [-99.99996508  24.99999524]  f(x): 1.2419899798022584e-09  grad at x: [ 6.98374322e-05 -9.52328620e-06]  gradient norm: 7.048375642095868e-05\n",
            "iter: 7475  x: [-99.99996515  24.99999525]  f(x): 1.2370269882330316e-09  grad at x: [ 6.96977574e-05 -9.50423962e-06]  gradient norm: 7.034278891920711e-05\n",
            "iter: 7476  x: [-99.99996522  24.99999526]  f(x): 1.232083828037388e-09  grad at x: [ 6.95583618e-05 -9.48523114e-06]  gradient norm: 7.020210333137856e-05\n",
            "iter: 7477  x: [-99.99996529  24.99999527]  f(x): 1.2271604206435496e-09  grad at x: [ 6.94192451e-05 -9.46626068e-06]  gradient norm: 7.006169911281197e-05\n",
            "iter: 7478  x: [-99.99996536  24.99999528]  f(x): 1.2222566876844638e-09  grad at x: [ 6.92804066e-05 -9.44732815e-06]  gradient norm: 6.992157571692628e-05\n",
            "iter: 7479  x: [-99.99996543  24.99999529]  f(x): 1.2173725501826257e-09  grad at x: [ 6.91418458e-05 -9.42843350e-06]  gradient norm: 6.978173257185939e-05\n",
            "iter: 7480  x: [-99.9999655  24.9999953]  f(x): 1.2125079293381549e-09  grad at x: [ 6.90035621e-05 -9.40957663e-06]  gradient norm: 6.964216910286913e-05\n",
            "iter: 7481  x: [-99.99996557  24.9999953 ]  f(x): 1.20766274767445e-09  grad at x: [ 6.88655550e-05 -9.39075748e-06]  gradient norm: 6.950288476529445e-05\n",
            "iter: 7482  x: [-99.99996564  24.99999531]  f(x): 1.2028369269736817e-09  grad at x: [ 6.87278239e-05 -9.37197597e-06]  gradient norm: 6.936387898535322e-05\n",
            "iter: 7483  x: [-99.9999657   24.99999532]  f(x): 1.198030390268818e-09  grad at x: [ 6.85903682e-05 -9.35323202e-06]  gradient norm: 6.922515121742438e-05\n",
            "iter: 7484  x: [-99.99996577  24.99999533]  f(x): 1.1932430608934627e-09  grad at x: [ 6.84531875e-05 -9.33452555e-06]  gradient norm: 6.908670091684687e-05\n",
            "iter: 7485  x: [-99.99996584  24.99999534]  f(x): 1.1884748614768384e-09  grad at x: [ 6.83162811e-05 -9.31585650e-06]  gradient norm: 6.894852751079861e-05\n",
            "iter: 7486  x: [-99.99996591  24.99999535]  f(x): 1.18372571585749e-09  grad at x: [ 6.81796485e-05 -9.29722479e-06]  gradient norm: 6.881063045365854e-05\n",
            "iter: 7487  x: [-99.99996598  24.99999536]  f(x): 1.1789955481391453e-09  grad at x: [ 6.80432892e-05 -9.27863034e-06]  gradient norm: 6.867300919980557e-05\n",
            "iter: 7488  x: [-99.99996605  24.99999537]  f(x): 1.1742842817578411e-09  grad at x: [ 6.79072027e-05 -9.26007309e-06]  gradient norm: 6.853566317641761e-05\n",
            "iter: 7489  x: [-99.99996611  24.99999538]  f(x): 1.1695918413510068e-09  grad at x: [ 6.77713882e-05 -9.24155294e-06]  gradient norm: 6.839859183787358e-05\n",
            "iter: 7490  x: [-99.99996618  24.99999539]  f(x): 1.1649181528129066e-09  grad at x: [ 6.76358455e-05 -9.22306983e-06]  gradient norm: 6.826179466767356e-05\n",
            "iter: 7491  x: [-99.99996625  24.9999954 ]  f(x): 1.1602631394142883e-09  grad at x: [ 6.75005738e-05 -9.20462369e-06]  gradient norm: 6.812527106483433e-05\n",
            "iter: 7492  x: [-99.99996632  24.99999541]  f(x): 1.1556267275417513e-09  grad at x: [ 6.73655726e-05 -9.18621445e-06]  gradient norm: 6.798902051189592e-05\n",
            "iter: 7493  x: [-99.99996638  24.99999542]  f(x): 1.151008842912041e-09  grad at x: [ 6.72308415e-05 -9.16784202e-06]  gradient norm: 6.785304246419732e-05\n",
            "iter: 7494  x: [-99.99996645  24.99999543]  f(x): 1.1464094114690596e-09  grad at x: [ 6.70963798e-05 -9.14950634e-06]  gradient norm: 6.771733637611745e-05\n",
            "iter: 7495  x: [-99.99996652  24.99999543]  f(x): 1.1418283594483288e-09  grad at x: [ 6.69621870e-05 -9.13120733e-06]  gradient norm: 6.758190170299527e-05\n",
            "iter: 7496  x: [-99.99996659  24.99999544]  f(x): 1.1372656133435497e-09  grad at x: [ 6.68282626e-05 -9.11294492e-06]  gradient norm: 6.744673790016978e-05\n",
            "iter: 7497  x: [-99.99996665  24.99999545]  f(x): 1.1327210998735532e-09  grad at x: [ 6.66946061e-05 -9.09471903e-06]  gradient norm: 6.73118444220199e-05\n",
            "iter: 7498  x: [-99.99996672  24.99999546]  f(x): 1.1281947460463787e-09  grad at x: [ 6.65612169e-05 -9.07652959e-06]  gradient norm: 6.717722072388463e-05\n",
            "iter: 7499  x: [-99.99996679  24.99999547]  f(x): 1.1236864800700332e-09  grad at x: [ 6.64280945e-05 -9.05837653e-06]  gradient norm: 6.704286628926401e-05\n",
            "iter: 7500  x: [-99.99996685  24.99999548]  f(x): 1.1191962285159807e-09  grad at x: [ 6.62952383e-05 -9.04025978e-06]  gradient norm: 6.690878054533592e-05\n",
            "iter: 7501  x: [-99.99996692  24.99999549]  f(x): 1.114723920066149e-09  grad at x: [ 6.61626478e-05 -9.02217926e-06]  gradient norm: 6.67749629746404e-05\n",
            "iter: 7502  x: [-99.99996698  24.9999955 ]  f(x): 1.1102694836809594e-09  grad at x: [ 6.60303225e-05 -9.00413490e-06]  gradient norm: 6.66414130606775e-05\n",
            "iter: 7503  x: [-99.99996705  24.99999551]  f(x): 1.1058328466934474e-09  grad at x: [ 6.58982619e-05 -8.98612662e-06]  gradient norm: 6.65081302306251e-05\n",
            "iter: 7504  x: [-99.99996712  24.99999552]  f(x): 1.101413938565693e-09  grad at x: [ 6.57664653e-05 -8.96815437e-06]  gradient norm: 6.63751139679833e-05\n",
            "iter: 7505  x: [-99.99996718  24.99999552]  f(x): 1.0970126880711775e-09  grad at x: [ 6.56349324e-05 -8.95021806e-06]  gradient norm: 6.624236372809103e-05\n",
            "iter: 7506  x: [-99.99996725  24.99999553]  f(x): 1.0926290251651514e-09  grad at x: [ 6.55036625e-05 -8.93231763e-06]  gradient norm: 6.610987899444837e-05\n",
            "iter: 7507  x: [-99.99996731  24.99999554]  f(x): 1.0882628800139284e-09  grad at x: [ 6.53726552e-05 -8.91445299e-06]  gradient norm: 6.597765924959534e-05\n",
            "iter: 7508  x: [-99.99996738  24.99999555]  f(x): 1.0839141812350525e-09  grad at x: [ 6.52419099e-05 -8.89662409e-06]  gradient norm: 6.584570392166986e-05\n",
            "iter: 7509  x: [-99.99996744  24.99999556]  f(x): 1.0795828604147937e-09  grad at x: [ 6.51114261e-05 -8.87883084e-06]  gradient norm: 6.571401252137306e-05\n",
            "iter: 7510  x: [-99.99996751  24.99999557]  f(x): 1.0752688475575145e-09  grad at x: [ 6.49812032e-05 -8.86107318e-06]  gradient norm: 6.558258450404389e-05\n",
            "iter: 7511  x: [-99.99996757  24.99999558]  f(x): 1.0709720729463621e-09  grad at x: [ 6.48512408e-05 -8.84335103e-06]  gradient norm: 6.545141932598138e-05\n",
            "iter: 7512  x: [-99.99996764  24.99999559]  f(x): 1.0666924688876696e-09  grad at x: [ 6.47215383e-05 -8.82566433e-06]  gradient norm: 6.532051649788662e-05\n",
            "iter: 7513  x: [-99.9999677  24.9999956]  f(x): 1.0624299661147154e-09  grad at x: [ 6.45920953e-05 -8.80801300e-06]  gradient norm: 6.51898754750986e-05\n",
            "iter: 7514  x: [-99.99996777  24.9999956 ]  f(x): 1.0581844956372742e-09  grad at x: [ 6.44629111e-05 -8.79039698e-06]  gradient norm: 6.505949571391633e-05\n",
            "iter: 7515  x: [-99.99996783  24.99999561]  f(x): 1.053955990475571e-09  grad at x: [ 6.43339853e-05 -8.77281618e-06]  gradient norm: 6.492937672504091e-05\n",
            "iter: 7516  x: [-99.9999679   24.99999562]  f(x): 1.0497443821166818e-09  grad at x: [ 6.42053173e-05 -8.75527055e-06]  gradient norm: 6.479951796477136e-05\n",
            "iter: 7517  x: [-99.99996796  24.99999563]  f(x): 1.0455496031391126e-09  grad at x: [ 6.40769066e-05 -8.73776001e-06]  gradient norm: 6.466991891564772e-05\n",
            "iter: 7518  x: [-99.99996803  24.99999564]  f(x): 1.0413715873275314e-09  grad at x: [ 6.39487528e-05 -8.72028448e-06]  gradient norm: 6.454057909029114e-05\n",
            "iter: 7519  x: [-99.99996809  24.99999565]  f(x): 1.0372102668487872e-09  grad at x: [ 6.38208553e-05 -8.70284391e-06]  gradient norm: 6.441149794404062e-05\n",
            "iter: 7520  x: [-99.99996815  24.99999566]  f(x): 1.0330655750159874e-09  grad at x: [ 6.36932136e-05 -8.68543822e-06]  gradient norm: 6.428267496039621e-05\n",
            "iter: 7521  x: [-99.99996822  24.99999567]  f(x): 1.0289374453754195e-09  grad at x: [ 6.35658272e-05 -8.66806734e-06]  gradient norm: 6.415410962285797e-05\n",
            "iter: 7522  x: [-99.99996828  24.99999567]  f(x): 1.0248258117366177e-09  grad at x: [ 6.34386956e-05 -8.65073121e-06]  gradient norm: 6.4025801415886e-05\n",
            "iter: 7523  x: [-99.99996834  24.99999568]  f(x): 1.0207306081100475e-09  grad at x: [ 6.33118182e-05 -8.63342975e-06]  gradient norm: 6.389774982298039e-05\n",
            "iter: 7524  x: [-99.99996841  24.99999569]  f(x): 1.0166517687373595e-09  grad at x: [ 6.31851946e-05 -8.61616289e-06]  gradient norm: 6.376995432764114e-05\n",
            "iter: 7525  x: [-99.99996847  24.9999957 ]  f(x): 1.0125892280907296e-09  grad at x: [ 6.30588242e-05 -8.59893057e-06]  gradient norm: 6.364241441336838e-05\n",
            "iter: 7526  x: [-99.99996853  24.99999571]  f(x): 1.0085429217665235e-09  grad at x: [ 6.29327065e-05 -8.58173271e-06]  gradient norm: 6.351512959182318e-05\n",
            "iter: 7527  x: [-99.9999686   24.99999572]  f(x): 1.0045127838285073e-09  grad at x: [ 6.28068411e-05 -8.56456924e-06]  gradient norm: 6.338809931930464e-05\n",
            "iter: 7528  x: [-99.99996866  24.99999573]  f(x): 1.000498750327051e-09  grad at x: [ 6.26812274e-05 -8.54744010e-06]  gradient norm: 6.326132310747384e-05\n",
            "iter: 7529  x: [-99.99996872  24.99999573]  f(x): 9.96500757533289e-10  grad at x: [ 6.25558650e-05 -8.53034522e-06]  gradient norm: 6.313480046799195e-05\n",
            "iter: 7530  x: [-99.99996878  24.99999574]  f(x): 9.925187401943574e-10  grad at x: [ 6.24307532e-05 -8.51328453e-06]  gradient norm: 6.300853085715799e-05\n",
            "iter: 7531  x: [-99.99996885  24.99999575]  f(x): 9.88552635031525e-10  grad at x: [ 6.23058917e-05 -8.49625795e-06]  gradient norm: 6.288251378663308e-05\n",
            "iter: 7532  x: [-99.99996891  24.99999576]  f(x): 9.84602379015052e-10  grad at x: [ 6.21812799e-05 -8.47926544e-06]  gradient norm: 6.275674876903844e-05\n",
            "iter: 7533  x: [-99.99996897  24.99999577]  f(x): 9.806679075393801e-10  grad at x: [ 6.20569174e-05 -8.46230691e-06]  gradient norm: 6.263123525971303e-05\n",
            "iter: 7534  x: [-99.99996903  24.99999578]  f(x): 9.767491588710518e-10  grad at x: [ 6.19328035e-05 -8.44538229e-06]  gradient norm: 6.250597279847909e-05\n",
            "iter: 7535  x: [-99.9999691   24.99999579]  f(x): 9.728460688814188e-10  grad at x: [ 6.18089379e-05 -8.42849153e-06]  gradient norm: 6.238096084163561e-05\n",
            "iter: 7536  x: [-99.99996916  24.99999579]  f(x): 9.689585763018403e-10  grad at x: [ 6.16853200e-05 -8.41163455e-06]  gradient norm: 6.225619892996489e-05\n",
            "iter: 7537  x: [-99.99996922  24.9999958 ]  f(x): 9.65086617387967e-10  grad at x: [ 6.15619494e-05 -8.39481127e-06]  gradient norm: 6.213168651784585e-05\n",
            "iter: 7538  x: [-99.99996928  24.99999581]  f(x): 9.612301313334228e-10  grad at x: [ 6.14388255e-05 -8.37802165e-06]  gradient norm: 6.20074231470208e-05\n",
            "iter: 7539  x: [-99.99996934  24.99999582]  f(x): 9.573890557367536e-10  grad at x: [ 6.13159478e-05 -8.36126561e-06]  gradient norm: 6.188340830098981e-05\n",
            "iter: 7540  x: [-99.9999694   24.99999583]  f(x): 9.53563329286997e-10  grad at x: [ 6.11933160e-05 -8.34454308e-06]  gradient norm: 6.175964149141403e-05\n",
            "iter: 7541  x: [-99.99996946  24.99999584]  f(x): 9.497528900481918e-10  grad at x: [ 6.10709293e-05 -8.32785400e-06]  gradient norm: 6.163612220275354e-05\n",
            "iter: 7542  x: [-99.99996953  24.99999584]  f(x): 9.459576771403693e-10  grad at x: [ 6.09487874e-05 -8.31119829e-06]  gradient norm: 6.15128499466695e-05\n",
            "iter: 7543  x: [-99.99996959  24.99999585]  f(x): 9.421776299545536e-10  grad at x: [ 6.08268899e-05 -8.29457589e-06]  gradient norm: 6.138982423674313e-05\n",
            "iter: 7544  x: [-99.99996965  24.99999586]  f(x): 9.384126880045218e-10  grad at x: [ 6.07052361e-05 -8.27798674e-06]  gradient norm: 6.126704458367555e-05\n",
            "iter: 7545  x: [-99.99996971  24.99999587]  f(x): 9.346627911031913e-10  grad at x: [ 6.05838256e-05 -8.26143076e-06]  gradient norm: 6.114451050104797e-05\n",
            "iter: 7546  x: [-99.99996977  24.99999588]  f(x): 9.309278783554523e-10  grad at x: [ 6.0462658e-05 -8.2449079e-06]  gradient norm: 6.1022221472360456e-05\n",
            "iter: 7547  x: [-99.99996983  24.99999589]  f(x): 9.27207890826972e-10  grad at x: [ 6.03417326e-05 -8.22841809e-06]  gradient norm: 6.090017703839528e-05\n",
            "iter: 7548  x: [-99.99996989  24.99999589]  f(x): 9.235027680445994e-10  grad at x: [ 6.02210492e-05 -8.21196125e-06]  gradient norm: 6.0778376682652504e-05\n",
            "iter: 7549  x: [-99.99996995  24.9999959 ]  f(x): 9.198124506336888e-10  grad at x: [ 6.01006071e-05 -8.19553733e-06]  gradient norm: 6.065681991775332e-05\n",
            "iter: 7550  x: [-99.99997001  24.99999591]  f(x): 9.161368802795789e-10  grad at x: [ 5.99804059e-05 -8.17914625e-06]  gradient norm: 6.0535506284479986e-05\n",
            "iter: 7551  x: [-99.99997007  24.99999592]  f(x): 9.124759971664721e-10  grad at x: [ 5.98604450e-05 -8.16278796e-06]  gradient norm: 6.041443526729261e-05\n",
            "iter: 7552  x: [-99.99997013  24.99999593]  f(x): 9.088297433608155e-10  grad at x: [ 5.97407242e-05 -8.14646238e-06]  gradient norm: 6.0293606406013417e-05\n",
            "iter: 7553  x: [-99.99997019  24.99999593]  f(x): 9.051980594915631e-10  grad at x: [ 5.96212427e-05 -8.13016946e-06]  gradient norm: 6.017301918606256e-05\n",
            "iter: 7554  x: [-99.99997025  24.99999594]  f(x): 9.015808880329783e-10  grad at x: [ 5.95020002e-05 -8.11390912e-06]  gradient norm: 6.0052673147262256e-05\n",
            "iter: 7555  x: [-99.99997031  24.99999595]  f(x): 8.979781708423348e-10  grad at x: [ 5.93829962e-05 -8.09768130e-06]  gradient norm: 5.99325678022337e-05\n",
            "iter: 7556  x: [-99.99997037  24.99999596]  f(x): 8.943898499522868e-10  grad at x: [ 5.92642302e-05 -8.08148594e-06]  gradient norm: 5.981270266263804e-05\n",
            "iter: 7557  x: [-99.99997043  24.99999597]  f(x): 8.908158684970416e-10  grad at x: [ 5.91457018e-05 -8.06532297e-06]  gradient norm: 5.969307727021758e-05\n",
            "iter: 7558  x: [-99.99997049  24.99999598]  f(x): 8.872561680719204e-10  grad at x: [ 5.90274104e-05 -8.04919232e-06]  gradient norm: 5.9573691108472385e-05\n",
            "iter: 7559  x: [-99.99997055  24.99999598]  f(x): 8.83710692212924e-10  grad at x: [ 5.89093555e-05 -8.03309393e-06]  gradient norm: 5.9454543719144764e-05\n",
            "iter: 7560  x: [-99.9999706   24.99999599]  f(x): 8.801793845939173e-10  grad at x: [ 5.87915368e-05 -8.01702775e-06]  gradient norm: 5.933563464205695e-05\n",
            "iter: 7561  x: [-99.99997066  24.999996  ]  f(x): 8.766621874443137e-10  grad at x: [ 5.86739538e-05 -8.00099369e-06]  gradient norm: 5.921696336166905e-05\n",
            "iter: 7562  x: [-99.99997072  24.99999601]  f(x): 8.731590456972481e-10  grad at x: [ 5.85566059e-05 -7.98499170e-06]  gradient norm: 5.9098529446924415e-05\n",
            "iter: 7563  x: [-99.99997078  24.99999602]  f(x): 8.696699020094611e-10  grad at x: [ 5.84394926e-05 -7.96902172e-06]  gradient norm: 5.8980332383243184e-05\n",
            "iter: 7564  x: [-99.99997084  24.99999602]  f(x): 8.66194700844725e-10  grad at x: [ 5.83226137e-05 -7.95308367e-06]  gradient norm: 5.8862371710447585e-05\n",
            "iter: 7565  x: [-99.9999709   24.99999603]  f(x): 8.627333868875064e-10  grad at x: [ 5.82059684e-05 -7.93717751e-06]  gradient norm: 5.87446469693199e-05\n",
            "iter: 7566  x: [-99.99997096  24.99999604]  f(x): 8.59285904188496e-10  grad at x: [ 5.80895565e-05 -7.92130315e-06]  gradient norm: 5.8627157672481306e-05\n",
            "iter: 7567  x: [-99.99997101  24.99999605]  f(x): 8.558521978200253e-10  grad at x: [ 5.79733774e-05 -7.90546054e-06]  gradient norm: 5.8509903360714085e-05\n",
            "iter: 7568  x: [-99.99997107  24.99999606]  f(x): 8.524322122508776e-10  grad at x: [ 5.78574306e-05 -7.88964962e-06]  gradient norm: 5.839288354759945e-05\n",
            "iter: 7569  x: [-99.99997113  24.99999606]  f(x): 8.490258929108879e-10  grad at x: [ 5.77417158e-05 -7.87387032e-06]  gradient norm: 5.827609777295964e-05\n",
            "iter: 7570  x: [-99.99997119  24.99999607]  f(x): 8.456331854754796e-10  grad at x: [ 5.76262323e-05 -7.85812259e-06]  gradient norm: 5.815954557853696e-05\n",
            "iter: 7571  x: [-99.99997124  24.99999608]  f(x): 8.422540349358412e-10  grad at x: [ 5.75109798e-05 -7.84240634e-06]  gradient norm: 5.804322647599257e-05\n",
            "iter: 7572  x: [-99.9999713   24.99999609]  f(x): 8.388883881652823e-10  grad at x: [ 5.73959579e-05 -7.82672153e-06]  gradient norm: 5.7927140035229854e-05\n",
            "iter: 7573  x: [-99.99997136  24.99999609]  f(x): 8.355361905624879e-10  grad at x: [ 5.72811660e-05 -7.81106809e-06]  gradient norm: 5.7811285768870006e-05\n",
            "iter: 7574  x: [-99.99997142  24.9999961 ]  f(x): 8.32197387720043e-10  grad at x: [ 5.71666037e-05 -7.79544595e-06]  gradient norm: 5.76956631895342e-05\n",
            "iter: 7575  x: [-99.99997147  24.99999611]  f(x): 8.288719270453915e-10  grad at x: [ 5.70522704e-05 -7.77985506e-06]  gradient norm: 5.758027186616581e-05\n",
            "iter: 7576  x: [-99.99997153  24.99999612]  f(x): 8.255597545074962e-10  grad at x: [ 5.69381659e-05 -7.76429535e-06]  gradient norm: 5.746511131138601e-05\n",
            "iter: 7577  x: [-99.99997159  24.99999613]  f(x): 8.222608179101399e-10  grad at x: [ 5.68242896e-05 -7.74876676e-06]  gradient norm: 5.7350181095098205e-05\n",
            "iter: 7578  x: [-99.99997164  24.99999613]  f(x): 8.189750635963633e-10  grad at x: [ 5.67106410e-05 -7.73326922e-06]  gradient norm: 5.723548072992358e-05\n",
            "iter: 7579  x: [-99.9999717   24.99999614]  f(x): 8.157024389046659e-10  grad at x: [ 5.65972197e-05 -7.71780268e-06]  gradient norm: 5.712100975664439e-05\n",
            "iter: 7580  x: [-99.99997176  24.99999615]  f(x): 8.124428921877691e-10  grad at x: [ 5.64840253e-05 -7.70236708e-06]  gradient norm: 5.700676774516405e-05\n",
            "iter: 7581  x: [-99.99997181  24.99999616]  f(x): 8.09196370318617e-10  grad at x: [ 5.63710572e-05 -7.68696234e-06]  gradient norm: 5.689275420714371e-05\n",
            "iter: 7582  x: [-99.99997187  24.99999616]  f(x): 8.059628212139086e-10  grad at x: [ 5.62583151e-05 -7.67158841e-06]  gradient norm: 5.677896868432566e-05\n",
            "iter: 7583  x: [-99.99997193  24.99999617]  f(x): 8.027421937706e-10  grad at x: [ 5.61457985e-05 -7.65624524e-06]  gradient norm: 5.666541074661332e-05\n",
            "iter: 7584  x: [-99.99997198  24.99999618]  f(x): 7.995344362376896e-10  grad at x: [ 5.60335069e-05 -7.64093275e-06]  gradient norm: 5.655207993478895e-05\n",
            "iter: 7585  x: [-99.99997204  24.99999619]  f(x): 7.963394962510547e-10  grad at x: [ 5.59214398e-05 -7.62565089e-06]  gradient norm: 5.643897576147373e-05\n",
            "iter: 7586  x: [-99.9999721   24.99999619]  f(x): 7.931573240132864e-10  grad at x: [ 5.58095970e-05 -7.61039959e-06]  gradient norm: 5.6326097823772116e-05\n",
            "iter: 7587  x: [-99.99997215  24.9999962 ]  f(x): 7.899878675472326e-10  grad at x: [ 5.56979778e-05 -7.59517879e-06]  gradient norm: 5.621344563526532e-05\n",
            "iter: 7588  x: [-99.99997221  24.99999621]  f(x): 7.868310758248907e-10  grad at x: [ 5.55865818e-05 -7.57998843e-06]  gradient norm: 5.610101873673564e-05\n",
            "iter: 7589  x: [-99.99997226  24.99999622]  f(x): 7.836868988129807e-10  grad at x: [ 5.54754087e-05 -7.56482845e-06]  gradient norm: 5.598881669808644e-05\n",
            "iter: 7590  x: [-99.99997232  24.99999623]  f(x): 7.805552858370795e-10  grad at x: [ 5.53644578e-05 -7.54969879e-06]  gradient norm: 5.587683906010001e-05\n",
            "iter: 7591  x: [-99.99997237  24.99999623]  f(x): 7.774361872132158e-10  grad at x: [ 5.52537289e-05 -7.53459940e-06]  gradient norm: 5.576508539267974e-05\n",
            "iter: 7592  x: [-99.99997243  24.99999624]  f(x): 7.743295518349096e-10  grad at x: [ 5.51432215e-05 -7.51953020e-06]  gradient norm: 5.565355520844682e-05\n",
            "iter: 7593  x: [-99.99997248  24.99999625]  f(x): 7.712353311256167e-10  grad at x: [ 5.50329350e-05 -7.50449114e-06]  gradient norm: 5.55422481045057e-05\n",
            "iter: 7594  x: [-99.99997254  24.99999626]  f(x): 7.681534743849339e-10  grad at x: [ 5.49228691e-05 -7.48948216e-06]  gradient norm: 5.543116359539763e-05\n",
            "iter: 7595  x: [-99.99997259  24.99999626]  f(x): 7.650839333518978e-10  grad at x: [ 5.48130234e-05 -7.47450319e-06]  gradient norm: 5.532030127726702e-05\n",
            "iter: 7596  x: [-99.99997265  24.99999627]  f(x): 7.620266576500906e-10  grad at x: [ 5.47033974e-05 -7.45955418e-06]  gradient norm: 5.5209660663695103e-05\n",
            "iter: 7597  x: [-99.9999727   24.99999628]  f(x): 7.589815994120485e-10  grad at x: [ 5.45939906e-05 -7.44463507e-06]  gradient norm: 5.509924135274636e-05\n",
            "iter: 7598  x: [-99.99997276  24.99999629]  f(x): 7.559487086097952e-10  grad at x: [ 5.44848026e-05 -7.42974581e-06]  gradient norm: 5.4989042858001997e-05\n",
            "iter: 7599  x: [-99.99997281  24.99999629]  f(x): 7.529279376613019e-10  grad at x: [ 5.43758330e-05 -7.41488631e-06]  gradient norm: 5.487906477560644e-05\n",
            "iter: 7600  x: [-99.99997287  24.9999963 ]  f(x): 7.499192376828399e-10  grad at x: [ 5.42670813e-05 -7.40005654e-06]  gradient norm: 5.476930664826203e-05\n",
            "iter: 7601  x: [-99.99997292  24.99999631]  f(x): 7.469225606808994e-10  grad at x: [ 5.41585472e-05 -7.38525642e-06]  gradient norm: 5.4659768044912134e-05\n",
            "iter: 7602  x: [-99.99997297  24.99999631]  f(x): 7.439378580868712e-10  grad at x: [ 5.40502301e-05 -7.37048591e-06]  gradient norm: 5.455044850729905e-05\n",
            "iter: 7603  x: [-99.99997303  24.99999632]  f(x): 7.409650822709887e-10  grad at x: [ 5.39421296e-05 -7.35574494e-06]  gradient norm: 5.444134760532618e-05\n",
            "iter: 7604  x: [-99.99997308  24.99999633]  f(x): 7.380041857430482e-10  grad at x: [ 5.38342453e-05 -7.34103345e-06]  gradient norm: 5.433246490793688e-05\n",
            "iter: 7605  x: [-99.99997314  24.99999634]  f(x): 7.350551212042585e-10  grad at x: [ 5.37265769e-05 -7.32635139e-06]  gradient norm: 5.422379998503456e-05\n",
            "iter: 7606  x: [-99.99997319  24.99999634]  f(x): 7.321178407325828e-10  grad at x: [ 5.36191237e-05 -7.31169868e-06]  gradient norm: 5.4115352377401475e-05\n",
            "iter: 7607  x: [-99.99997324  24.99999635]  f(x): 7.291922981492741e-10  grad at x: [ 5.35118855e-05 -7.29707529e-06]  gradient norm: 5.400712168406215e-05\n",
            "iter: 7608  x: [-99.9999733   24.99999636]  f(x): 7.262784458634427e-10  grad at x: [ 5.34048617e-05 -7.28248114e-06]  gradient norm: 5.389910744579887e-05\n",
            "iter: 7609  x: [-99.99997335  24.99999637]  f(x): 7.233762372370135e-10  grad at x: [ 5.32980520e-05 -7.26791617e-06]  gradient norm: 5.379130923251501e-05\n",
            "iter: 7610  x: [-99.9999734   24.99999637]  f(x): 7.204856257689632e-10  grad at x: [ 5.31914559e-05 -7.25338034e-06]  gradient norm: 5.3683726613153944e-05\n",
            "iter: 7611  x: [-99.99997346  24.99999638]  f(x): 7.176065651722657e-10  grad at x: [ 5.30850729e-05 -7.23887358e-06]  gradient norm: 5.35763591585791e-05\n",
            "iter: 7612  x: [-99.99997351  24.99999639]  f(x): 7.147390092702497e-10  grad at x: [ 5.29789028e-05 -7.22439583e-06]  gradient norm: 5.346920643773385e-05\n",
            "iter: 7613  x: [-99.99997356  24.9999964 ]  f(x): 7.118829120734948e-10  grad at x: [ 5.28729450e-05 -7.20994704e-06]  gradient norm: 5.336226802052157e-05\n",
            "iter: 7614  x: [-99.99997362  24.9999964 ]  f(x): 7.0903822775355e-10  grad at x: [ 5.27671991e-05 -7.19552714e-06]  gradient norm: 5.325554347684567e-05\n",
            "iter: 7615  x: [-99.99997367  24.99999641]  f(x): 7.062049106424726e-10  grad at x: [ 5.26616647e-05 -7.18113608e-06]  gradient norm: 5.3149032376609554e-05\n",
            "iter: 7616  x: [-99.99997372  24.99999642]  f(x): 7.033829159792396e-10  grad at x: [ 5.25563414e-05 -7.16677381e-06]  gradient norm: 5.304273431787768e-05\n",
            "iter: 7617  x: [-99.99997377  24.99999642]  f(x): 7.005721976656907e-10  grad at x: [ 5.24512287e-05 -7.15244027e-06]  gradient norm: 5.293664884239238e-05\n",
            "iter: 7618  x: [-99.99997383  24.99999643]  f(x): 6.977727112311911e-10  grad at x: [ 5.23463262e-05 -7.13813539e-06]  gradient norm: 5.28307755472581e-05\n",
            "iter: 7619  x: [-99.99997388  24.99999644]  f(x): 6.94984411666254e-10  grad at x: [ 5.22416336e-05 -7.12385912e-06]  gradient norm: 5.272511400333826e-05\n",
            "iter: 7620  x: [-99.99997393  24.99999645]  f(x): 6.922072540689115e-10  grad at x: [ 5.21371503e-05 -7.10961140e-06]  gradient norm: 5.2619663779576225e-05\n",
            "iter: 7621  x: [-99.99997398  24.99999645]  f(x): 6.894411937203888e-10  grad at x: [ 5.20328760e-05 -7.09539218e-06]  gradient norm: 5.25144244458754e-05\n",
            "iter: 7622  x: [-99.99997404  24.99999646]  f(x): 6.866861868223503e-10  grad at x: [ 5.19288103e-05 -7.08120140e-06]  gradient norm: 5.2409395601260286e-05\n",
            "iter: 7623  x: [-99.99997409  24.99999647]  f(x): 6.839421889405402e-10  grad at x: [ 5.18249526e-05 -7.06703899e-06]  gradient norm: 5.2304576814674264e-05\n",
            "iter: 7624  x: [-99.99997414  24.99999647]  f(x): 6.812091558224024e-10  grad at x: [ 5.17213027e-05 -7.05290491e-06]  gradient norm: 5.219996765602073e-05\n",
            "iter: 7625  x: [-99.99997419  24.99999648]  f(x): 6.784870441299118e-10  grad at x: [ 5.16178601e-05 -7.03879910e-06]  gradient norm: 5.209556772432418e-05\n",
            "iter: 7626  x: [-99.99997424  24.99999649]  f(x): 6.757758098925344e-10  grad at x: [ 5.15146244e-05 -7.02472150e-06]  gradient norm: 5.1991376588528005e-05\n",
            "iter: 7627  x: [-99.99997429  24.99999649]  f(x): 6.730754100754605e-10  grad at x: [ 5.14115952e-05 -7.01067206e-06]  gradient norm: 5.18873938476567e-05\n",
            "iter: 7628  x: [-99.99997435  24.9999965 ]  f(x): 6.703858010136409e-10  grad at x: [ 5.13087720e-05 -6.99665072e-06]  gradient norm: 5.178361907065364e-05\n",
            "iter: 7629  x: [-99.9999744   24.99999651]  f(x): 6.67706939246069e-10  grad at x: [ 5.12061544e-05 -6.98265742e-06]  gradient norm: 5.168005182838226e-05\n",
            "iter: 7630  x: [-99.99997445  24.99999652]  f(x): 6.650387821667931e-10  grad at x: [ 5.11037421e-05 -6.96869211e-06]  gradient norm: 5.157669171890702e-05\n",
            "iter: 7631  x: [-99.9999745   24.99999652]  f(x): 6.623812873173889e-10  grad at x: [ 5.10015346e-05 -6.95475472e-06]  gradient norm: 5.1473538340292436e-05\n",
            "iter: 7632  x: [-99.99997455  24.99999653]  f(x): 6.597344116632177e-10  grad at x: [ 5.08995316e-05 -6.94084522e-06]  gradient norm: 5.1370591262441884e-05\n",
            "iter: 7633  x: [-99.9999746   24.99999654]  f(x): 6.570981130440033e-10  grad at x: [ 5.07977325e-05 -6.92696353e-06]  gradient norm: 5.126785008341986e-05\n",
            "iter: 7634  x: [-99.99997465  24.99999654]  f(x): 6.544723487252994e-10  grad at x: [ 5.0696137e-05 -6.9131096e-06]  gradient norm: 5.116531437312975e-05\n",
            "iter: 7635  x: [-99.9999747   24.99999655]  f(x): 6.518570775867745e-10  grad at x: [ 5.05947448e-05 -6.89928338e-06]  gradient norm: 5.106298375875716e-05\n",
            "iter: 7636  x: [-99.99997475  24.99999656]  f(x): 6.49252256445977e-10  grad at x: [ 5.04935553e-05 -6.88548482e-06]  gradient norm: 5.0960857781084376e-05\n",
            "iter: 7637  x: [-99.9999748   24.99999656]  f(x): 6.466578444743508e-10  grad at x: [ 5.03925682e-05 -6.87171384e-06]  gradient norm: 5.0858936067297e-05\n",
            "iter: 7638  x: [-99.99997485  24.99999657]  f(x): 6.440737995526187e-10  grad at x: [ 5.02917830e-05 -6.85797042e-06]  gradient norm: 5.075721818825846e-05\n",
            "iter: 7639  x: [-99.9999749   24.99999658]  f(x): 6.415000803758445e-10  grad at x: [ 5.01911994e-05 -6.84425448e-06]  gradient norm: 5.06557037410732e-05\n",
            "iter: 7640  x: [-99.99997495  24.99999658]  f(x): 6.389366458315837e-10  grad at x: [ 5.00908170e-05 -6.83056597e-06]  gradient norm: 5.055439232476576e-05\n",
            "iter: 7641  x: [-99.999975    24.99999659]  f(x): 6.363834549263345e-10  grad at x: [ 4.99906354e-05 -6.81690484e-06]  gradient norm: 5.0453283537400596e-05\n",
            "iter: 7642  x: [-99.99997505  24.9999966 ]  f(x): 6.338404668095427e-10  grad at x: [ 4.98906541e-05 -6.80327103e-06]  gradient norm: 5.0352376977042215e-05\n",
            "iter: 7643  x: [-99.9999751   24.99999661]  f(x): 6.31307640065624e-10  grad at x: [ 4.97908728e-05 -6.78966449e-06]  gradient norm: 5.0251672213594e-05\n",
            "iter: 7644  x: [-99.99997515  24.99999661]  f(x): 6.287849348631741e-10  grad at x: [ 4.96912911e-05 -6.77608516e-06]  gradient norm: 5.015116887424157e-05\n",
            "iter: 7645  x: [-99.9999752   24.99999662]  f(x): 6.262723100971736e-10  grad at x: [ 4.95919085e-05 -6.76253299e-06]  gradient norm: 5.0050866529848354e-05\n",
            "iter: 7646  x: [-99.99997525  24.99999663]  f(x): 6.237697261680869e-10  grad at x: [ 4.94927247e-05 -6.74900792e-06]  gradient norm: 4.995076480567988e-05\n",
            "iter: 7647  x: [-99.9999753   24.99999663]  f(x): 6.212771422558546e-10  grad at x: [ 4.93937392e-05 -6.73550991e-06]  gradient norm: 4.985086327259959e-05\n",
            "iter: 7648  x: [-99.99997535  24.99999664]  f(x): 6.18794519087407e-10  grad at x: [ 4.92949518e-05 -6.72203889e-06]  gradient norm: 4.9751161557793075e-05\n",
            "iter: 7649  x: [-99.9999754   24.99999665]  f(x): 6.163218160780686e-10  grad at x: [ 4.91963619e-05 -6.70859481e-06]  gradient norm: 4.9651659230203727e-05\n",
            "iter: 7650  x: [-99.99997545  24.99999665]  f(x): 6.138589942316853e-10  grad at x: [ 4.90979691e-05 -6.69517762e-06]  gradient norm: 4.955235591701712e-05\n",
            "iter: 7651  x: [-99.9999755   24.99999666]  f(x): 6.114060139893022e-10  grad at x: [ 4.89997732e-05 -6.68178726e-06]  gradient norm: 4.945325121725779e-05\n",
            "iter: 7652  x: [-99.99997555  24.99999667]  f(x): 6.089628352120652e-10  grad at x: [ 4.89017737e-05 -6.66842369e-06]  gradient norm: 4.9354344700829134e-05\n",
            "iter: 7653  x: [-99.9999756   24.99999667]  f(x): 6.06529420009481e-10  grad at x: [ 4.88039701e-05 -6.65508684e-06]  gradient norm: 4.925563602307785e-05\n",
            "iter: 7654  x: [-99.99997565  24.99999668]  f(x): 6.041057285178019e-10  grad at x: [ 4.87063622e-05 -6.64177667e-06]  gradient norm: 4.915712475390732e-05\n",
            "iter: 7655  x: [-99.9999757   24.99999669]  f(x): 6.016917217073342e-10  grad at x: [ 4.86089494e-05 -6.62849312e-06]  gradient norm: 4.905881049138204e-05\n",
            "iter: 7656  x: [-99.99997574  24.99999669]  f(x): 5.992873613986077e-10  grad at x: [ 4.85117315e-05 -6.61523613e-06]  gradient norm: 4.896069286268762e-05\n",
            "iter: 7657  x: [-99.99997579  24.9999967 ]  f(x): 5.968926088553679e-10  grad at x: [ 4.84147081e-05 -6.60200566e-06]  gradient norm: 4.8862771466848575e-05\n",
            "iter: 7658  x: [-99.99997584  24.99999671]  f(x): 5.945074261409584e-10  grad at x: [ 4.83178787e-05 -6.58880165e-06]  gradient norm: 4.876504593009047e-05\n",
            "iter: 7659  x: [-99.99997589  24.99999671]  f(x): 5.921317747641271e-10  grad at x: [ 4.82212429e-05 -6.57562405e-06]  gradient norm: 4.8667515850477805e-05\n",
            "iter: 7660  x: [-99.99997594  24.99999672]  f(x): 5.897656163927218e-10  grad at x: [ 4.81248004e-05 -6.56247280e-06]  gradient norm: 4.857018082703509e-05\n",
            "iter: 7661  x: [-99.99997599  24.99999673]  f(x): 5.874089128297966e-10  grad at x: [ 4.80285508e-05 -6.54934785e-06]  gradient norm: 4.847304045878685e-05\n",
            "iter: 7662  x: [-99.99997603  24.99999673]  f(x): 5.850616266711622e-10  grad at x: [ 4.79324937e-05 -6.53624915e-06]  gradient norm: 4.8376094371958645e-05\n",
            "iter: 7663  x: [-99.99997608  24.99999674]  f(x): 5.827237206649666e-10  grad at x: [ 4.78366287e-05 -6.52317665e-06]  gradient norm: 4.82793421937361e-05\n",
            "iter: 7664  x: [-99.99997613  24.99999674]  f(x): 5.803951570095323e-10  grad at x: [ 4.77409555e-05 -6.51013030e-06]  gradient norm: 4.818278352314371e-05\n",
            "iter: 7665  x: [-99.99997618  24.99999675]  f(x): 5.780758980137806e-10  grad at x: [ 4.76454736e-05 -6.49711004e-06]  gradient norm: 4.808641795824599e-05\n",
            "iter: 7666  x: [-99.99997622  24.99999676]  f(x): 5.757659068188753e-10  grad at x: [ 4.75501826e-05 -6.48411581e-06]  gradient norm: 4.799024512622853e-05\n",
            "iter: 7667  x: [-99.99997627  24.99999676]  f(x): 5.73465146019103e-10  grad at x: [ 4.74550823e-05 -6.47114759e-06]  gradient norm: 4.789426462611585e-05\n",
            "iter: 7668  x: [-99.99997632  24.99999677]  f(x): 5.711735789913712e-10  grad at x: [ 4.73601721e-05 -6.45820529e-06]  gradient norm: 4.779847608413353e-05\n",
            "iter: 7669  x: [-99.99997637  24.99999678]  f(x): 5.688911692624356e-10  grad at x: [ 4.72654517e-05 -6.44528888e-06]  gradient norm: 4.7702879127467165e-05\n",
            "iter: 7670  x: [-99.99997641  24.99999678]  f(x): 5.666178804626086e-10  grad at x: [ 4.71709209e-05 -6.43239830e-06]  gradient norm: 4.760747338234234e-05\n",
            "iter: 7671  x: [-99.99997646  24.99999679]  f(x): 5.64353675725052e-10  grad at x: [ 4.70765790e-05 -6.41953351e-06]  gradient norm: 4.75122584487436e-05\n",
            "iter: 7672  x: [-99.99997651  24.9999968 ]  f(x): 5.620985182682704e-10  grad at x: [ 4.69824259e-05 -6.40669444e-06]  gradient norm: 4.741723392473544e-05\n",
            "iter: 7673  x: [-99.99997656  24.9999968 ]  f(x): 5.598523727969821e-10  grad at x: [ 4.68884610e-05 -6.39388105e-06]  gradient norm: 4.732239946566455e-05\n",
            "iter: 7674  x: [-99.9999766   24.99999681]  f(x): 5.576152027828729e-10  grad at x: [ 4.67946841e-05 -6.38109329e-06]  gradient norm: 4.722775466959542e-05\n",
            "iter: 7675  x: [-99.99997665  24.99999682]  f(x): 5.553869725366569e-10  grad at x: [ 4.67010947e-05 -6.36833110e-06]  gradient norm: 4.713329916467367e-05\n",
            "iter: 7676  x: [-99.9999767   24.99999682]  f(x): 5.531676464479545e-10  grad at x: [ 4.66076926e-05 -6.35559444e-06]  gradient norm: 4.7039032577124905e-05\n",
            "iter: 7677  x: [-99.99997674  24.99999683]  f(x): 5.509571883919831e-10  grad at x: [ 4.65144772e-05 -6.34288325e-06]  gradient norm: 4.694495450597362e-05\n",
            "iter: 7678  x: [-99.99997679  24.99999683]  f(x): 5.487555636921676e-10  grad at x: [ 4.64214482e-05 -6.33019748e-06]  gradient norm: 4.685106460656652e-05\n",
            "iter: 7679  x: [-99.99997684  24.99999684]  f(x): 5.465627364730904e-10  grad at x: [ 4.63286053e-05 -6.31753709e-06]  gradient norm: 4.675736247792813e-05\n",
            "iter: 7680  x: [-99.99997688  24.99999685]  f(x): 5.44378671622094e-10  grad at x: [ 4.62359481e-05 -6.30490201e-06]  gradient norm: 4.666384774628402e-05\n",
            "iter: 7681  x: [-99.99997693  24.99999685]  f(x): 5.422033341938836e-10  grad at x: [ 4.61434762e-05 -6.29229221e-06]  gradient norm: 4.6570520039779824e-05\n",
            "iter: 7682  x: [-99.99997697  24.99999686]  f(x): 5.400366899749169e-10  grad at x: [ 4.60511893e-05 -6.27970762e-06]  gradient norm: 4.647737901280221e-05\n",
            "iter: 7683  x: [-99.99997702  24.99999687]  f(x): 5.378787035844012e-10  grad at x: [ 4.59590869e-05 -6.26714820e-06]  gradient norm: 4.6384424264375695e-05\n",
            "iter: 7684  x: [-99.99997707  24.99999687]  f(x): 5.35729340442246e-10  grad at x: [ 4.58671687e-05 -6.25461391e-06]  gradient norm: 4.6291655422645927e-05\n",
            "iter: 7685  x: [-99.99997711  24.99999688]  f(x): 5.335885660449119e-10  grad at x: [ 4.57754344e-05 -6.24210468e-06]  gradient norm: 4.619907211383847e-05\n",
            "iter: 7686  x: [-99.99997716  24.99999689]  f(x): 5.314563460319053e-10  grad at x: [ 4.56838835e-05 -6.22962047e-06]  gradient norm: 4.610667396513894e-05\n",
            "iter: 7687  x: [-99.9999772   24.99999689]  f(x): 5.293326461631228e-10  grad at x: [ 4.55925157e-05 -6.21716124e-06]  gradient norm: 4.601446060373295e-05\n",
            "iter: 7688  x: [-99.99997725  24.9999969 ]  f(x): 5.272174329651193e-10  grad at x: [ 4.55013307e-05 -6.20472692e-06]  gradient norm: 4.5922431684967176e-05\n",
            "iter: 7689  x: [-99.99997729  24.9999969 ]  f(x): 5.251106717883387e-10  grad at x: [ 4.54103280e-05 -6.19231746e-06]  gradient norm: 4.583058680786615e-05\n",
            "iter: 7690  x: [-99.99997734  24.99999691]  f(x): 5.23012329395819e-10  grad at x: [ 4.53195074e-05 -6.17993283e-06]  gradient norm: 4.573892562777656e-05\n",
            "iter: 7691  x: [-99.99997739  24.99999692]  f(x): 5.209223720217202e-10  grad at x: [ 4.52288683e-05 -6.16757297e-06]  gradient norm: 4.5647447771884036e-05\n",
            "iter: 7692  x: [-99.99997743  24.99999692]  f(x): 5.188407660188907e-10  grad at x: [ 4.51384106e-05 -6.15523783e-06]  gradient norm: 4.555615286737416e-05\n",
            "iter: 7693  x: [-99.99997748  24.99999693]  f(x): 5.167674784986989e-10  grad at x: [ 4.50481338e-05 -6.14292735e-06]  gradient norm: 4.546504056959364e-05\n",
            "iter: 7694  x: [-99.99997752  24.99999693]  f(x): 5.147024754076137e-10  grad at x: [ 4.49580375e-05 -6.13064150e-06]  gradient norm: 4.537411047756699e-05\n",
            "iter: 7695  x: [-99.99997757  24.99999694]  f(x): 5.126457240901259e-10  grad at x: [ 4.48681214e-05 -6.11838022e-06]  gradient norm: 4.5283362246640914e-05\n",
            "iter: 7696  x: [-99.99997761  24.99999695]  f(x): 5.105971920029526e-10  grad at x: [ 4.47783852e-05 -6.10614346e-06]  gradient norm: 4.5192795532162095e-05\n",
            "iter: 7697  x: [-99.99997766  24.99999695]  f(x): 5.085568454445912e-10  grad at x: [ 4.46888284e-05 -6.09393117e-06]  gradient norm: 4.5102409933155064e-05\n",
            "iter: 7698  x: [-99.9999777   24.99999696]  f(x): 5.065246521245e-10  grad at x: [ 4.45994507e-05 -6.08174331e-06]  gradient norm: 4.501220510592655e-05\n",
            "iter: 7699  x: [-99.99997774  24.99999697]  f(x): 5.045005798200961e-10  grad at x: [ 4.45102519e-05 -6.06957982e-06]  gradient norm: 4.492218070486321e-05\n",
            "iter: 7700  x: [-99.99997779  24.99999697]  f(x): 5.024845952004298e-10  grad at x: [ 4.44212313e-05 -6.05744066e-06]  gradient norm: 4.48323363299496e-05\n",
            "iter: 7701  x: [-99.99997783  24.99999698]  f(x): 5.004766669236419e-10  grad at x: [ 4.43323889e-05 -6.04532578e-06]  gradient norm: 4.474267166469351e-05\n",
            "iter: 7702  x: [-99.99997788  24.99999698]  f(x): 4.984767618669066e-10  grad at x: [ 4.42437241e-05 -6.03323512e-06]  gradient norm: 4.465318630811945e-05\n",
            "iter: 7703  x: [-99.99997792  24.99999699]  f(x): 4.964848489099114e-10  grad at x: [ 4.41552367e-05 -6.02116865e-06]  gradient norm: 4.4563879943735214e-05\n",
            "iter: 7704  x: [-99.99997797  24.999997  ]  f(x): 4.945008951796496e-10  grad at x: [ 4.40669262e-05 -6.00912632e-06]  gradient norm: 4.447475217152535e-05\n",
            "iter: 7705  x: [-99.99997801  24.999997  ]  f(x): 4.925248697546508e-10  grad at x: [ 4.39787923e-05 -5.99710806e-06]  gradient norm: 4.43858026740376e-05\n",
            "iter: 7706  x: [-99.99997805  24.99999701]  f(x): 4.905567406127226e-10  grad at x: [ 4.38908347e-05 -5.98511384e-06]  gradient norm: 4.429703107941762e-05\n",
            "iter: 7707  x: [-99.9999781   24.99999701]  f(x): 4.885964758240123e-10  grad at x: [ 4.38030531e-05 -5.97314362e-06]  gradient norm: 4.420843701485101e-05\n",
            "iter: 7708  x: [-99.99997814  24.99999702]  f(x): 4.866440442144792e-10  grad at x: [ 4.37154470e-05 -5.96119733e-06]  gradient norm: 4.4120020136644506e-05\n",
            "iter: 7709  x: [-99.99997819  24.99999703]  f(x): 4.846994146757409e-10  grad at x: [ 4.36280161e-05 -5.94927494e-06]  gradient norm: 4.4031780099184766e-05\n",
            "iter: 7710  x: [-99.99997823  24.99999703]  f(x): 4.82762555630815e-10  grad at x: [ 4.35407600e-05 -5.93737639e-06]  gradient norm: 4.3943716530617435e-05\n",
            "iter: 7711  x: [-99.99997827  24.99999704]  f(x): 4.808334362114525e-10  grad at x: [ 4.34536785e-05 -5.92550163e-06]  gradient norm: 4.3855829086289205e-05\n",
            "iter: 7712  x: [-99.99997832  24.99999704]  f(x): 4.789120256775861e-10  grad at x: [ 4.33667711e-05 -5.91365063e-06]  gradient norm: 4.3768117422506816e-05\n",
            "iter: 7713  x: [-99.99997836  24.99999705]  f(x): 4.769982933539134e-10  grad at x: [ 4.32800376e-05 -5.90182333e-06]  gradient norm: 4.368058119365691e-05\n",
            "iter: 7714  x: [-99.9999784   24.99999705]  f(x): 4.750922080999016e-10  grad at x: [ 4.31934775e-05 -5.89001968e-06]  gradient norm: 4.359322002788514e-05\n",
            "iter: 7715  x: [-99.99997845  24.99999706]  f(x): 4.731937394986134e-10  grad at x: [ 4.31070906e-05 -5.87823964e-06]  gradient norm: 4.350603358149825e-05\n",
            "iter: 7716  x: [-99.99997849  24.99999707]  f(x): 4.713028572180505e-10  grad at x: [ 4.30208764e-05 -5.86648316e-06]  gradient norm: 4.341902150984292e-05\n",
            "iter: 7717  x: [-99.99997853  24.99999707]  f(x): 4.69419531031863e-10  grad at x: [ 4.29348346e-05 -5.85475020e-06]  gradient norm: 4.3332183468265846e-05\n",
            "iter: 7718  x: [-99.99997858  24.99999708]  f(x): 4.675437308190485e-10  grad at x: [ 4.2848965e-05 -5.8430407e-06]  gradient norm: 4.324551911211373e-05\n",
            "iter: 7719  x: [-99.99997862  24.99999708]  f(x): 4.656754259766674e-10  grad at x: [ 4.27632670e-05 -5.83135461e-06]  gradient norm: 4.315902806953222e-05\n",
            "iter: 7720  x: [-99.99997866  24.99999709]  f(x): 4.6381458722419957e-10  grad at x: [ 4.26777405e-05 -5.81969191e-06]  gradient norm: 4.307271002498912e-05\n",
            "iter: 7721  x: [-99.9999787  24.9999971]  f(x): 4.6196118412872697e-10  grad at x: [ 4.25923850e-05 -5.80805252e-06]  gradient norm: 4.2986564604710015e-05\n",
            "iter: 7722  x: [-99.99997875  24.9999971 ]  f(x): 4.6011518703233573e-10  grad at x: [ 4.25072002e-05 -5.79643642e-06]  gradient norm: 4.290059146596166e-05\n",
            "iter: 7723  x: [-99.99997879  24.99999711]  f(x): 4.582765669218241e-10  grad at x: [ 4.24221859e-05 -5.78484354e-06]  gradient norm: 4.281479029129182e-05\n",
            "iter: 7724  x: [-99.99997883  24.99999711]  f(x): 4.564452937205999e-10  grad at x: [ 4.23373415e-05 -5.77327386e-06]  gradient norm: 4.272916070884612e-05\n",
            "iter: 7725  x: [-99.99997887  24.99999712]  f(x): 4.5462133806043057e-10  grad at x: [ 4.22526668e-05 -5.76172731e-06]  gradient norm: 4.264370237493131e-05\n",
            "iter: 7726  x: [-99.99997892  24.99999712]  f(x): 4.528046712546914e-10  grad at x: [ 4.21681615e-05 -5.75020385e-06]  gradient norm: 4.255841497305516e-05\n",
            "iter: 7727  x: [-99.99997896  24.99999713]  f(x): 4.5099526353891764e-10  grad at x: [ 4.20838251e-05 -5.73870345e-06]  gradient norm: 4.247329813136332e-05\n",
            "iter: 7728  x: [-99.999979    24.99999714]  f(x): 4.4919308642900126e-10  grad at x: [ 4.19996575e-05 -5.72722605e-06]  gradient norm: 4.2388351533363563e-05\n",
            "iter: 7729  x: [-99.99997904  24.99999714]  f(x): 4.473981109424751e-10  grad at x: [ 4.19156582e-05 -5.71577159e-06]  gradient norm: 4.230357483440259e-05\n",
            "iter: 7730  x: [-99.99997908  24.99999715]  f(x): 4.4561030821893214e-10  grad at x: [ 4.18318268e-05 -5.70434005e-06]  gradient norm: 4.221896769078714e-05\n",
            "iter: 7731  x: [-99.99997913  24.99999715]  f(x): 4.4382964949930504e-10  grad at x: [ 4.17481632e-05 -5.69293137e-06]  gradient norm: 4.213452975882394e-05\n",
            "iter: 7732  x: [-99.99997917  24.99999716]  f(x): 4.420561061053901e-10  grad at x: [ 4.16646669e-05 -5.68154550e-06]  gradient norm: 4.205026069385968e-05\n",
            "iter: 7733  x: [-99.99997921  24.99999716]  f(x): 4.402896500709542e-10  grad at x: [ 4.15813375e-05 -5.67018241e-06]  gradient norm: 4.1966160180362183e-05\n",
            "iter: 7734  x: [-99.99997925  24.99999717]  f(x): 4.3853025234605546e-10  grad at x: [ 4.14981749e-05 -5.65884205e-06]  gradient norm: 4.18822278464771e-05\n",
            "iter: 7735  x: [-99.99997929  24.99999718]  f(x): 4.367778857312337e-10  grad at x: [ 4.14151785e-05 -5.64752437e-06]  gradient norm: 4.1798463403873294e-05\n",
            "iter: 7736  x: [-99.99997933  24.99999718]  f(x): 4.350325213555582e-10  grad at x: [ 4.13323482e-05 -5.63622932e-06]  gradient norm: 4.171486647973637e-05\n",
            "iter: 7737  x: [-99.99997938  24.99999719]  f(x): 4.332941316449571e-10  grad at x: [ 4.12496835e-05 -5.62495686e-06]  gradient norm: 4.163143675853415e-05\n",
            "iter: 7738  x: [-99.99997942  24.99999719]  f(x): 4.3156268853497514e-10  grad at x: [ 4.11671841e-05 -5.61370694e-06]  gradient norm: 4.1548173896573366e-05\n",
            "iter: 7739  x: [-99.99997946  24.9999972 ]  f(x): 4.2983816406021134e-10  grad at x: [ 4.10848498e-05 -5.60247953e-06]  gradient norm: 4.1465077550160755e-05\n",
            "iter: 7740  x: [-99.9999795  24.9999972]  f(x): 4.2812053091685133e-10  grad at x: [ 4.10026801e-05 -5.59127457e-06]  gradient norm: 4.138214740280409e-05\n",
            "iter: 7741  x: [-99.99997954  24.99999721]  f(x): 4.2640976133336686e-10  grad at x: [ 4.09206747e-05 -5.58009202e-06]  gradient norm: 4.129938311081011e-05\n",
            "iter: 7742  x: [-99.99997958  24.99999722]  f(x): 4.2470582821679444e-10  grad at x: [ 4.08388334e-05 -5.56893183e-06]  gradient norm: 4.1216784358646636e-05\n",
            "iter: 7743  x: [-99.99997962  24.99999722]  f(x): 4.2300870396851394e-10  grad at x: [ 4.07571557e-05 -5.55779397e-06]  gradient norm: 4.1134350801660354e-05\n",
            "iter: 7744  x: [-99.99997966  24.99999723]  f(x): 4.213183611074241e-10  grad at x: [ 4.06756414e-05 -5.54667838e-06]  gradient norm: 4.1052082096158e-05\n",
            "iter: 7745  x: [-99.9999797   24.99999723]  f(x): 4.196347728266744e-10  grad at x: [ 4.05942901e-05 -5.53558503e-06]  gradient norm: 4.09699779266074e-05\n",
            "iter: 7746  x: [-99.99997974  24.99999724]  f(x): 4.179579123922603e-10  grad at x: [ 4.05131015e-05 -5.52451385e-06]  gradient norm: 4.0888037976516326e-05\n",
            "iter: 7747  x: [-99.99997978  24.99999724]  f(x): 4.162877526271495e-10  grad at x: [ 4.04320753e-05 -5.51346483e-06]  gradient norm: 4.080626190315155e-05\n",
            "iter: 7748  x: [-99.99997982  24.99999725]  f(x): 4.146242669850317e-10  grad at x: [ 4.03512112e-05 -5.50243790e-06]  gradient norm: 4.072464939002086e-05\n",
            "iter: 7749  x: [-99.99997986  24.99999725]  f(x): 4.1296742843915764e-10  grad at x: [ 4.02705088e-05 -5.49143302e-06]  gradient norm: 4.064320009247095e-05\n",
            "iter: 7750  x: [-99.99997991  24.99999726]  f(x): 4.113172106690117e-10  grad at x: [ 4.01899678e-05 -5.48045016e-06]  gradient norm: 4.056191369592967e-05\n",
            "iter: 7751  x: [-99.99997995  24.99999727]  f(x): 4.096735868362208e-10  grad at x: [ 4.01095878e-05 -5.46948925e-06]  gradient norm: 4.0480789855743716e-05\n",
            "iter: 7752  x: [-99.99997999  24.99999727]  f(x): 4.080365313745125e-10  grad at x: [ 4.00293686e-05 -5.45855028e-06]  gradient norm: 4.039982828550203e-05\n",
            "iter: 7753  x: [-99.99998003  24.99999728]  f(x): 4.064060176293993e-10  grad at x: [ 3.99493099e-05 -5.44763318e-06]  gradient norm: 4.031902864055132e-05\n",
            "iter: 7754  x: [-99.99998007  24.99999728]  f(x): 4.047820190607902e-10  grad at x: [ 3.98694113e-05 -5.43673791e-06]  gradient norm: 4.02383905771983e-05\n",
            "iter: 7755  x: [-99.99998011  24.99999729]  f(x): 4.0316451033485617e-10  grad at x: [ 3.97896725e-05 -5.42586444e-06]  gradient norm: 4.015791380711185e-05\n",
            "iter: 7756  x: [-99.99998014  24.99999729]  f(x): 4.01553465113093e-10  grad at x: [ 3.97100931e-05 -5.41501271e-06]  gradient norm: 4.007759798755874e-05\n",
            "iter: 7757  x: [-99.99998018  24.9999973 ]  f(x): 3.999488576757502e-10  grad at x: [ 3.96306730e-05 -5.40418268e-06]  gradient norm: 3.999744280204674e-05\n",
            "iter: 7758  x: [-99.99998022  24.9999973 ]  f(x): 3.983506618688327e-10  grad at x: [ 3.95514116e-05 -5.39337432e-06]  gradient norm: 3.991744790784264e-05\n",
            "iter: 7759  x: [-99.99998026  24.99999731]  f(x): 3.96758852715399e-10  grad at x: [ 3.94723088e-05 -5.38258757e-06]  gradient norm: 3.983761301661529e-05\n",
            "iter: 7760  x: [-99.9999803   24.99999731]  f(x): 3.951734042224511e-10  grad at x: [ 3.93933642e-05 -5.37182239e-06]  gradient norm: 3.9757937784671436e-05\n",
            "iter: 7761  x: [-99.99998034  24.99999732]  f(x): 3.935942910487437e-10  grad at x: [ 3.93145774e-05 -5.36107874e-06]  gradient norm: 3.967842189647888e-05\n",
            "iter: 7762  x: [-99.99998038  24.99999732]  f(x): 3.9202148849892364e-10  grad at x: [ 3.92359483e-05 -5.35035658e-06]  gradient norm: 3.959906506466654e-05\n",
            "iter: 7763  x: [-99.99998042  24.99999733]  f(x): 3.9045497086728946e-10  grad at x: [ 3.91574764e-05 -5.33965587e-06]  gradient norm: 3.95198669465012e-05\n",
            "iter: 7764  x: [-99.99998046  24.99999734]  f(x): 3.88894713038818e-10  grad at x: [ 3.90791615e-05 -5.32897656e-06]  gradient norm: 3.944082722453057e-05\n",
            "iter: 7765  x: [-99.9999805   24.99999734]  f(x): 3.873406894887992e-10  grad at x: [ 3.90010031e-05 -5.31831860e-06]  gradient norm: 3.936194555602145e-05\n",
            "iter: 7766  x: [-99.99998054  24.99999735]  f(x): 3.8579287587152055e-10  grad at x: [ 3.89230011e-05 -5.30768196e-06]  gradient norm: 3.928322165360273e-05\n",
            "iter: 7767  x: [-99.99998058  24.99999735]  f(x): 3.8425124737187357e-10  grad at x: [ 3.88451551e-05 -5.29706659e-06]  gradient norm: 3.920465520174223e-05\n",
            "iter: 7768  x: [-99.99998062  24.99999736]  f(x): 3.8271577926156575e-10  grad at x: [ 3.87674648e-05 -5.28647246e-06]  gradient norm: 3.912624588490778e-05\n",
            "iter: 7769  x: [-99.99998066  24.99999736]  f(x): 3.8118644689887275e-10  grad at x: [ 3.86899299e-05 -5.27589951e-06]  gradient norm: 3.904799338756719e-05\n",
            "iter: 7770  x: [-99.99998069  24.99999737]  f(x): 3.7966322572839064e-10  grad at x: [ 3.86125500e-05 -5.26534771e-06]  gradient norm: 3.896989739418828e-05\n",
            "iter: 7771  x: [-99.99998073  24.99999737]  f(x): 3.781460912807888e-10  grad at x: [ 3.85353249e-05 -5.25481701e-06]  gradient norm: 3.889195758923887e-05\n",
            "iter: 7772  x: [-99.99998077  24.99999738]  f(x): 3.7663501973771933e-10  grad at x: [ 3.84582542e-05 -5.24430738e-06]  gradient norm: 3.881417368630791e-05\n",
            "iter: 7773  x: [-99.99998081  24.99999738]  f(x): 3.7512998623384174e-10  grad at x: [ 3.83813377e-05 -5.23381877e-06]  gradient norm: 3.8736545340742084e-05\n",
            "iter: 7774  x: [-99.99998085  24.99999739]  f(x): 3.7363096655657414e-10  grad at x: [ 3.83045750e-05 -5.22335113e-06]  gradient norm: 3.865907223700921e-05\n",
            "iter: 7775  x: [-99.99998089  24.99999739]  f(x): 3.72137937121676e-10  grad at x: [ 3.82279659e-05 -5.21290443e-06]  gradient norm: 3.858175408773821e-05\n",
            "iter: 7776  x: [-99.99998092  24.9999974 ]  f(x): 3.7065087390172327e-10  grad at x: [ 3.81515100e-05 -5.20247862e-06]  gradient norm: 3.8504590578356925e-05\n",
            "iter: 7777  x: [-99.99998096  24.9999974 ]  f(x): 3.691697529168657e-10  grad at x: [ 3.80752069e-05 -5.19207366e-06]  gradient norm: 3.8427581392373145e-05\n",
            "iter: 7778  x: [-99.999981    24.99999741]  f(x): 3.6769455084858857e-10  grad at x: [ 3.79990565e-05 -5.18168951e-06]  gradient norm: 3.835072624337581e-05\n",
            "iter: 7779  x: [-99.99998104  24.99999741]  f(x): 3.662252433618261e-10  grad at x: [ 3.79230584e-05 -5.17132613e-06]  gradient norm: 3.827402478767166e-05\n",
            "iter: 7780  x: [-99.99998108  24.99999742]  f(x): 3.64761807285404e-10  grad at x: [ 3.78472123e-05 -5.16098348e-06]  gradient norm: 3.819747673788958e-05\n",
            "iter: 7781  x: [-99.99998111  24.99999742]  f(x): 3.6330421899072194e-10  grad at x: [ 3.77715179e-05 -5.15066151e-06]  gradient norm: 3.8121081778497416e-05\n",
            "iter: 7782  x: [-99.99998115  24.99999743]  f(x): 3.618524554865474e-10  grad at x: [ 3.76959748e-05 -5.14036019e-06]  gradient norm: 3.8044839623084096e-05\n",
            "iter: 7783  x: [-99.99998119  24.99999743]  f(x): 3.6040649328931636e-10  grad at x: [ 3.76205829e-05 -5.13007947e-06]  gradient norm: 3.79687499551574e-05\n",
            "iter: 7784  x: [-99.99998123  24.99999744]  f(x): 3.589663090349338e-10  grad at x: [ 3.75453417e-05 -5.11981931e-06]  gradient norm: 3.78928124601452e-05\n",
            "iter: 7785  x: [-99.99998126  24.99999745]  f(x): 3.575318794237104e-10  grad at x: [ 3.74702510e-05 -5.10957967e-06]  gradient norm: 3.78170268225153e-05\n",
            "iter: 7786  x: [-99.9999813   24.99999745]  f(x): 3.5610318176984054e-10  grad at x: [ 3.73953105e-05 -5.09936051e-06]  gradient norm: 3.774139275489661e-05\n",
            "iter: 7787  x: [-99.99998134  24.99999746]  f(x): 3.5468019348357305e-10  grad at x: [ 3.73205199e-05 -5.08916179e-06]  gradient norm: 3.7665909970878074e-05\n",
            "iter: 7788  x: [-99.99998138  24.99999746]  f(x): 3.532628914873842e-10  grad at x: [ 3.72458789e-05 -5.07898346e-06]  gradient norm: 3.7590578153967475e-05\n",
            "iter: 7789  x: [-99.99998141  24.99999747]  f(x): 3.518512528216846e-10  grad at x: [ 3.71713871e-05 -5.06882549e-06]  gradient norm: 3.7515396989592664e-05\n",
            "iter: 7790  x: [-99.99998145  24.99999747]  f(x): 3.504452551174854e-10  grad at x: [ 3.70970443e-05 -5.05868784e-06]  gradient norm: 3.744036619038256e-05\n",
            "iter: 7791  x: [-99.99998149  24.99999748]  f(x): 3.4904487610081456e-10  grad at x: [ 3.70228503e-05 -5.04857046e-06]  gradient norm: 3.73654854699261e-05\n",
            "iter: 7792  x: [-99.99998153  24.99999748]  f(x): 3.4765009303144776e-10  grad at x: [ 3.69488046e-05 -5.03847333e-06]  gradient norm: 3.729075451269109e-05\n",
            "iter: 7793  x: [-99.99998156  24.99999749]  f(x): 3.462608832500116e-10  grad at x: [ 3.68749070e-05 -5.02839638e-06]  gradient norm: 3.721617300314537e-05\n",
            "iter: 7794  x: [-99.9999816   24.99999749]  f(x): 3.448772247007232e-10  grad at x: [ 3.68011571e-05 -5.01833959e-06]  gradient norm: 3.714174065391784e-05\n",
            "iter: 7795  x: [-99.99998164  24.9999975 ]  f(x): 3.4349909542178855e-10  grad at x: [ 3.67275548e-05 -5.00830291e-06]  gradient norm: 3.706745717859743e-05\n",
            "iter: 7796  x: [-99.99998167  24.9999975 ]  f(x): 3.4212647298860896e-10  grad at x: [ 3.66540997e-05 -4.99828630e-06]  gradient norm: 3.699332226165198e-05\n",
            "iter: 7797  x: [-99.99998171  24.99999751]  f(x): 3.4075933557636524e-10  grad at x: [ 3.65807915e-05 -4.98828972e-06]  gradient norm: 3.691933561571038e-05\n",
            "iter: 7798  x: [-99.99998175  24.99999751]  f(x): 3.3939766145346713e-10  grad at x: [ 3.65076299e-05 -4.97831314e-06]  gradient norm: 3.6845496954361584e-05\n",
            "iter: 7799  x: [-99.99998178  24.99999752]  f(x): 3.3804142842809417e-10  grad at x: [ 3.64346147e-05 -4.96835651e-06]  gradient norm: 3.67718059620734e-05\n",
            "iter: 7800  x: [-99.99998182  24.99999752]  f(x): 3.3669061492203296e-10  grad at x: [ 3.63617455e-05 -4.95841980e-06]  gradient norm: 3.669826235243478e-05\n",
            "iter: 7801  x: [-99.99998186  24.99999753]  f(x): 3.353451994142415e-10  grad at x: [ 3.62890220e-05 -4.94850296e-06]  gradient norm: 3.662486583807463e-05\n",
            "iter: 7802  x: [-99.99998189  24.99999753]  f(x): 3.340051599436931e-10  grad at x: [ 3.62164439e-05 -4.93860595e-06]  gradient norm: 3.6551616103460766e-05\n",
            "iter: 7803  x: [-99.99998193  24.99999754]  f(x): 3.3267047515909687e-10  grad at x: [ 3.61440110e-05 -4.92872874e-06]  gradient norm: 3.647851286218214e-05\n",
            "iter: 7804  x: [-99.99998196  24.99999754]  f(x): 3.313411237657941e-10  grad at x: [ 3.60717230e-05 -4.91887128e-06]  gradient norm: 3.640555582686764e-05\n",
            "iter: 7805  x: [-99.999982    24.99999755]  f(x): 3.3001708456060435e-10  grad at x: [ 3.59995796e-05 -4.90903354e-06]  gradient norm: 3.633274471110623e-05\n",
            "iter: 7806  x: [-99.99998204  24.99999755]  f(x): 3.2869833639663014e-10  grad at x: [ 3.59275804e-05 -4.89921548e-06]  gradient norm: 3.62600792275268e-05\n",
            "iter: 7807  x: [-99.99998207  24.99999756]  f(x): 3.2738485769105224e-10  grad at x: [ 3.58557252e-05 -4.88941704e-06]  gradient norm: 3.618755906059718e-05\n",
            "iter: 7808  x: [-99.99998211  24.99999756]  f(x): 3.2607662797291657e-10  grad at x: [ 3.57840138e-05 -4.87963820e-06]  gradient norm: 3.6115183952067394e-05\n",
            "iter: 7809  x: [-99.99998214  24.99999757]  f(x): 3.247736258252424e-10  grad at x: [ 3.57124458e-05 -4.86987893e-06]  gradient norm: 3.6042953587365306e-05\n",
            "iter: 7810  x: [-99.99998218  24.99999757]  f(x): 3.234758303971866e-10  grad at x: [ 3.56410209e-05 -4.86013917e-06]  gradient norm: 3.5970867679119814e-05\n",
            "iter: 7811  x: [-99.99998222  24.99999757]  f(x): 3.221832209106801e-10  grad at x: [ 3.55697388e-05 -4.85041889e-06]  gradient norm: 3.5898925939959825e-05\n",
            "iter: 7812  x: [-99.99998225  24.99999758]  f(x): 3.20895776677418e-10  grad at x: [ 3.54985993e-05 -4.84071805e-06]  gradient norm: 3.582712808347429e-05\n",
            "iter: 7813  x: [-99.99998229  24.99999758]  f(x): 3.196134770813173e-10  grad at x: [ 3.54276021e-05 -4.83103661e-06]  gradient norm: 3.5755473823252144e-05\n",
            "iter: 7814  x: [-99.99998232  24.99999759]  f(x): 3.1833630156118224e-10  grad at x: [ 3.53567469e-05 -4.82137454e-06]  gradient norm: 3.5683962871922294e-05\n",
            "iter: 7815  x: [-99.99998236  24.99999759]  f(x): 3.170642296448595e-10  grad at x: [ 3.52860334e-05 -4.81173179e-06]  gradient norm: 3.561259494307369e-05\n",
            "iter: 7816  x: [-99.99998239  24.9999976 ]  f(x): 3.157972409147404e-10  grad at x: [ 3.52154614e-05 -4.80210833e-06]  gradient norm: 3.554136974933523e-05\n",
            "iter: 7817  x: [-99.99998243  24.9999976 ]  f(x): 3.1453531504178017e-10  grad at x: [ 3.51450304e-05 -4.79250411e-06]  gradient norm: 3.547028700429587e-05\n",
            "iter: 7818  x: [-99.99998246  24.99999761]  f(x): 3.1327843175113724e-10  grad at x: [ 3.50747404e-05 -4.78291911e-06]  gradient norm: 3.5399346420584504e-05\n",
            "iter: 7819  x: [-99.9999825   24.99999761]  f(x): 3.120265713535022e-10  grad at x: [ 3.50045909e-05 -4.77335327e-06]  gradient norm: 3.532854773995117e-05\n",
            "iter: 7820  x: [-99.99998253  24.99999762]  f(x): 3.107797132165493e-10  grad at x: [ 3.49345817e-05 -4.76380656e-06]  gradient norm: 3.525789064686368e-05\n",
            "iter: 7821  x: [-99.99998257  24.99999762]  f(x): 3.0953783731189596e-10  grad at x: [ 3.48647125e-05 -4.75427895e-06]  gradient norm: 3.5187374855871015e-05\n",
            "iter: 7822  x: [-99.9999826   24.99999763]  f(x): 3.083009241253029e-10  grad at x: [ 3.47949831e-05 -4.74477039e-06]  gradient norm: 3.511700010680314e-05\n",
            "iter: 7823  x: [-99.99998264  24.99999763]  f(x): 3.070689537660175e-10  grad at x: [ 3.47253932e-05 -4.73528085e-06]  gradient norm: 3.504676611420903e-05\n",
            "iter: 7824  x: [-99.99998267  24.99999764]  f(x): 3.058419063795637e-10  grad at x: [ 3.46559424e-05 -4.72581029e-06]  gradient norm: 3.497667259071759e-05\n",
            "iter: 7825  x: [-99.99998271  24.99999764]  f(x): 3.0461976219814474e-10  grad at x: [ 3.45866305e-05 -4.71635867e-06]  gradient norm: 3.4906719249917757e-05\n",
            "iter: 7826  x: [-99.99998274  24.99999765]  f(x): 3.034025015068294e-10  grad at x: [ 3.45174572e-05 -4.70692595e-06]  gradient norm: 3.483690580443845e-05\n",
            "iter: 7827  x: [-99.99998278  24.99999765]  f(x): 3.0219010516643786e-10  grad at x: [ 3.44484223e-05 -4.69751210e-06]  gradient norm: 3.47672319960297e-05\n",
            "iter: 7828  x: [-99.99998281  24.99999766]  f(x): 3.0098255361449815e-10  grad at x: [ 3.43795255e-05 -4.68811707e-06]  gradient norm: 3.469769753828044e-05\n",
            "iter: 7829  x: [-99.99998284  24.99999766]  f(x): 2.99779827340915e-10  grad at x: [ 3.43107664e-05 -4.67874084e-06]  gradient norm: 3.462830214381958e-05\n",
            "iter: 7830  x: [-99.99998288  24.99999767]  f(x): 2.985819069377034e-10  grad at x: [ 3.42421449e-05 -4.66938336e-06]  gradient norm: 3.4559045527196114e-05\n",
            "iter: 7831  x: [-99.99998291  24.99999767]  f(x): 2.97388773501315e-10  grad at x: [ 3.41736606e-05 -4.66004459e-06]  gradient norm: 3.4489927428239974e-05\n",
            "iter: 7832  x: [-99.99998295  24.99999767]  f(x): 2.9620040775788584e-10  grad at x: [ 3.41053133e-05 -4.65072450e-06]  gradient norm: 3.4420947561500156e-05\n",
            "iter: 7833  x: [-99.99998298  24.99999768]  f(x): 2.9501679095235134e-10  grad at x: [ 3.40371026e-05 -4.64142305e-06]  gradient norm: 3.435210566776665e-05\n",
            "iter: 7834  x: [-99.99998302  24.99999768]  f(x): 2.9383790394411094e-10  grad at x: [ 3.39690284e-05 -4.63214020e-06]  gradient norm: 3.428340146158844e-05\n",
            "iter: 7835  x: [-99.99998305  24.99999769]  f(x): 2.9266372761099245e-10  grad at x: [ 3.39010904e-05 -4.62287592e-06]  gradient norm: 3.421483465463438e-05\n",
            "iter: 7836  x: [-99.99998308  24.99999769]  f(x): 2.914942434286935e-10  grad at x: [ 3.38332882e-05 -4.61363017e-06]  gradient norm: 3.414640498961456e-05\n",
            "iter: 7837  x: [-99.99998312  24.9999977 ]  f(x): 2.903294324401751e-10  grad at x: [ 3.37656216e-05 -4.60440291e-06]  gradient norm: 3.4078112180117905e-05\n",
            "iter: 7838  x: [-99.99998315  24.9999977 ]  f(x): 2.8916927621820463e-10  grad at x: [ 3.36980904e-05 -4.59519411e-06]  gradient norm: 3.400995596693442e-05\n",
            "iter: 7839  x: [-99.99998318  24.99999771]  f(x): 2.8801375593727783e-10  grad at x: [ 3.36306942e-05 -4.58600372e-06]  gradient norm: 3.394193606365305e-05\n",
            "iter: 7840  x: [-99.99998322  24.99999771]  f(x): 2.868628528387638e-10  grad at x: [ 3.35634328e-05 -4.57683171e-06]  gradient norm: 3.387405218386273e-05\n",
            "iter: 7841  x: [-99.99998325  24.99999772]  f(x): 2.8571654869049276e-10  grad at x: [ 3.34963059e-05 -4.56767805e-06]  gradient norm: 3.380630406835345e-05\n",
            "iter: 7842  x: [-99.99998329  24.99999772]  f(x): 2.845748253554907e-10  grad at x: [ 3.34293133e-05 -4.55854269e-06]  gradient norm: 3.373869145983529e-05\n",
            "iter: 7843  x: [-99.99998332  24.99999773]  f(x): 2.8343766425271893e-10  grad at x: [ 3.33624547e-05 -4.54942560e-06]  gradient norm: 3.367121407093715e-05\n",
            "iter: 7844  x: [-99.99998335  24.99999773]  f(x): 2.823050473728083e-10  grad at x: [ 3.32957298e-05 -4.54032676e-06]  gradient norm: 3.360387164436909e-05\n",
            "iter: 7845  x: [-99.99998339  24.99999773]  f(x): 2.811769562639887e-10  grad at x: [ 3.32291383e-05 -4.53124611e-06]  gradient norm: 3.3536663892760034e-05\n",
            "iter: 7846  x: [-99.99998342  24.99999774]  f(x): 2.800533730276978e-10  grad at x: [ 3.31626800e-05 -4.52218362e-06]  gradient norm: 3.346959055786e-05\n",
            "iter: 7847  x: [-99.99998345  24.99999774]  f(x): 2.7893427981111964e-10  grad at x: [ 3.30963547e-05 -4.51313925e-06]  gradient norm: 3.340265138045899e-05\n",
            "iter: 7848  x: [-99.99998348  24.99999775]  f(x): 2.7781965838578946e-10  grad at x: [ 3.30301620e-05 -4.50411297e-06]  gradient norm: 3.3335846075105964e-05\n",
            "iter: 7849  x: [-99.99998352  24.99999775]  f(x): 2.7670949104078116e-10  grad at x: [ 3.29641016e-05 -4.49510474e-06]  gradient norm: 3.326917438355098e-05\n",
            "iter: 7850  x: [-99.99998355  24.99999776]  f(x): 2.7560376011047996e-10  grad at x: [ 3.28981735e-05 -4.48611453e-06]  gradient norm: 3.320263604658401e-05\n",
            "iter: 7851  x: [-99.99998358  24.99999776]  f(x): 2.745024475557118e-10  grad at x: [ 3.28323771e-05 -4.47714230e-06]  gradient norm: 3.313623077875405e-05\n",
            "iter: 7852  x: [-99.99998362  24.99999777]  f(x): 2.734055358357069e-10  grad at x: [ 3.27667124e-05 -4.46818801e-06]  gradient norm: 3.306995832085108e-05\n",
            "iter: 7853  x: [-99.99998365  24.99999777]  f(x): 2.723130075022298e-10  grad at x: [ 3.27011790e-05 -4.45925163e-06]  gradient norm: 3.3003818415585176e-05\n",
            "iter: 7854  x: [-99.99998368  24.99999777]  f(x): 2.7122484467206486e-10  grad at x: [ 3.26357766e-05 -4.45033313e-06]  gradient norm: 3.2937810775585246e-05\n",
            "iter: 7855  x: [-99.99998371  24.99999778]  f(x): 2.701410300206259e-10  grad at x: [ 3.25705050e-05 -4.44143247e-06]  gradient norm: 3.2871935143561346e-05\n",
            "iter: 7856  x: [-99.99998375  24.99999778]  f(x): 2.6906154625190317e-10  grad at x: [ 3.2505364e-05 -4.4325496e-06]  gradient norm: 3.2806191260303484e-05\n",
            "iter: 7857  x: [-99.99998378  24.99999779]  f(x): 2.679863761457567e-10  grad at x: [ 3.24403533e-05 -4.42368450e-06]  gradient norm: 3.274057886756169e-05\n",
            "iter: 7858  x: [-99.99998381  24.99999779]  f(x): 2.6691550255758844e-10  grad at x: [ 3.23754726e-05 -4.41483714e-06]  gradient norm: 3.2675097708046015e-05\n",
            "iter: 7859  x: [-99.99998384  24.9999978 ]  f(x): 2.658489083710563e-10  grad at x: [ 3.23107216e-05 -4.40600746e-06]  gradient norm: 3.2609747522546466e-05\n",
            "iter: 7860  x: [-99.99998388  24.9999978 ]  f(x): 2.6478657608683955e-10  grad at x: [ 3.22461002e-05 -4.39719545e-06]  gradient norm: 3.254452802465198e-05\n",
            "iter: 7861  x: [-99.99998391  24.99999781]  f(x): 2.637284887415518e-10  grad at x: [ 3.21816080e-05 -4.38840106e-06]  gradient norm: 3.247943895707263e-05\n",
            "iter: 7862  x: [-99.99998394  24.99999781]  f(x): 2.6267462985614645e-10  grad at x: [ 3.21172448e-05 -4.37962426e-06]  gradient norm: 3.2414480088759496e-05\n",
            "iter: 7863  x: [-99.99998397  24.99999781]  f(x): 2.616249821115791e-10  grad at x: [ 3.20530103e-05 -4.37086501e-06]  gradient norm: 3.234965113330152e-05\n",
            "iter: 7864  x: [-99.99998401  24.99999782]  f(x): 2.6057952872138104e-10  grad at x: [ 3.19889043e-05 -4.36212328e-06]  gradient norm: 3.228495183340877e-05\n",
            "iter: 7865  x: [-99.99998404  24.99999782]  f(x): 2.595382529266932e-10  grad at x: [ 3.19249264e-05 -4.35339903e-06]  gradient norm: 3.222038192987123e-05\n",
            "iter: 7866  x: [-99.99998407  24.99999783]  f(x): 2.5850113805815034e-10  grad at x: [ 3.18610766e-05 -4.34469223e-06]  gradient norm: 3.215594116539899e-05\n",
            "iter: 7867  x: [-99.9999841   24.99999783]  f(x): 2.574681674891911e-10  grad at x: [ 3.17973544e-05 -4.33600285e-06]  gradient norm: 3.209162928174206e-05\n",
            "iter: 7868  x: [-99.99998413  24.99999784]  f(x): 2.5643932465142003e-10  grad at x: [ 3.17337597e-05 -4.32733084e-06]  gradient norm: 3.202744602065048e-05\n",
            "iter: 7869  x: [-99.99998416  24.99999784]  f(x): 2.554145930344412e-10  grad at x: [ 3.16702922e-05 -4.31867618e-06]  gradient norm: 3.1963391123874274e-05\n",
            "iter: 7870  x: [-99.9999842   24.99999784]  f(x): 2.5439395618569225e-10  grad at x: [ 3.16069516e-05 -4.31003883e-06]  gradient norm: 3.189946433316348e-05\n",
            "iter: 7871  x: [-99.99998423  24.99999785]  f(x): 2.5337739815854237e-10  grad at x: [ 3.15437377e-05 -4.30141876e-06]  gradient norm: 3.183566541842921e-05\n",
            "iter: 7872  x: [-99.99998426  24.99999785]  f(x): 2.523649021655436e-10  grad at x: [ 3.14806503e-05 -4.29281592e-06]  gradient norm: 3.177199409326041e-05\n",
            "iter: 7873  x: [-99.99998429  24.99999786]  f(x): 2.513564519418683e-10  grad at x: [ 3.14176889e-05 -4.28423029e-06]  gradient norm: 3.170845010036715e-05\n",
            "iter: 7874  x: [-99.99998432  24.99999786]  f(x): 2.5035203169493946e-10  grad at x: [ 3.13548536e-05 -4.27566182e-06]  gradient norm: 3.1645033208700505e-05\n",
            "iter: 7875  x: [-99.99998435  24.99999787]  f(x): 2.493516248266893e-10  grad at x: [ 3.12921439e-05 -4.26711050e-06]  gradient norm: 3.1581743132809454e-05\n",
            "iter: 7876  x: [-99.99998439  24.99999787]  f(x): 2.4835521567181055e-10  grad at x: [ 3.12295596e-05 -4.25857628e-06]  gradient norm: 3.1518579642605126e-05\n",
            "iter: 7877  x: [-99.99998442  24.99999787]  f(x): 2.4736278817521603e-10  grad at x: [ 3.11671004e-05 -4.25005912e-06]  gradient norm: 3.145554247983754e-05\n",
            "iter: 7878  x: [-99.99998445  24.99999788]  f(x): 2.4637432635341687e-10  grad at x: [ 3.11047662e-05 -4.24155900e-06]  gradient norm: 3.139263138721677e-05\n",
            "iter: 7879  x: [-99.99998448  24.99999788]  f(x): 2.4538981469023355e-10  grad at x: [ 3.10425567e-05 -4.23307588e-06]  gradient norm: 3.132984613369389e-05\n",
            "iter: 7880  x: [-99.99998451  24.99999789]  f(x): 2.444092368717814e-10  grad at x: [ 3.09804716e-05 -4.22460973e-06]  gradient norm: 3.126718643381789e-05\n",
            "iter: 7881  x: [-99.99998454  24.99999789]  f(x): 2.4343257750740793e-10  grad at x: [ 3.09185106e-05 -4.21616051e-06]  gradient norm: 3.120465205749988e-05\n",
            "iter: 7882  x: [-99.99998457  24.9999979 ]  f(x): 2.4245982082032565e-10  grad at x: [ 3.08566736e-05 -4.20772819e-06]  gradient norm: 3.1142242746489895e-05\n",
            "iter: 7883  x: [-99.9999846  24.9999979]  f(x): 2.4149095154201754e-10  grad at x: [ 3.07949603e-05 -4.19931273e-06]  gradient norm: 3.1079958271659084e-05\n",
            "iter: 7884  x: [-99.99998463  24.9999979 ]  f(x): 2.4052595356752963e-10  grad at x: [ 3.07333704e-05 -4.19091411e-06]  gradient norm: 3.10177983465964e-05\n",
            "iter: 7885  x: [-99.99998466  24.99999791]  f(x): 2.39564811722567e-10  grad at x: [ 3.06719036e-05 -4.18253229e-06]  gradient norm: 3.0955762741212954e-05\n",
            "iter: 7886  x: [-99.99998469  24.99999791]  f(x): 2.3860751088459407e-10  grad at x: [ 3.06105598e-05 -4.17416722e-06]  gradient norm: 3.089385122541986e-05\n",
            "iter: 7887  x: [-99.99998473  24.99999792]  f(x): 2.376540351144269e-10  grad at x: [ 3.05493387e-05 -4.16581889e-06]  gradient norm: 3.083206351280607e-05\n",
            "iter: 7888  x: [-99.99998476  24.99999792]  f(x): 2.367043694125994e-10  grad at x: [ 3.04882400e-05 -4.15748725e-06]  gradient norm: 3.0770399374242734e-05\n",
            "iter: 7889  x: [-99.99998479  24.99999793]  f(x): 2.3575849881609714e-10  grad at x: [ 3.04272635e-05 -4.14917227e-06]  gradient norm: 3.070885857964097e-05\n",
            "iter: 7890  x: [-99.99998482  24.99999793]  f(x): 2.3481640799625304e-10  grad at x: [ 3.03664090e-05 -4.14087393e-06]  gradient norm: 3.064744087171084e-05\n",
            "iter: 7891  x: [-99.99998485  24.99999793]  f(x): 2.338780816640795e-10  grad at x: [ 3.03056762e-05 -4.13259218e-06]  gradient norm: 3.0586145992202385e-05\n",
            "iter: 7892  x: [-99.99998488  24.99999794]  f(x): 2.3294350501471805e-10  grad at x: [ 3.02450648e-05 -4.12432700e-06]  gradient norm: 3.052497371102672e-05\n",
            "iter: 7893  x: [-99.99998491  24.99999794]  f(x): 2.3201266286508545e-10  grad at x: [ 3.01845747e-05 -4.11607834e-06]  gradient norm: 3.0463923769933868e-05\n",
            "iter: 7894  x: [-99.99998494  24.99999795]  f(x): 2.31085540100707e-10  grad at x: [ 3.01242056e-05 -4.10784619e-06]  gradient norm: 3.0402995911633907e-05\n",
            "iter: 7895  x: [-99.99998497  24.99999795]  f(x): 2.3016212207351778e-10  grad at x: [ 3.00639571e-05 -4.09963049e-06]  gradient norm: 3.034218990603795e-05\n",
            "iter: 7896  x: [-99.999985    24.99999795]  f(x): 2.2924239420027022e-10  grad at x: [ 3.00038292e-05 -4.09143124e-06]  gradient norm: 3.0281505524017145e-05\n",
            "iter: 7897  x: [-99.99998503  24.99999796]  f(x): 2.2832634149320176e-10  grad at x: [ 2.99438215e-05 -4.08324837e-06]  gradient norm: 3.0220942506361495e-05\n",
            "iter: 7898  x: [-99.99998506  24.99999796]  f(x): 2.2741394948618134e-10  grad at x: [ 2.98839339e-05 -4.07508188e-06]  gradient norm: 3.016050062490219e-05\n",
            "iter: 7899  x: [-99.99998509  24.99999797]  f(x): 2.2650520329552563e-10  grad at x: [ 2.98241660e-05 -4.06693171e-06]  gradient norm: 3.010017962042922e-05\n",
            "iter: 7900  x: [-99.99998512  24.99999797]  f(x): 2.2560008854258995e-10  grad at x: [ 2.97645177e-05 -4.05879784e-06]  gradient norm: 3.0039979263813744e-05\n",
            "iter: 7901  x: [-99.99998515  24.99999797]  f(x): 2.2469859047604499e-10  grad at x: [ 2.97049887e-05 -4.05068025e-06]  gradient norm: 2.9979899297765828e-05\n",
            "iter: 7902  x: [-99.99998518  24.99999798]  f(x): 2.2380069480417622e-10  grad at x: [ 2.96455787e-05 -4.04257889e-06]  gradient norm: 2.9919939492196586e-05\n",
            "iter: 7903  x: [-99.99998521  24.99999798]  f(x): 2.229063872845468e-10  grad at x: [ 2.95862875e-05 -4.03449373e-06]  gradient norm: 2.9860099617017142e-05\n",
            "iter: 7904  x: [-99.99998524  24.99999799]  f(x): 2.220156533185585e-10  grad at x: [ 2.95271149e-05 -4.02642474e-06]  gradient norm: 2.9800379414937554e-05\n",
            "iter: 7905  x: [-99.99998527  24.99999799]  f(x): 2.2112847876434754e-10  grad at x: [ 2.94680607e-05 -4.01837189e-06]  gradient norm: 2.974077865586895e-05\n",
            "iter: 7906  x: [-99.9999853   24.99999799]  f(x): 2.2024484954315205e-10  grad at x: [ 2.94091246e-05 -4.01033515e-06]  gradient norm: 2.968129711068248e-05\n",
            "iter: 7907  x: [-99.99998532  24.999998  ]  f(x): 2.193647511935013e-10  grad at x: [ 2.93503064e-05 -4.00231448e-06]  gradient norm: 2.9621934521128176e-05\n",
            "iter: 7908  x: [-99.99998535  24.999998  ]  f(x): 2.184881697362911e-10  grad at x: [ 2.92916058e-05 -3.99430985e-06]  gradient norm: 2.956269065807719e-05\n",
            "iter: 7909  x: [-99.99998538  24.99999801]  f(x): 2.1761509081114902e-10  grad at x: [ 2.92330225e-05 -3.98632123e-06]  gradient norm: 2.9503565263279555e-05\n",
            "iter: 7910  x: [-99.99998541  24.99999801]  f(x): 2.1674550093851428e-10  grad at x: [ 2.91745565e-05 -3.97834859e-06]  gradient norm: 2.9444558134807477e-05\n",
            "iter: 7911  x: [-99.99998544  24.99999801]  f(x): 2.158793858702341e-10  grad at x: [ 2.91162074e-05 -3.97039189e-06]  gradient norm: 2.938566901537102e-05\n",
            "iter: 7912  x: [-99.99998547  24.99999802]  f(x): 2.1501673182231703e-10  grad at x: [ 2.9057975e-05 -3.9624511e-06]  gradient norm: 2.9326897675841338e-05\n",
            "iter: 7913  x: [-99.9999855   24.99999802]  f(x): 2.1415752504449e-10  grad at x: [ 2.8999859e-05 -3.9545262e-06]  gradient norm: 2.9268243886129555e-05\n",
            "iter: 7914  x: [-99.99998553  24.99999803]  f(x): 2.133017514369577e-10  grad at x: [ 2.89418593e-05 -3.94661715e-06]  gradient norm: 2.920970738894573e-05\n",
            "iter: 7915  x: [-99.99998556  24.99999803]  f(x): 2.124493977576519e-10  grad at x: [ 2.88839756e-05 -3.93872391e-06]  gradient norm: 2.9151287982362077e-05\n",
            "iter: 7916  x: [-99.99998559  24.99999803]  f(x): 2.1160045000336136e-10  grad at x: [ 2.88262076e-05 -3.93084647e-06]  gradient norm: 2.9092985409088656e-05\n",
            "iter: 7917  x: [-99.99998562  24.99999804]  f(x): 2.1075489461626986e-10  grad at x: [ 2.87685552e-05 -3.92298477e-06]  gradient norm: 2.903479943903659e-05\n",
            "iter: 7918  x: [-99.99998564  24.99999804]  f(x): 2.099127180996677e-10  grad at x: [ 2.87110181e-05 -3.91513881e-06]  gradient norm: 2.8976729843077028e-05\n",
            "iter: 7919  x: [-99.99998567  24.99999805]  f(x): 2.0907390698991568e-10  grad at x: [ 2.86535961e-05 -3.90730852e-06]  gradient norm: 2.8918776391121095e-05\n",
            "iter: 7920  x: [-99.9999857   24.99999805]  f(x): 2.082384474777803e-10  grad at x: [ 2.85962889e-05 -3.89949390e-06]  gradient norm: 2.886093882587885e-05\n",
            "iter: 7921  x: [-99.99998573  24.99999805]  f(x): 2.0740632661509458e-10  grad at x: [ 2.85390963e-05 -3.89169492e-06]  gradient norm: 2.880321694638254e-05\n",
            "iter: 7922  x: [-99.99998576  24.99999806]  f(x): 2.065775310784378e-10  grad at x: [ 2.84820181e-05 -3.88391152e-06]  gradient norm: 2.8745610522543284e-05\n",
            "iter: 7923  x: [-99.99998579  24.99999806]  f(x): 2.0575204720074646e-10  grad at x: [ 2.84250541e-05 -3.87614370e-06]  gradient norm: 2.8688119297071144e-05\n",
            "iter: 7924  x: [-99.99998582  24.99999807]  f(x): 2.0492986217074954e-10  grad at x: [ 2.83682040e-05 -3.86839142e-06]  gradient norm: 2.863074306899837e-05\n",
            "iter: 7925  x: [-99.99998584  24.99999807]  f(x): 2.0411096240172112e-10  grad at x: [ 2.83114676e-05 -3.86065464e-06]  gradient norm: 2.8573481580074984e-05\n",
            "iter: 7926  x: [-99.99998587  24.99999807]  f(x): 2.032953351593391e-10  grad at x: [ 2.82548446e-05 -3.85293333e-06]  gradient norm: 2.8516334628373198e-05\n",
            "iter: 7927  x: [-99.9999859   24.99999808]  f(x): 2.024829669779389e-10  grad at x: [ 2.81983349e-05 -3.84522746e-06]  gradient norm: 2.8459301957563113e-05\n",
            "iter: 7928  x: [-99.99998593  24.99999808]  f(x): 2.0167384521328847e-10  grad at x: [ 2.81419383e-05 -3.83753701e-06]  gradient norm: 2.8402383365716932e-05\n",
            "iter: 7929  x: [-99.99998596  24.99999809]  f(x): 2.008679564654444e-10  grad at x: [ 2.80856544e-05 -3.82986193e-06]  gradient norm: 2.8345578594584686e-05\n",
            "iter: 7930  x: [-99.99998599  24.99999809]  f(x): 2.0006528820706937e-10  grad at x: [ 2.80294831e-05 -3.82220221e-06]  gradient norm: 2.828888744415866e-05\n",
            "iter: 7931  x: [-99.99998601  24.99999809]  f(x): 1.9926582713073393e-10  grad at x: [ 2.79734241e-05 -3.81455781e-06]  gradient norm: 2.8232309656188878e-05\n",
            "iter: 7932  x: [-99.99998604  24.9999981 ]  f(x): 1.9846956077099945e-10  grad at x: [ 2.79174773e-05 -3.80692869e-06]  gradient norm: 2.8175845028747545e-05\n",
            "iter: 7933  x: [-99.99998607  24.9999981 ]  f(x): 1.9767647633551965e-10  grad at x: [ 2.78616423e-05 -3.79931483e-06]  gradient norm: 2.8119493333665856e-05\n",
            "iter: 7934  x: [-99.9999861  24.9999981]  f(x): 1.9688656104987824e-10  grad at x: [ 2.7805919e-05 -3.7917162e-06]  gradient norm: 2.806325434085493e-05\n",
            "iter: 7935  x: [-99.99998612  24.99999811]  f(x): 1.9609980219806152e-10  grad at x: [ 2.77503072e-05 -3.78413277e-06]  gradient norm: 2.8007127821185915e-05\n",
            "iter: 7936  x: [-99.99998615  24.99999811]  f(x): 1.9531618748892705e-10  grad at x: [ 2.76948066e-05 -3.77656450e-06]  gradient norm: 2.795111357273102e-05\n",
            "iter: 7937  x: [-99.99998618  24.99999812]  f(x): 1.9453570391410503e-10  grad at x: [ 2.76394169e-05 -3.76901138e-06]  gradient norm: 2.789521133916035e-05\n",
            "iter: 7938  x: [-99.99998621  24.99999812]  f(x): 1.9375833926999567e-10  grad at x: [ 2.75841381e-05 -3.76147336e-06]  gradient norm: 2.783942091854611e-05\n",
            "iter: 7939  x: [-99.99998624  24.99999812]  f(x): 1.9298408100309605e-10  grad at x: [ 2.75289698e-05 -3.75395041e-06]  gradient norm: 2.7783742080799416e-05\n",
            "iter: 7940  x: [-99.99998626  24.99999813]  f(x): 1.922129166308473e-10  grad at x: [ 2.74739119e-05 -3.74644251e-06]  gradient norm: 2.7728174597751458e-05\n",
            "iter: 7941  x: [-99.99998629  24.99999813]  f(x): 1.9144483368810653e-10  grad at x: [ 2.74189641e-05 -3.73894962e-06]  gradient norm: 2.7672718239313356e-05\n",
            "iter: 7942  x: [-99.99998632  24.99999813]  f(x): 1.906798201559237e-10  grad at x: [ 2.73641261e-05 -3.73147172e-06]  gradient norm: 2.7617372804517356e-05\n",
            "iter: 7943  x: [-99.99998635  24.99999814]  f(x): 1.8991786366798256e-10  grad at x: [ 2.73093979e-05 -3.72400878e-06]  gradient norm: 2.7562138064234607e-05\n",
            "iter: 7944  x: [-99.99998637  24.99999814]  f(x): 1.8915895190168386e-10  grad at x: [ 2.72547791e-05 -3.71656076e-06]  gradient norm: 2.7507013789336266e-05\n",
            "iter: 7945  x: [-99.9999864   24.99999815]  f(x): 1.8840307256484132e-10  grad at x: [ 2.72002695e-05 -3.70912764e-06]  gradient norm: 2.745199974973345e-05\n",
            "iter: 7946  x: [-99.99998643  24.99999815]  f(x): 1.8765021380775514e-10  grad at x: [ 2.71458690e-05 -3.70170938e-06]  gradient norm: 2.7397095744458398e-05\n",
            "iter: 7947  x: [-99.99998645  24.99999815]  f(x): 1.8690036343598227e-10  grad at x: [ 2.70915772e-05 -3.69430596e-06]  gradient norm: 2.734230154438227e-05\n",
            "iter: 7948  x: [-99.99998648  24.99999816]  f(x): 1.861535096825251e-10  grad at x: [ 2.70373941e-05 -3.68691735e-06]  gradient norm: 2.7287616948537306e-05\n",
            "iter: 7949  x: [-99.99998651  24.99999816]  f(x): 1.8540964043695126e-10  grad at x: [ 2.69833193e-05 -3.67954351e-06]  gradient norm: 2.723304172779466e-05\n",
            "iter: 7950  x: [-99.99998654  24.99999816]  f(x): 1.846687436318074e-10  grad at x: [ 2.69293527e-05 -3.67218443e-06]  gradient norm: 2.7178575653025484e-05\n",
            "iter: 7951  x: [-99.99998656  24.99999817]  f(x): 1.8393080722947382e-10  grad at x: [ 2.68754940e-05 -3.66484006e-06]  gradient norm: 2.71242184941409e-05\n",
            "iter: 7952  x: [-99.99998659  24.99999817]  f(x): 1.8319581962931784e-10  grad at x: [ 2.68217430e-05 -3.65751038e-06]  gradient norm: 2.706997005017315e-05\n",
            "iter: 7953  x: [-99.99998662  24.99999817]  f(x): 1.824637692702614e-10  grad at x: [ 2.67680995e-05 -3.65019535e-06]  gradient norm: 2.701583012015447e-05\n",
            "iter: 7954  x: [-99.99998664  24.99999818]  f(x): 1.8173464387139848e-10  grad at x: [ 2.67145633e-05 -3.64289496e-06]  gradient norm: 2.696179844679494e-05\n",
            "iter: 7955  x: [-99.99998667  24.99999818]  f(x): 1.8100843233386623e-10  grad at x: [ 2.66611342e-05 -3.63560917e-06]  gradient norm: 2.6907874857287873e-05\n",
            "iter: 7956  x: [-99.9999867   24.99999819]  f(x): 1.8028512246062115e-10  grad at x: [ 2.66078119e-05 -3.62833795e-06]  gradient norm: 2.685405909434335e-05\n",
            "iter: 7957  x: [-99.99998672  24.99999819]  f(x): 1.7956470323187044e-10  grad at x: [ 2.65545963e-05 -3.62108128e-06]  gradient norm: 2.6800350985154688e-05\n",
            "iter: 7958  x: [-99.99998675  24.99999819]  f(x): 1.7884716252112088e-10  grad at x: [ 2.65014871e-05 -3.61383911e-06]  gradient norm: 2.6746750271471924e-05\n",
            "iter: 7959  x: [-99.99998678  24.9999982 ]  f(x): 1.7813248940008562e-10  grad at x: [ 2.64484841e-05 -3.60661144e-06]  gradient norm: 2.6693256781448427e-05\n",
            "iter: 7960  x: [-99.9999868  24.9999982]  f(x): 1.774206718252402e-10  grad at x: [ 2.63955871e-05 -3.59939821e-06]  gradient norm: 2.6639870256834223e-05\n",
            "iter: 7961  x: [-99.99998683  24.9999982 ]  f(x): 1.767116989336473e-10  grad at x: [ 2.63427960e-05 -3.59219941e-06]  gradient norm: 2.6586590524822644e-05\n",
            "iter: 7962  x: [-99.99998685  24.99999821]  f(x): 1.7600555916343597e-10  grad at x: [ 2.62901104e-05 -3.58501502e-06]  gradient norm: 2.653341735724488e-05\n",
            "iter: 7963  x: [-99.99998688  24.99999821]  f(x): 1.7530224096863626e-10  grad at x: [ 2.62375302e-05 -3.57784499e-06]  gradient norm: 2.6480350524012045e-05\n",
            "iter: 7964  x: [-99.99998691  24.99999821]  f(x): 1.7460173322948118e-10  grad at x: [ 2.61850551e-05 -3.57068929e-06]  gradient norm: 2.642738982415639e-05\n",
            "iter: 7965  x: [-99.99998693  24.99999822]  f(x): 1.7390402486440815e-10  grad at x: [ 2.61326850e-05 -3.56354791e-06]  gradient norm: 2.637453505671015e-05\n",
            "iter: 7966  x: [-99.99998696  24.99999822]  f(x): 1.7320910445932764e-10  grad at x: [ 2.60804196e-05 -3.55642081e-06]  gradient norm: 2.6321785992544475e-05\n",
            "iter: 7967  x: [-99.99998699  24.99999823]  f(x): 1.7251696102360035e-10  grad at x: [ 2.60282588e-05 -3.54930798e-06]  gradient norm: 2.6269142431651656e-05\n",
            "iter: 7968  x: [-99.99998701  24.99999823]  f(x): 1.7182758321004974e-10  grad at x: [ 2.59762023e-05 -3.54220936e-06]  gradient norm: 2.6216604143942804e-05\n",
            "iter: 7969  x: [-99.99998704  24.99999823]  f(x): 1.711409600932891e-10  grad at x: [ 2.59242499e-05 -3.53512495e-06]  gradient norm: 2.6164170928450158e-05\n",
            "iter: 7970  x: [-99.99998706  24.99999824]  f(x): 1.7045708078558803e-10  grad at x: [ 2.58724014e-05 -3.52805470e-06]  gradient norm: 2.6111842584205967e-05\n",
            "iter: 7971  x: [-99.99998709  24.99999824]  f(x): 1.697759344367667e-10  grad at x: [ 2.58206566e-05 -3.52099859e-06]  gradient norm: 2.6059618910242465e-05\n",
            "iter: 7972  x: [-99.99998712  24.99999824]  f(x): 1.69097509867891e-10  grad at x: [ 2.57690153e-05 -3.51395659e-06]  gradient norm: 2.6007499677430817e-05\n",
            "iter: 7973  x: [-99.99998714  24.99999825]  f(x): 1.6842179630576406e-10  grad at x: [ 2.57174773e-05 -3.50692868e-06]  gradient norm: 2.5955484684803253e-05\n",
            "iter: 7974  x: [-99.99998717  24.99999825]  f(x): 1.677487826496793e-10  grad at x: [ 2.56660423e-05 -3.49991482e-06]  gradient norm: 2.5903573703230934e-05\n",
            "iter: 7975  x: [-99.99998719  24.99999825]  f(x): 1.6707845857939327e-10  grad at x: [ 2.56147102e-05 -3.49291499e-06]  gradient norm: 2.585176656086723e-05\n",
            "iter: 7976  x: [-99.99998722  24.99999826]  f(x): 1.6641081305733302e-10  grad at x: [ 2.55634808e-05 -3.48592916e-06]  gradient norm: 2.5800063027623248e-05\n",
            "iter: 7977  x: [-99.99998724  24.99999826]  f(x): 1.6574583546075687e-10  grad at x: [ 2.55123538e-05 -3.47895730e-06]  gradient norm: 2.574846290253124e-05\n",
            "iter: 7978  x: [-99.99998727  24.99999826]  f(x): 1.650835152160586e-10  grad at x: [ 2.54613291e-05 -3.47199939e-06]  gradient norm: 2.569696598558348e-05\n",
            "iter: 7979  x: [-99.99998729  24.99999827]  f(x): 1.6442384140050578e-10  grad at x: [ 2.54104065e-05 -3.46505539e-06]  gradient norm: 2.5645572046691086e-05\n",
            "iter: 7980  x: [-99.99998732  24.99999827]  f(x): 1.6376680387628116e-10  grad at x: [ 2.53595857e-05 -3.45812528e-06]  gradient norm: 2.5594280914007424e-05\n",
            "iter: 7981  x: [-99.99998735  24.99999827]  f(x): 1.6311239179522356e-10  grad at x: [ 2.53088665e-05 -3.45120903e-06]  gradient norm: 2.5543092357443612e-05\n",
            "iter: 7982  x: [-99.99998737  24.99999828]  f(x): 1.6246059473194865e-10  grad at x: [ 2.52582488e-05 -3.44430661e-06]  gradient norm: 2.5492006176991928e-05\n",
            "iter: 7983  x: [-99.9999874   24.99999828]  f(x): 1.6181140228503706e-10  grad at x: [ 2.52077323e-05 -3.43741800e-06]  gradient norm: 2.5441022171684616e-05\n",
            "iter: 7984  x: [-99.99998742  24.99999828]  f(x): 1.6116480408924182e-10  grad at x: [ 2.51573168e-05 -3.43054317e-06]  gradient norm: 2.539014014055392e-05\n",
            "iter: 7985  x: [-99.99998745  24.99999829]  f(x): 1.6052078945859399e-10  grad at x: [ 2.51070022e-05 -3.42368208e-06]  gradient norm: 2.533935985447099e-05\n",
            "iter: 7986  x: [-99.99998747  24.99999829]  f(x): 1.5987934845810312e-10  grad at x: [ 2.50567882e-05 -3.41683472e-06]  gradient norm: 2.5288681140629152e-05\n",
            "iter: 7987  x: [-99.9999875   24.99999829]  f(x): 1.5924047048716943e-10  grad at x: [ 2.50066746e-05 -3.41000105e-06]  gradient norm: 2.5238103770859603e-05\n",
            "iter: 7988  x: [-99.99998752  24.9999983 ]  f(x): 1.586041456688324e-10  grad at x: [ 2.49566613e-05 -3.40318105e-06]  gradient norm: 2.5187627571395636e-05\n",
            "iter: 7989  x: [-99.99998755  24.9999983 ]  f(x): 1.579703634752848e-10  grad at x: [ 2.49067479e-05 -3.39637469e-06]  gradient norm: 2.5137252314068442e-05\n",
            "iter: 7990  x: [-99.99998757  24.99999831]  f(x): 1.5733911375821057e-10  grad at x: [ 2.48569344e-05 -3.38958194e-06]  gradient norm: 2.508697779791026e-05\n",
            "iter: 7991  x: [-99.9999876   24.99999831]  f(x): 1.5671038675727604e-10  grad at x: [ 2.48072206e-05 -3.38280278e-06]  gradient norm: 2.5036803850114418e-05\n",
            "iter: 7992  x: [-99.99998762  24.99999831]  f(x): 1.5608417204102327e-10  grad at x: [ 2.47576061e-05 -3.37603718e-06]  gradient norm: 2.4986730241552076e-05\n",
            "iter: 7993  x: [-99.99998765  24.99999832]  f(x): 1.5546045956718892e-10  grad at x: [ 2.47080909e-05 -3.36928510e-06]  gradient norm: 2.4936756771255473e-05\n",
            "iter: 7994  x: [-99.99998767  24.99999832]  f(x): 1.5483923969102447e-10  grad at x: [ 2.46586747e-05 -3.36254654e-06]  gradient norm: 2.4886883267377975e-05\n",
            "iter: 7995  x: [-99.9999877   24.99999832]  f(x): 1.542205020766451e-10  grad at x: [ 2.46093574e-05 -3.35582144e-06]  gradient norm: 2.48371094998307e-05\n",
            "iter: 7996  x: [-99.99998772  24.99999833]  f(x): 1.5360423679883226e-10  grad at x: [ 2.45601387e-05 -3.34910980e-06]  gradient norm: 2.478743526860593e-05\n",
            "iter: 7997  x: [-99.99998774  24.99999833]  f(x): 1.529904343035668e-10  grad at x: [ 2.45110184e-05 -3.34241157e-06]  gradient norm: 2.4737860400896987e-05\n",
            "iter: 7998  x: [-99.99998777  24.99999833]  f(x): 1.5237908438538932e-10  grad at x: [ 2.44619963e-05 -3.33572675e-06]  gradient norm: 2.4688384668535067e-05\n",
            "iter: 7999  x: [-99.99998779  24.99999834]  f(x): 1.5177017754638935e-10  grad at x: [ 2.44130723e-05 -3.32905530e-06]  gradient norm: 2.463900789775346e-05\n",
            "iter: 8000  x: [-99.99998782  24.99999834]  f(x): 1.5116370399789401e-10  grad at x: [ 2.43642462e-05 -3.32239718e-06]  gradient norm: 2.4589729888544445e-05\n",
            "iter: 8001  x: [-99.99998784  24.99999834]  f(x): 1.5055965397381313e-10  grad at x: [ 2.43155177e-05 -3.31575239e-06]  gradient norm: 2.454055043994027e-05\n",
            "iter: 8002  x: [-99.99998787  24.99999835]  f(x): 1.4995801775417086e-10  grad at x: [ 2.42668867e-05 -3.30912088e-06]  gradient norm: 2.4491469351933204e-05\n",
            "iter: 8003  x: [-99.99998789  24.99999835]  f(x): 1.4935878562969086e-10  grad at x: [ 2.42183529e-05 -3.30250264e-06]  gradient norm: 2.444248642259546e-05\n",
            "iter: 8004  x: [-99.99998792  24.99999835]  f(x): 1.4876194794877002e-10  grad at x: [ 2.41699162e-05 -3.29589763e-06]  gradient norm: 2.4393601451919314e-05\n",
            "iter: 8005  x: [-99.99998794  24.99999836]  f(x): 1.4816749508208752e-10  grad at x: [ 2.41215764e-05 -3.28930584e-06]  gradient norm: 2.4344814238937006e-05\n",
            "iter: 8006  x: [-99.99998796  24.99999836]  f(x): 1.4757541778805154e-10  grad at x: [ 2.40733332e-05 -3.28272723e-06]  gradient norm: 2.42961246118019e-05\n",
            "iter: 8007  x: [-99.99998799  24.99999836]  f(x): 1.4698570649140902e-10  grad at x: [ 2.40251866e-05 -3.27616177e-06]  gradient norm: 2.4247532368586213e-05\n",
            "iter: 8008  x: [-99.99998801  24.99999837]  f(x): 1.4639835167400808e-10  grad at x: [ 2.39771362e-05 -3.26960944e-06]  gradient norm: 2.419903730928221e-05\n",
            "iter: 8009  x: [-99.99998804  24.99999837]  f(x): 1.4581334385128223e-10  grad at x: [ 2.39291819e-05 -3.26307023e-06]  gradient norm: 2.4150639233882174e-05\n",
            "iter: 8010  x: [-99.99998806  24.99999837]  f(x): 1.452306738883881e-10  grad at x: [ 2.38813236e-05 -3.25654408e-06]  gradient norm: 2.41023379686194e-05\n",
            "iter: 8011  x: [-99.99998808  24.99999837]  f(x): 1.4465033202704593e-10  grad at x: [ 2.38335609e-05 -3.25003099e-06]  gradient norm: 2.4054133285325075e-05\n",
            "iter: 8012  x: [-99.99998811  24.99999838]  f(x): 1.440723092210237e-10  grad at x: [ 2.37858938e-05 -3.24353093e-06]  gradient norm: 2.4006025012152573e-05\n",
            "iter: 8013  x: [-99.99998813  24.99999838]  f(x): 1.4349659643158218e-10  grad at x: [ 2.37383220e-05 -3.23704387e-06]  gradient norm: 2.395801297533518e-05\n",
            "iter: 8014  x: [-99.99998815  24.99999838]  f(x): 1.4292318400019698e-10  grad at x: [ 2.36908454e-05 -3.23056978e-06]  gradient norm: 2.3910096946704082e-05\n",
            "iter: 8015  x: [-99.99998818  24.99999839]  f(x): 1.423520629645913e-10  grad at x: [ 2.36434637e-05 -3.22410864e-06]  gradient norm: 2.386227675345262e-05\n",
            "iter: 8016  x: [-99.9999882   24.99999839]  f(x): 1.4178322406891847e-10  grad at x: [ 2.35961768e-05 -3.21766042e-06]  gradient norm: 2.3814552195573063e-05\n",
            "iter: 8017  x: [-99.99998823  24.99999839]  f(x): 1.4121665841340324e-10  grad at x: [ 2.35489844e-05 -3.21122510e-06]  gradient norm: 2.376692310025875e-05\n",
            "iter: 8018  x: [-99.99998825  24.9999984 ]  f(x): 1.4065235679443596e-10  grad at x: [ 2.35018865e-05 -3.20480265e-06]  gradient norm: 2.3719389266541917e-05\n",
            "iter: 8019  x: [-99.99998827  24.9999984 ]  f(x): 1.400903100525067e-10  grad at x: [ 2.34548827e-05 -3.19839304e-06]  gradient norm: 2.3671950494414835e-05\n",
            "iter: 8020  x: [-99.9999883  24.9999984]  f(x): 1.395305090493158e-10  grad at x: [ 2.34079729e-05 -3.19199625e-06]  gradient norm: 2.3624606582909762e-05\n",
            "iter: 8021  x: [-99.99998832  24.99999841]  f(x): 1.3897294502240915e-10  grad at x: [ 2.33611570e-05 -3.18561226e-06]  gradient norm: 2.3577357360180054e-05\n",
            "iter: 8022  x: [-99.99998834  24.99999841]  f(x): 1.3841760922774211e-10  grad at x: [ 2.33144347e-05 -3.17924103e-06]  gradient norm: 2.353020265341904e-05\n",
            "iter: 8023  x: [-99.99998837  24.99999841]  f(x): 1.3786449262032834e-10  grad at x: [ 2.32678058e-05 -3.17288255e-06]  gradient norm: 2.3483142261658967e-05\n",
            "iter: 8024  x: [-99.99998839  24.99999842]  f(x): 1.3731358619870838e-10  grad at x: [ 2.32212702e-05 -3.16653679e-06]  gradient norm: 2.3436175984892106e-05\n",
            "iter: 8025  x: [-99.99998841  24.99999842]  f(x): 1.3676488098228804e-10  grad at x: [ 2.31748276e-05 -3.16020371e-06]  gradient norm: 2.3389303622150705e-05\n",
            "iter: 8026  x: [-99.99998844  24.99999842]  f(x): 1.3621836836244176e-10  grad at x: [ 2.31284780e-05 -3.15388331e-06]  gradient norm: 2.3342525001588133e-05\n",
            "iter: 8027  x: [-99.99998846  24.99999843]  f(x): 1.3567403974863647e-10  grad at x: [ 2.30822210e-05 -3.14757554e-06]  gradient norm: 2.3295839950397707e-05\n",
            "iter: 8028  x: [-99.99998848  24.99999843]  f(x): 1.351318862522612e-10  grad at x: [ 2.30360566e-05 -3.14128039e-06]  gradient norm: 2.3249248267611683e-05\n",
            "iter: 8029  x: [-99.99998851  24.99999843]  f(x): 1.3459189935437205e-10  grad at x: [ 2.29899845e-05 -3.13499783e-06]  gradient norm: 2.3202749781383416e-05\n",
            "iter: 8030  x: [-99.99998853  24.99999844]  f(x): 1.3405407023899688e-10  grad at x: [ 2.29440045e-05 -3.12872783e-06]  gradient norm: 2.315634429170519e-05\n",
            "iter: 8031  x: [-99.99998855  24.99999844]  f(x): 1.3351839009953121e-10  grad at x: [ 2.28981165e-05 -3.12247037e-06]  gradient norm: 2.311003159664921e-05\n",
            "iter: 8032  x: [-99.99998857  24.99999844]  f(x): 1.329848505189733e-10  grad at x: [ 2.28523203e-05 -3.11622544e-06]  gradient norm: 2.306381152532888e-05\n",
            "iter: 8033  x: [-99.9999886   24.99999845]  f(x): 1.3245344308690499e-10  grad at x: [ 2.28066156e-05 -3.10999299e-06]  gradient norm: 2.3017683904937524e-05\n",
            "iter: 8034  x: [-99.99998862  24.99999845]  f(x): 1.3192415909823385e-10  grad at x: [ 2.27610024e-05 -3.10377300e-06]  gradient norm: 2.297164853450739e-05\n",
            "iter: 8035  x: [-99.99998864  24.99999845]  f(x): 1.3139699021296555e-10  grad at x: [ 2.27154804e-05 -3.09756546e-06]  gradient norm: 2.2925705242191836e-05\n",
            "iter: 8036  x: [-99.99998866  24.99999845]  f(x): 1.3087192778648603e-10  grad at x: [ 2.26700494e-05 -3.09137033e-06]  gradient norm: 2.287985382702311e-05\n",
            "iter: 8037  x: [-99.99998869  24.99999846]  f(x): 1.3034896353776836e-10  grad at x: [ 2.26247093e-05 -3.08518759e-06]  gradient norm: 2.283409411715458e-05\n",
            "iter: 8038  x: [-99.99998871  24.99999846]  f(x): 1.298280892032046e-10  grad at x: [ 2.25794599e-05 -3.07901722e-06]  gradient norm: 2.278842593977957e-05\n",
            "iter: 8039  x: [-99.99998873  24.99999846]  f(x): 1.2930929623823882e-10  grad at x: [ 2.25343010e-05 -3.07285919e-06]  gradient norm: 2.274284909489036e-05\n",
            "iter: 8040  x: [-99.99998876  24.99999847]  f(x): 1.287925764377733e-10  grad at x: [ 2.24892324e-05 -3.06671347e-06]  gradient norm: 2.269736340968028e-05\n",
            "iter: 8041  x: [-99.99998878  24.99999847]  f(x): 1.2827792131683303e-10  grad at x: [ 2.24442539e-05 -3.06058004e-06]  gradient norm: 2.2651968684141608e-05\n",
            "iter: 8042  x: [-99.9999888   24.99999847]  f(x): 1.2776532272849276e-10  grad at x: [ 2.23993654e-05 -3.05445888e-06]  gradient norm: 2.2606664745467676e-05\n",
            "iter: 8043  x: [-99.99998882  24.99999848]  f(x): 1.2725477256469394e-10  grad at x: [ 2.23545667e-05 -3.04834996e-06]  gradient norm: 2.2561451421811847e-05\n",
            "iter: 8044  x: [-99.99998885  24.99999848]  f(x): 1.2674626242820755e-10  grad at x: [ 2.23098576e-05 -3.04225326e-06]  gradient norm: 2.25163285131664e-05\n",
            "iter: 8045  x: [-99.99998887  24.99999848]  f(x): 1.2623978425775628e-10  grad at x: [ 2.22652378e-05 -3.03616876e-06]  gradient norm: 2.2471295846724664e-05\n",
            "iter: 8046  x: [-99.99998889  24.99999848]  f(x): 1.257353300198578e-10  grad at x: [ 2.22207074e-05 -3.03009642e-06]  gradient norm: 2.2426353249679967e-05\n",
            "iter: 8047  x: [-99.99998891  24.99999849]  f(x): 1.25232891730234e-10  grad at x: [ 2.21762660e-05 -3.02403622e-06]  gradient norm: 2.2381500551145717e-05\n",
            "iter: 8048  x: [-99.99998893  24.99999849]  f(x): 1.2473246108539452e-10  grad at x: [ 2.21319134e-05 -3.01798815e-06]  gradient norm: 2.233673754919411e-05\n",
            "iter: 8049  x: [-99.99998896  24.99999849]  f(x): 1.2423403015799952e-10  grad at x: [ 2.20876496e-05 -3.01195217e-06]  gradient norm: 2.229206407293856e-05\n",
            "iter: 8050  x: [-99.99998898  24.9999985 ]  f(x): 1.2373759102665632e-10  grad at x: [ 2.20434743e-05 -3.00592827e-06]  gradient norm: 2.2247479949572387e-05\n",
            "iter: 8051  x: [-99.999989   24.9999985]  f(x): 1.2324313548474256e-10  grad at x: [ 2.19993873e-05 -2.99991641e-06]  gradient norm: 2.2202984978127834e-05\n",
            "iter: 8052  x: [-99.99998902  24.9999985 ]  f(x): 1.2275065600074269e-10  grad at x: [ 2.19553886e-05 -2.99391658e-06]  gradient norm: 2.2158579015879395e-05\n",
            "iter: 8053  x: [-99.99998904  24.99999851]  f(x): 1.2226014441310607e-10  grad at x: [ 2.19114778e-05 -2.98792875e-06]  gradient norm: 2.2114261860899276e-05\n",
            "iter: 8054  x: [-99.99998907  24.99999851]  f(x): 1.2177159293256807e-10  grad at x: [ 2.18676549e-05 -2.98195289e-06]  gradient norm: 2.2070033342300875e-05\n",
            "iter: 8055  x: [-99.99998909  24.99999851]  f(x): 1.2128499377563426e-10  grad at x: [ 2.18239196e-05 -2.97598898e-06]  gradient norm: 2.2025893287277522e-05\n",
            "iter: 8056  x: [-99.99998911  24.99999851]  f(x): 1.2080033888685499e-10  grad at x: [ 2.17802717e-05 -2.97003701e-06]  gradient norm: 2.19818414958215e-05\n",
            "iter: 8057  x: [-99.99998913  24.99999852]  f(x): 1.203176208473693e-10  grad at x: [ 2.17367112e-05 -2.96409694e-06]  gradient norm: 2.1937877823287218e-05\n",
            "iter: 8058  x: [-99.99998915  24.99999852]  f(x): 1.1983683165664532e-10  grad at x: [ 2.16932378e-05 -2.95816874e-06]  gradient norm: 2.1894002069666962e-05\n",
            "iter: 8059  x: [-99.99998918  24.99999852]  f(x): 1.1935796365097727e-10  grad at x: [ 2.16498513e-05 -2.95225241e-06]  gradient norm: 2.1850214063114097e-05\n",
            "iter: 8060  x: [-99.9999892   24.99999853]  f(x): 1.1888100918280584e-10  grad at x: [ 2.16065516e-05 -2.94634791e-06]  gradient norm: 2.1806513630821947e-05\n",
            "iter: 8061  x: [-99.99998922  24.99999853]  f(x): 1.184059606416409e-10  grad at x: [ 2.15633385e-05 -2.94045521e-06]  gradient norm: 2.1762900600943884e-05\n",
            "iter: 8062  x: [-99.99998924  24.99999853]  f(x): 1.1793281043302983e-10  grad at x: [ 2.15202118e-05 -2.93457430e-06]  gradient norm: 2.171937480067323e-05\n",
            "iter: 8063  x: [-99.99998926  24.99999854]  f(x): 1.1746155100980147e-10  grad at x: [ 2.14771714e-05 -2.92870515e-06]  gradient norm: 2.1675936059123397e-05\n",
            "iter: 8064  x: [-99.99998928  24.99999854]  f(x): 1.169921745256479e-10  grad at x: [ 2.14342170e-05 -2.92284774e-06]  gradient norm: 2.1632584175326618e-05\n",
            "iter: 8065  x: [-99.9999893   24.99999854]  f(x): 1.1652467378136174e-10  grad at x: [ 2.13913486e-05 -2.91700205e-06]  gradient norm: 2.1589319005597352e-05\n",
            "iter: 8066  x: [-99.99998933  24.99999854]  f(x): 1.1605904128773633e-10  grad at x: [ 2.13485659e-05 -2.91116805e-06]  gradient norm: 2.1546140377128924e-05\n",
            "iter: 8067  x: [-99.99998935  24.99999855]  f(x): 1.1559526928928442e-10  grad at x: [ 2.13058688e-05 -2.90534571e-06]  gradient norm: 2.1503048089913617e-05\n",
            "iter: 8068  x: [-99.99998937  24.99999855]  f(x): 1.1513335066329393e-10  grad at x: [ 2.12632570e-05 -2.89953502e-06]  gradient norm: 2.1460042000265883e-05\n",
            "iter: 8069  x: [-99.99998939  24.99999855]  f(x): 1.1467327769715889e-10  grad at x: [ 2.12207305e-05 -2.89373595e-06]  gradient norm: 2.141712190721796e-05\n",
            "iter: 8070  x: [-99.99998941  24.99999856]  f(x): 1.1421504332907814e-10  grad at x: [ 2.11782890e-05 -2.88794848e-06]  gradient norm: 2.1374287668044346e-05\n",
            "iter: 8071  x: [-99.99998943  24.99999856]  f(x): 1.137586398993478e-10  grad at x: [ 2.11359325e-05 -2.88217258e-06]  gradient norm: 2.1331539081777274e-05\n",
            "iter: 8072  x: [-99.99998945  24.99999856]  f(x): 1.13304060386205e-10  grad at x: [ 2.10936606e-05 -2.87640824e-06]  gradient norm: 2.12888760047312e-05\n",
            "iter: 8073  x: [-99.99998947  24.99999856]  f(x): 1.1285129748176893e-10  grad at x: [ 2.10514733e-05 -2.87065542e-06]  gradient norm: 2.124629826409946e-05\n",
            "iter: 8074  x: [-99.9999895   24.99999857]  f(x): 1.1240034361542959e-10  grad at x: [ 2.10093703e-05 -2.86491411e-06]  gradient norm: 2.120380565987432e-05\n",
            "iter: 8075  x: [-99.99998952  24.99999857]  f(x): 1.1195119184039203e-10  grad at x: [ 2.09673516e-05 -2.85918428e-06]  gradient norm: 2.116139804837025e-05\n",
            "iter: 8076  x: [-99.99998954  24.99999857]  f(x): 1.1150383493552779e-10  grad at x: [ 2.09254169e-05 -2.85346591e-06]  gradient norm: 2.11190752577406e-05\n",
            "iter: 8077  x: [-99.99998956  24.99999858]  f(x): 1.110582657050609e-10  grad at x: [ 2.08835661e-05 -2.84775898e-06]  gradient norm: 2.107683711613874e-05\n",
            "iter: 8078  x: [-99.99998958  24.99999858]  f(x): 1.1061447696839817e-10  grad at x: [ 2.08417989e-05 -2.84206346e-06]  gradient norm: 2.1034683450758006e-05\n",
            "iter: 8079  x: [-99.9999896   24.99999858]  f(x): 1.10172461580311e-10  grad at x: [ 2.08001154e-05 -2.83637933e-06]  gradient norm: 2.0992614089751758e-05\n",
            "iter: 8080  x: [-99.99998962  24.99999858]  f(x): 1.0973221242070508e-10  grad at x: [ 2.07585151e-05 -2.83070657e-06]  gradient norm: 2.0950628861273362e-05\n",
            "iter: 8081  x: [-99.99998964  24.99999859]  f(x): 1.0929372239454811e-10  grad at x: [ 2.07169981e-05 -2.82504516e-06]  gradient norm: 2.0908727593476185e-05\n",
            "iter: 8082  x: [-99.99998966  24.99999859]  f(x): 1.0885698471559838e-10  grad at x: [ 2.06755641e-05 -2.81939506e-06]  gradient norm: 2.0866910141714645e-05\n",
            "iter: 8083  x: [-99.99998968  24.99999859]  f(x): 1.084219923470246e-10  grad at x: [ 2.06342130e-05 -2.81375628e-06]  gradient norm: 2.0825176335102144e-05\n",
            "iter: 8084  x: [-99.9999897  24.9999986]  f(x): 1.0798873796417171e-10  grad at x: [ 2.05929445e-05 -2.80812876e-06]  gradient norm: 2.0783525972670922e-05\n",
            "iter: 8085  x: [-99.99998972  24.9999986 ]  f(x): 1.0755721486365929e-10  grad at x: [ 2.05517586e-05 -2.80251250e-06]  gradient norm: 2.0741958910735436e-05\n",
            "iter: 8086  x: [-99.99998974  24.9999986 ]  f(x): 1.0712741636447248e-10  grad at x: [ 2.05106551e-05 -2.79690748e-06]  gradient norm: 2.0700475005610134e-05\n",
            "iter: 8087  x: [-99.99998977  24.9999986 ]  f(x): 1.0669933522612029e-10  grad at x: [ 2.04696338e-05 -2.79131367e-06]  gradient norm: 2.06590740572873e-05\n",
            "iter: 8088  x: [-99.99998979  24.99999861]  f(x): 1.0627296480573359e-10  grad at x: [ 2.04286946e-05 -2.78573104e-06]  gradient norm: 2.061775592112135e-05\n",
            "iter: 8089  x: [-99.99998981  24.99999861]  f(x): 1.0584829791311494e-10  grad at x: [ 2.03878372e-05 -2.78015958e-06]  gradient norm: 2.0576520397104553e-05\n",
            "iter: 8090  x: [-99.99998983  24.99999861]  f(x): 1.0542532826212018e-10  grad at x: [ 2.03470615e-05 -2.77459926e-06]  gradient norm: 2.05353673706725e-05\n",
            "iter: 8091  x: [-99.99998985  24.99999862]  f(x): 1.0500404870086497e-10  grad at x: [ 2.03063674e-05 -2.76905006e-06]  gradient norm: 2.0494296640857424e-05\n",
            "iter: 8092  x: [-99.99998987  24.99999862]  f(x): 1.0458445240196004e-10  grad at x: [ 2.02657546e-05 -2.76351197e-06]  gradient norm: 2.0453308035812697e-05\n",
            "iter: 8093  x: [-99.99998989  24.99999862]  f(x): 1.0416653284965219e-10  grad at x: [ 2.02252231e-05 -2.75798494e-06]  gradient norm: 2.0412401411852766e-05\n",
            "iter: 8094  x: [-99.99998991  24.99999862]  f(x): 1.0375028325341514e-10  grad at x: [ 2.01847727e-05 -2.75246897e-06]  gradient norm: 2.0371576596170964e-05\n",
            "iter: 8095  x: [-99.99998993  24.99999863]  f(x): 1.0333569715267277e-10  grad at x: [ 2.01444031e-05 -2.74696404e-06]  gradient norm: 2.033083344604178e-05\n",
            "iter: 8096  x: [-99.99998995  24.99999863]  f(x): 1.0292276780331877e-10  grad at x: [ 2.01041143e-05 -2.74147011e-06]  gradient norm: 2.0290171788658545e-05\n",
            "iter: 8097  x: [-99.99998997  24.99999863]  f(x): 1.0251148850471603e-10  grad at x: [ 2.00639061e-05 -2.73598717e-06]  gradient norm: 2.0249591453134657e-05\n",
            "iter: 8098  x: [-99.99998999  24.99999863]  f(x): 1.0210185256062912e-10  grad at x: [ 2.00237783e-05 -2.73051519e-06]  gradient norm: 2.020909226666345e-05\n",
            "iter: 8099  x: [-99.99999001  24.99999864]  f(x): 1.0169385359237859e-10  grad at x: [ 1.99837307e-05 -2.72505416e-06]  gradient norm: 2.016867408555938e-05\n",
            "iter: 8100  x: [-99.99999003  24.99999864]  f(x): 1.0128748495933656e-10  grad at x: [ 1.99437633e-05 -2.71960405e-06]  gradient norm: 2.01283367379758e-05\n",
            "iter: 8101  x: [-99.99999005  24.99999864]  f(x): 1.0088274005419653e-10  grad at x: [ 1.99038757e-05 -2.71416484e-06]  gradient norm: 2.0088080053026126e-05\n",
            "iter: 8102  x: [-99.99999007  24.99999865]  f(x): 1.0047961255622271e-10  grad at x: [ 1.98640680e-05 -2.70873652e-06]  gradient norm: 2.0047903886064767e-05\n",
            "iter: 8103  x: [-99.99999009  24.99999865]  f(x): 1.0007809588431754e-10  grad at x: [ 1.98243398e-05 -2.70331904e-06]  gradient norm: 2.0007808064285058e-05\n",
            "iter: 8104  x: [-99.99999011  24.99999865]  f(x): 9.967818378126524e-11  grad at x: [ 1.97846911e-05 -2.69791240e-06]  gradient norm: 1.9967792444961485e-05\n",
            "iter: 8105  x: [-99.99999013  24.99999865]  f(x): 9.927986972077176e-11  grad at x: [ 1.97451218e-05 -2.69251657e-06]  gradient norm: 1.9927856856247415e-05\n",
            "iter: 8106  x: [-99.99999015  24.99999866]  f(x): 9.888314747992372e-11  grad at x: [ 1.97056315e-05 -2.68713154e-06]  gradient norm: 1.9888001154457298e-05\n",
            "iter: 8107  x: [-99.99999017  24.99999866]  f(x): 9.848801029789782e-11  grad at x: [ 1.96662203e-05 -2.68175728e-06]  gradient norm: 1.984822513958342e-05\n",
            "iter: 8108  x: [-99.99999019  24.99999866]  f(x): 9.809445226655885e-11  grad at x: [ 1.96268878e-05 -2.67639376e-06]  gradient norm: 1.9808528695141276e-05\n",
            "iter: 8109  x: [-99.99999021  24.99999866]  f(x): 9.770246695879099e-11  grad at x: [ 1.95876341e-05 -2.67104097e-06]  gradient norm: 1.976891165024428e-05\n",
            "iter: 8110  x: [-99.99999023  24.99999867]  f(x): 9.731204796101793e-11  grad at x: [ 1.95484588e-05 -2.66569889e-06]  gradient norm: 1.9729373833045784e-05\n",
            "iter: 8111  x: [-99.99999025  24.99999867]  f(x): 9.692318915047467e-11  grad at x: [ 1.95093619e-05 -2.66036749e-06]  gradient norm: 1.9689915098900214e-05\n",
            "iter: 8112  x: [-99.99999026  24.99999867]  f(x): 9.65358841673789e-11  grad at x: [ 1.94703432e-05 -2.65504676e-06]  gradient norm: 1.9650535276920973e-05\n",
            "iter: 8113  x: [-99.99999028  24.99999868]  f(x): 9.615012666534772e-11  grad at x: [ 1.94314025e-05 -2.64973666e-06]  gradient norm: 1.9611234195261422e-05\n",
            "iter: 8114  x: [-99.9999903   24.99999868]  f(x): 9.576591086259357e-11  grad at x: [ 1.93925397e-05 -2.64443719e-06]  gradient norm: 1.9572011737437067e-05\n",
            "iter: 8115  x: [-99.99999032  24.99999868]  f(x): 9.538323018940032e-11  grad at x: [ 1.93537546e-05 -2.63914831e-06]  gradient norm: 1.9532867704400223e-05\n",
            "iter: 8116  x: [-99.99999034  24.99999868]  f(x): 9.500207891496875e-11  grad at x: [ 1.93150471e-05 -2.63387002e-06]  gradient norm: 1.9493801980626434e-05\n",
            "iter: 8117  x: [-99.99999036  24.99999869]  f(x): 9.46224504955916e-11  grad at x: [ 1.92764170e-05 -2.62860227e-06]  gradient norm: 1.9454814365147935e-05\n",
            "iter: 8118  x: [-99.99999038  24.99999869]  f(x): 9.424433925119865e-11  grad at x: [ 1.92378642e-05 -2.62334507e-06]  gradient norm: 1.941590474340031e-05\n",
            "iter: 8119  x: [-99.9999904   24.99999869]  f(x): 9.386773896478762e-11  grad at x: [ 1.91993884e-05 -2.61809838e-06]  gradient norm: 1.9377072943536917e-05\n",
            "iter: 8120  x: [-99.99999042  24.99999869]  f(x): 9.34926434418003e-11  grad at x: [ 1.91609897e-05 -2.61286218e-06]  gradient norm: 1.9338318793711132e-05\n",
            "iter: 8121  x: [-99.99999044  24.9999987 ]  f(x): 9.311904678180507e-11  grad at x: [ 1.91226677e-05 -2.60763646e-06]  gradient norm: 1.9299642150237406e-05\n",
            "iter: 8122  x: [-99.99999046  24.9999987 ]  f(x): 9.274694310450683e-11  grad at x: [ 1.90844223e-05 -2.60242119e-06]  gradient norm: 1.9261042869430185e-05\n",
            "iter: 8123  x: [-99.99999048  24.9999987 ]  f(x): 9.237632627902794e-11  grad at x: [ 1.90462535e-05 -2.59721635e-06]  gradient norm: 1.922252077944284e-05\n",
            "iter: 8124  x: [-99.9999905  24.9999987]  f(x): 9.200719046680359e-11  grad at x: [ 1.90081610e-05 -2.59202192e-06]  gradient norm: 1.9184075736589822e-05\n",
            "iter: 8125  x: [-99.99999051  24.99999871]  f(x): 9.163952984923251e-11  grad at x: [ 1.89701447e-05 -2.58683788e-06]  gradient norm: 1.914570759718559e-05\n",
            "iter: 8126  x: [-99.99999053  24.99999871]  f(x): 9.127333835857871e-11  grad at x: [ 1.89322044e-05 -2.58166421e-06]  gradient norm: 1.9107416189383506e-05\n",
            "iter: 8127  x: [-99.99999055  24.99999871]  f(x): 9.090861021761629e-11  grad at x: [ 1.88943400e-05 -2.57650088e-06]  gradient norm: 1.906920136949802e-05\n",
            "iter: 8128  x: [-99.99999057  24.99999871]  f(x): 9.054533940094302e-11  grad at x: [ 1.88565513e-05 -2.57134787e-06]  gradient norm: 1.90310629656825e-05\n",
            "iter: 8129  x: [-99.99999059  24.99999872]  f(x): 9.018352018158563e-11  grad at x: [ 1.88188382e-05 -2.56620518e-06]  gradient norm: 1.8993000835211443e-05\n",
            "iter: 8130  x: [-99.99999061  24.99999872]  f(x): 8.982314683397554e-11  grad at x: [ 1.87812005e-05 -2.56107276e-06]  gradient norm: 1.8955014833439253e-05\n",
            "iter: 8131  x: [-99.99999063  24.99999872]  f(x): 8.94642136703987e-11  grad at x: [ 1.87436382e-05 -2.55595062e-06]  gradient norm: 1.891710481764043e-05\n",
            "iter: 8132  x: [-99.99999065  24.99999872]  f(x): 8.910671474774138e-11  grad at x: [ 1.87061509e-05 -2.55083872e-06]  gradient norm: 1.8879270615968338e-05\n",
            "iter: 8133  x: [-99.99999067  24.99999873]  f(x): 8.875064440982662e-11  grad at x: [ 1.86687386e-05 -2.54573704e-06]  gradient norm: 1.8841512084737425e-05\n",
            "iter: 8134  x: [-99.99999068  24.99999873]  f(x): 8.839599675516001e-11  grad at x: [ 1.86314011e-05 -2.54064557e-06]  gradient norm: 1.8803829052101065e-05\n",
            "iter: 8135  x: [-99.9999907   24.99999873]  f(x): 8.804276643223874e-11  grad at x: [ 1.85941383e-05 -2.53556428e-06]  gradient norm: 1.8766221402534794e-05\n",
            "iter: 8136  x: [-99.99999072  24.99999873]  f(x): 8.769094757936159e-11  grad at x: [ 1.85569500e-05 -2.53049315e-06]  gradient norm: 1.8728688964191976e-05\n",
            "iter: 8137  x: [-99.99999074  24.99999874]  f(x): 8.734053462837985e-11  grad at x: [ 1.85198361e-05 -2.52543217e-06]  gradient norm: 1.8691231594347105e-05\n",
            "iter: 8138  x: [-99.99999076  24.99999874]  f(x): 8.699152174973602e-11  grad at x: [ 1.84827965e-05 -2.52038130e-06]  gradient norm: 1.8653849120193506e-05\n",
            "iter: 8139  x: [-99.99999078  24.99999874]  f(x): 8.664390367735406e-11  grad at x: [ 1.84458309e-05 -2.51534054e-06]  gradient norm: 1.861654142716676e-05\n",
            "iter: 8140  x: [-99.9999908   24.99999874]  f(x): 8.629767462997117e-11  grad at x: [ 1.84089392e-05 -2.51030986e-06]  gradient norm: 1.8579308343420234e-05\n",
            "iter: 8141  x: [-99.99999081  24.99999875]  f(x): 8.595282910856753e-11  grad at x: [ 1.83721213e-05 -2.50528925e-06]  gradient norm: 1.8542149725268377e-05\n",
            "iter: 8142  x: [-99.99999083  24.99999875]  f(x): 8.560936163313001e-11  grad at x: [ 1.83353771e-05 -2.50027867e-06]  gradient norm: 1.8505065429025645e-05\n",
            "iter: 8143  x: [-99.99999085  24.99999875]  f(x): 8.526726674259874e-11  grad at x: [ 1.82987064e-05 -2.49527811e-06]  gradient norm: 1.8468055311006487e-05\n",
            "iter: 8144  x: [-99.99999087  24.99999875]  f(x): 8.492653874414087e-11  grad at x: [ 1.82621089e-05 -2.49028756e-06]  gradient norm: 1.843111920032431e-05\n",
            "iter: 8145  x: [-99.99999089  24.99999876]  f(x): 8.458717220711764e-11  grad at x: [ 1.82255847e-05 -2.48530698e-06]  gradient norm: 1.8394256952333533e-05\n",
            "iter: 8146  x: [-99.99999091  24.99999876]  f(x): 8.42491617373722e-11  grad at x: [ 1.81891335e-05 -2.48033636e-06]  gradient norm: 1.835746842430864e-05\n",
            "iter: 8147  x: [-99.99999092  24.99999876]  f(x): 8.391250220858345e-11  grad at x: [ 1.81527553e-05 -2.47537569e-06]  gradient norm: 1.8320753500725177e-05\n",
            "iter: 8148  x: [-99.99999094  24.99999876]  f(x): 8.357718774747413e-11  grad at x: [ 1.81164498e-05 -2.47042494e-06]  gradient norm: 1.8284111982535454e-05\n",
            "iter: 8149  x: [-99.99999096  24.99999877]  f(x): 8.324321325673869e-11  grad at x: [ 1.80802168e-05 -2.46548409e-06]  gradient norm: 1.8247543753254977e-05\n",
            "iter: 8150  x: [-99.99999098  24.99999877]  f(x): 8.291057341671802e-11  grad at x: [ 1.80440564e-05 -2.46055312e-06]  gradient norm: 1.8211048670158238e-05\n",
            "iter: 8151  x: [-99.999991    24.99999877]  f(x): 8.257926266157389e-11  grad at x: [ 1.80079683e-05 -2.45563201e-06]  gradient norm: 1.8174626561398602e-05\n",
            "iter: 8152  x: [-99.99999101  24.99999877]  f(x): 8.224927595678003e-11  grad at x: [ 1.79719524e-05 -2.45072075e-06]  gradient norm: 1.8138277311451605e-05\n",
            "iter: 8153  x: [-99.99999103  24.99999878]  f(x): 8.192060777440771e-11  grad at x: [ 1.79360084e-05 -2.44581931e-06]  gradient norm: 1.810200074847062e-05\n",
            "iter: 8154  x: [-99.99999105  24.99999878]  f(x): 8.159325312435823e-11  grad at x: [ 1.79001364e-05 -2.44092767e-06]  gradient norm: 1.8065796757891218e-05\n",
            "iter: 8155  x: [-99.99999107  24.99999878]  f(x): 8.12672065163471e-11  grad at x: [ 1.78643362e-05 -2.43604581e-06]  gradient norm: 1.8029665167866774e-05\n",
            "iter: 8156  x: [-99.99999109  24.99999878]  f(x): 8.094246273374301e-11  grad at x: [ 1.78286075e-05 -2.43117372e-06]  gradient norm: 1.7993605834711732e-05\n",
            "iter: 8157  x: [-99.9999911   24.99999879]  f(x): 8.061901657811904e-11  grad at x: [ 1.77929503e-05 -2.42631137e-06]  gradient norm: 1.7957618614740546e-05\n",
            "iter: 8158  x: [-99.99999112  24.99999879]  f(x): 8.029686287780357e-11  grad at x: [ 1.77573644e-05 -2.42145875e-06]  gradient norm: 1.792170336522771e-05\n",
            "iter: 8159  x: [-99.99999114  24.99999879]  f(x): 7.997599672241396e-11  grad at x: [ 1.77218496e-05 -2.41661583e-06]  gradient norm: 1.7885859970648765e-05\n",
            "iter: 8160  x: [-99.99999116  24.99999879]  f(x): 7.96564127149243e-11  grad at x: [ 1.7686406e-05 -2.4117826e-06]  gradient norm: 1.7850088259157073e-05\n",
            "iter: 8161  x: [-99.99999117  24.9999988 ]  f(x): 7.933810573771071e-11  grad at x: [ 1.76510331e-05 -2.40695903e-06]  gradient norm: 1.7814388088027127e-05\n",
            "iter: 8162  x: [-99.99999119  24.9999988 ]  f(x): 7.902107067395891e-11  grad at x: [ 1.76157311e-05 -2.40214511e-06]  gradient norm: 1.7778759312613343e-05\n",
            "iter: 8163  x: [-99.99999121  24.9999988 ]  f(x): 7.870530244185226e-11  grad at x: [ 1.75804996e-05 -2.39734082e-06]  gradient norm: 1.774320179019021e-05\n",
            "iter: 8164  x: [-99.99999123  24.9999988 ]  f(x): 7.839079597734967e-11  grad at x: [ 1.75453386e-05 -2.39254614e-06]  gradient norm: 1.7707715378032218e-05\n",
            "iter: 8165  x: [-99.99999124  24.99999881]  f(x): 7.80775464660044e-11  grad at x: [ 1.75102479e-05 -2.38776105e-06]  gradient norm: 1.7672299959654872e-05\n",
            "iter: 8166  x: [-99.99999126  24.99999881]  f(x): 7.77655486294451e-11  grad at x: [ 1.74752275e-05 -2.38298553e-06]  gradient norm: 1.763695536417157e-05\n",
            "iter: 8167  x: [-99.99999128  24.99999881]  f(x): 7.745479744830838e-11  grad at x: [ 1.74402770e-05 -2.37821956e-06]  gradient norm: 1.7601681447896775e-05\n",
            "iter: 8168  x: [-99.9999913   24.99999881]  f(x): 7.714528817664878e-11  grad at x: [ 1.74053964e-05 -2.37346312e-06]  gradient norm: 1.7566478096266055e-05\n",
            "iter: 8169  x: [-99.99999131  24.99999882]  f(x): 7.683701558194701e-11  grad at x: [ 1.73705857e-05 -2.36871620e-06]  gradient norm: 1.7531345137432782e-05\n",
            "iter: 8170  x: [-99.99999133  24.99999882]  f(x): 7.65299749439179e-11  grad at x: [ 1.73358445e-05 -2.36397877e-06]  gradient norm: 1.7496282455872493e-05\n",
            "iter: 8171  x: [-99.99999135  24.99999882]  f(x): 7.622416106606502e-11  grad at x: [ 1.73011728e-05 -2.35925081e-06]  gradient norm: 1.7461289879738555e-05\n",
            "iter: 8172  x: [-99.99999137  24.99999882]  f(x): 7.591956927041331e-11  grad at x: [ 1.72665704e-05 -2.35453231e-06]  gradient norm: 1.7426367294466545e-05\n",
            "iter: 8173  x: [-99.99999138  24.99999883]  f(x): 7.561619464111537e-11  grad at x: [ 1.72320373e-05 -2.34982324e-06]  gradient norm: 1.7391514556370918e-05\n",
            "iter: 8174  x: [-99.9999914   24.99999883]  f(x): 7.531403228799899e-11  grad at x: [ 1.71975732e-05 -2.34512360e-06]  gradient norm: 1.735673152272616e-05\n",
            "iter: 8175  x: [-99.99999142  24.99999883]  f(x): 7.501307732148906e-11  grad at x: [ 1.71631781e-05 -2.34043335e-06]  gradient norm: 1.732201804888669e-05\n",
            "iter: 8176  x: [-99.99999144  24.99999883]  f(x): 7.471332513763114e-11  grad at x: [ 1.71288517e-05 -2.33575248e-06]  gradient norm: 1.7287374021248125e-05\n",
            "iter: 8177  x: [-99.99999145  24.99999883]  f(x): 7.441477063688162e-11  grad at x: [ 1.70945940e-05 -2.33108098e-06]  gradient norm: 1.7252799267003788e-05\n",
            "iter: 8178  x: [-99.99999147  24.99999884]  f(x): 7.411740924027271e-11  grad at x: [ 1.70604048e-05 -2.32641882e-06]  gradient norm: 1.721829367158926e-05\n",
            "iter: 8179  x: [-99.99999149  24.99999884]  f(x): 7.382123613371854e-11  grad at x: [ 1.70262840e-05 -2.32176598e-06]  gradient norm: 1.7183857091318997e-05\n",
            "iter: 8180  x: [-99.9999915   24.99999884]  f(x): 7.352624652841597e-11  grad at x: [ 1.69922315e-05 -2.31712244e-06]  gradient norm: 1.714948938346748e-05\n",
            "iter: 8181  x: [-99.99999152  24.99999884]  f(x): 7.323243564428151e-11  grad at x: [ 1.6958247e-05 -2.3124882e-06]  gradient norm: 1.7115190404349173e-05\n",
            "iter: 8182  x: [-99.99999154  24.99999885]  f(x): 7.293979871818352e-11  grad at x: [ 1.69243305e-05 -2.30786322e-06]  gradient norm: 1.7080960010278523e-05\n",
            "iter: 8183  x: [-99.99999155  24.99999885]  f(x): 7.264833125210391e-11  grad at x: [ 1.68904818e-05 -2.30324749e-06]  gradient norm: 1.7046798086691107e-05\n",
            "iter: 8184  x: [-99.99999157  24.99999885]  f(x): 7.235802851517767e-11  grad at x: [ 1.68567009e-05 -2.29864099e-06]  gradient norm: 1.7012704489901385e-05\n",
            "iter: 8185  x: [-99.99999159  24.99999885]  f(x): 7.206888580150004e-11  grad at x: [ 1.68229874e-05 -2.29404371e-06]  gradient norm: 1.6978679077183837e-05\n",
            "iter: 8186  x: [-99.99999161  24.99999886]  f(x): 7.178089865231932e-11  grad at x: [ 1.67893415e-05 -2.28945562e-06]  gradient norm: 1.694472173301401e-05\n",
            "iter: 8187  x: [-99.99999162  24.99999886]  f(x): 7.149406215557753e-11  grad at x: [ 1.67557628e-05 -2.28487671e-06]  gradient norm: 1.69108322865053e-05\n",
            "iter: 8188  x: [-99.99999164  24.99999886]  f(x): 7.120837187679844e-11  grad at x: [ 1.67222513e-05 -2.28030696e-06]  gradient norm: 1.6877010621173223e-05\n",
            "iter: 8189  x: [-99.99999166  24.99999886]  f(x): 7.092382318337748e-11  grad at x: [ 1.66888068e-05 -2.27574635e-06]  gradient norm: 1.6843256595252293e-05\n",
            "iter: 8190  x: [-99.99999167  24.99999886]  f(x): 7.064041167163386e-11  grad at x: [ 1.66554292e-05 -2.27119485e-06]  gradient norm: 1.6809570092258023e-05\n",
            "iter: 8191  x: [-99.99999169  24.99999887]  f(x): 7.035813249625808e-11  grad at x: [ 1.66221183e-05 -2.26665246e-06]  gradient norm: 1.6775950941303814e-05\n",
            "iter: 8192  x: [-99.99999171  24.99999887]  f(x): 7.007698130173087e-11  grad at x: [ 1.65888740e-05 -2.26211915e-06]  gradient norm: 1.6742399027825237e-05\n",
            "iter: 8193  x: [-99.99999172  24.99999887]  f(x): 6.979695373898915e-11  grad at x: [ 1.65556963e-05 -2.25759491e-06]  gradient norm: 1.670891423629784e-05\n",
            "iter: 8194  x: [-99.99999174  24.99999887]  f(x): 6.95180450118767e-11  grad at x: [ 1.65225849e-05 -2.25307973e-06]  gradient norm: 1.6675496395835022e-05\n",
            "iter: 8195  x: [-99.99999176  24.99999888]  f(x): 6.924025079507753e-11  grad at x: [ 1.64895397e-05 -2.24857357e-06]  gradient norm: 1.6642145389952286e-05\n",
            "iter: 8196  x: [-99.99999177  24.99999888]  f(x): 6.89635668016712e-11  grad at x: [ 1.64565606e-05 -2.24407642e-06]  gradient norm: 1.6608861105045248e-05\n",
            "iter: 8197  x: [-99.99999179  24.99999888]  f(x): 6.868798827628353e-11  grad at x: [ 1.64236475e-05 -2.23958827e-06]  gradient norm: 1.6575643369267275e-05\n",
            "iter: 8198  x: [-99.9999918   24.99999888]  f(x): 6.841351118039345e-11  grad at x: [ 1.63908002e-05 -2.23510909e-06]  gradient norm: 1.654249209525499e-05\n",
            "iter: 8199  x: [-99.99999182  24.99999888]  f(x): 6.814013079844448e-11  grad at x: [ 1.63580186e-05 -2.23063888e-06]  gradient norm: 1.6509407112121803e-05\n",
            "iter: 8200  x: [-99.99999184  24.99999889]  f(x): 6.786784288887472e-11  grad at x: [ 1.63253026e-05 -2.22617760e-06]  gradient norm: 1.6476388304343245e-05\n",
            "iter: 8201  x: [-99.99999185  24.99999889]  f(x): 6.759664300068838e-11  grad at x: [ 1.62926520e-05 -2.22172525e-06]  gradient norm: 1.6443435529193817e-05\n",
            "iter: 8202  x: [-99.99999187  24.99999889]  f(x): 6.732652692203796e-11  grad at x: [ 1.62600667e-05 -2.21728180e-06]  gradient norm: 1.641054867114905e-05\n",
            "iter: 8203  x: [-99.99999189  24.99999889]  f(x): 6.705749023245403e-11  grad at x: [ 1.62275466e-05 -2.21284724e-06]  gradient norm: 1.637772758748344e-05\n",
            "iter: 8204  x: [-99.9999919  24.9999989]  f(x): 6.678952851948886e-11  grad at x: [ 1.61950915e-05 -2.20842154e-06]  gradient norm: 1.6344972134511438e-05\n",
            "iter: 8205  x: [-99.99999192  24.9999989 ]  f(x): 6.652263762409416e-11  grad at x: [ 1.61627013e-05 -2.20400470e-06]  gradient norm: 1.631228219766862e-05\n",
            "iter: 8206  x: [-99.99999193  24.9999989 ]  f(x): 6.625681316411702e-11  grad at x: [ 1.61303759e-05 -2.19959669e-06]  gradient norm: 1.6279657633269443e-05\n",
            "iter: 8207  x: [-99.99999195  24.9999989 ]  f(x): 6.599205100976647e-11  grad at x: [ 1.60981152e-05 -2.19519750e-06]  gradient norm: 1.6247098326749485e-05\n",
            "iter: 8208  x: [-99.99999197  24.9999989 ]  f(x): 6.572834680901342e-11  grad at x: [ 1.60659189e-05 -2.19080711e-06]  gradient norm: 1.6214604134423193e-05\n",
            "iter: 8209  x: [-99.99999198  24.99999891]  f(x): 6.546569623330358e-11  grad at x: [ 1.60337871e-05 -2.18642550e-06]  gradient norm: 1.618217491356506e-05\n",
            "iter: 8210  x: [-99.999992    24.99999891]  f(x): 6.520409541672488e-11  grad at x: [ 1.60017195e-05 -2.18205265e-06]  gradient norm: 1.6149810576811714e-05\n",
            "iter: 8211  x: [-99.99999202  24.99999891]  f(x): 6.494353983224746e-11  grad at x: [ 1.59697161e-05 -2.17768854e-06]  gradient norm: 1.6117510953276558e-05\n",
            "iter: 8212  x: [-99.99999203  24.99999891]  f(x): 6.468402541543105e-11  grad at x: [ 1.59377767e-05 -2.17333316e-06]  gradient norm: 1.6085275927435132e-05\n",
            "iter: 8213  x: [-99.99999205  24.99999892]  f(x): 6.442554812325577e-11  grad at x: [ 1.59059011e-05 -2.16898650e-06]  gradient norm: 1.6053105384723016e-05\n",
            "iter: 8214  x: [-99.99999206  24.99999892]  f(x): 6.41681037007331e-11  grad at x: [ 1.58740893e-05 -2.16464853e-06]  gradient norm: 1.6020999182414697e-05\n",
            "iter: 8215  x: [-99.99999208  24.99999892]  f(x): 6.391168790058294e-11  grad at x: [ 1.58423411e-05 -2.16031923e-06]  gradient norm: 1.5988957176824627e-05\n",
            "iter: 8216  x: [-99.99999209  24.99999892]  f(x): 6.365629671560722e-11  grad at x: [ 1.58106564e-05 -2.15599859e-06]  gradient norm: 1.595697925242835e-05\n",
            "iter: 8217  x: [-99.99999211  24.99999892]  f(x): 6.34019261598076e-11  grad at x: [ 1.57790351e-05 -2.15168659e-06]  gradient norm: 1.5925065294661443e-05\n",
            "iter: 8218  x: [-99.99999213  24.99999893]  f(x): 6.314857203685798e-11  grad at x: [ 1.57474771e-05 -2.14738322e-06]  gradient norm: 1.5893215160798393e-05\n",
            "iter: 8219  x: [-99.99999214  24.99999893]  f(x): 6.289623038136706e-11  grad at x: [ 1.57159821e-05 -2.14308845e-06]  gradient norm: 1.5861428735314743e-05\n",
            "iter: 8220  x: [-99.99999216  24.99999893]  f(x): 6.264489702609095e-11  grad at x: [ 1.56845501e-05 -2.13880227e-06]  gradient norm: 1.5829705875484983e-05\n",
            "iter: 8221  x: [-99.99999217  24.99999893]  f(x): 6.239456804135609e-11  grad at x: [ 1.56531810e-05 -2.13452467e-06]  gradient norm: 1.5798046466744688e-05\n",
            "iter: 8222  x: [-99.99999219  24.99999893]  f(x): 6.2145239281221e-11  grad at x: [ 1.56218747e-05 -2.13025562e-06]  gradient norm: 1.5766450365408315e-05\n",
            "iter: 8223  x: [-99.9999922   24.99999894]  f(x): 6.189690683640104e-11  grad at x: [ 1.55906309e-05 -2.12599511e-06]  gradient norm: 1.5734917455951402e-05\n",
            "iter: 8224  x: [-99.99999222  24.99999894]  f(x): 6.164956682597021e-11  grad at x: [ 1.55594497e-05 -2.12174312e-06]  gradient norm: 1.5703447624769562e-05\n",
            "iter: 8225  x: [-99.99999224  24.99999894]  f(x): 6.140321513888879e-11  grad at x: [ 1.55283307e-05 -2.11749963e-06]  gradient norm: 1.5672040727217217e-05\n",
            "iter: 8226  x: [-99.99999225  24.99999894]  f(x): 6.115784792190579e-11  grad at x: [ 1.54972741e-05 -2.11326464e-06]  gradient norm: 1.5640696649689974e-05\n",
            "iter: 8227  x: [-99.99999227  24.99999895]  f(x): 6.091346110004394e-11  grad at x: [ 1.54662795e-05 -2.10903811e-06]  gradient norm: 1.560941524850229e-05\n",
            "iter: 8228  x: [-99.99999228  24.99999895]  f(x): 6.067005084004412e-11  grad at x: [ 1.54353470e-05 -2.10482003e-06]  gradient norm: 1.557819640908974e-05\n",
            "iter: 8229  x: [-99.9999923   24.99999895]  f(x): 6.04276133142156e-11  grad at x: [ 1.54044763e-05 -2.10061039e-06]  gradient norm: 1.5547040015927867e-05\n",
            "iter: 8230  x: [-99.99999231  24.99999895]  f(x): 6.018614449689781e-11  grad at x: [ 1.53736673e-05 -2.09640918e-06]  gradient norm: 1.551594592629116e-05\n",
            "iter: 8231  x: [-99.99999233  24.99999895]  f(x): 5.994564058774052e-11  grad at x: [ 1.53429200e-05 -2.09221636e-06]  gradient norm: 1.548491402465516e-05\n",
            "iter: 8232  x: [-99.99999234  24.99999896]  f(x): 5.970609780678973e-11  grad at x: [ 1.53122341e-05 -2.08803192e-06]  gradient norm: 1.5453944196455443e-05\n",
            "iter: 8233  x: [-99.99999236  24.99999896]  f(x): 5.946751216980958e-11  grad at x: [ 1.52816096e-05 -2.08385585e-06]  gradient norm: 1.5423036298966502e-05\n",
            "iter: 8234  x: [-99.99999237  24.99999896]  f(x): 5.92298799238793e-11  grad at x: [ 1.52510464e-05 -2.07968814e-06]  gradient norm: 1.539219021762391e-05\n",
            "iter: 8235  x: [-99.99999239  24.99999896]  f(x): 5.899319732151101e-11  grad at x: [ 1.52205443e-05 -2.07552877e-06]  gradient norm: 1.5361405836903215e-05\n",
            "iter: 8236  x: [-99.9999924   24.99999896]  f(x): 5.875746041954179e-11  grad at x: [ 1.51901032e-05 -2.07137771e-06]  gradient norm: 1.5330683014078896e-05\n",
            "iter: 8237  x: [-99.99999242  24.99999897]  f(x): 5.852266571279235e-11  grad at x: [ 1.51597230e-05 -2.06723496e-06]  gradient norm: 1.5300021661787587e-05\n",
            "iter: 8238  x: [-99.99999244  24.99999897]  f(x): 5.828880906943846e-11  grad at x: [ 1.51294036e-05 -2.06310049e-06]  gradient norm: 1.5269421609142694e-05\n",
            "iter: 8239  x: [-99.99999245  24.99999897]  f(x): 5.805588701747633e-11  grad at x: [ 1.50991448e-05 -2.05897429e-06]  gradient norm: 1.5238882769740876e-05\n",
            "iter: 8240  x: [-99.99999247  24.99999897]  f(x): 5.7823895660219873e-11  grad at x: [ 1.50689465e-05 -2.05485634e-06]  gradient norm: 1.5208404999896586e-05\n",
            "iter: 8241  x: [-99.99999248  24.99999897]  f(x): 5.759283133634449e-11  grad at x: [ 1.50388086e-05 -2.05074662e-06]  gradient norm: 1.5177988185045406e-05\n",
            "iter: 8242  x: [-99.9999925   24.99999898]  f(x): 5.7362690397075185e-11  grad at x: [ 1.50087310e-05 -2.04664513e-06]  gradient norm: 1.514763221062291e-05\n",
            "iter: 8243  x: [-99.99999251  24.99999898]  f(x): 5.7133468993291404e-11  grad at x: [ 1.49787135e-05 -2.04255184e-06]  gradient norm: 1.5117336933903591e-05\n",
            "iter: 8244  x: [-99.99999253  24.99999898]  f(x): 5.6905163707682495e-11  grad at x: [ 1.49487561e-05 -2.03846674e-06]  gradient norm: 1.5087102267524071e-05\n",
            "iter: 8245  x: [-99.99999254  24.99999898]  f(x): 5.6677770716944505e-11  grad at x: [ 1.49188586e-05 -2.03438981e-06]  gradient norm: 1.5056928068758846e-05\n",
            "iter: 8246  x: [-99.99999256  24.99999898]  f(x): 5.6451286416252395e-11  grad at x: [ 1.48890209e-05 -2.03032103e-06]  gradient norm: 1.5026814222083455e-05\n",
            "iter: 8247  x: [-99.99999257  24.99999899]  f(x): 5.622570700924564e-11  grad at x: [ 1.48592428e-05 -2.02626038e-06]  gradient norm: 1.4996760584772386e-05\n",
            "iter: 8248  x: [-99.99999259  24.99999899]  f(x): 5.600102913506936e-11  grad at x: [ 1.48295243e-05 -2.02220786e-06]  gradient norm: 1.4966767070422304e-05\n",
            "iter: 8249  x: [-99.9999926   24.99999899]  f(x): 5.57772490228416e-11  grad at x: [ 1.47998653e-05 -2.01816344e-06]  gradient norm: 1.49368335363077e-05\n",
            "iter: 8250  x: [-99.99999261  24.99999899]  f(x): 5.555436311837249e-11  grad at x: [ 1.47702656e-05 -2.01412711e-06]  gradient norm: 1.4906959866904116e-05\n",
            "iter: 8251  x: [-99.99999263  24.99999899]  f(x): 5.5332367894039625e-11  grad at x: [ 1.47407250e-05 -2.01009886e-06]  gradient norm: 1.4877145948607163e-05\n",
            "iter: 8252  x: [-99.99999264  24.999999  ]  f(x): 5.5111259812975615e-11  grad at x: [ 1.47112436e-05 -2.00607866e-06]  gradient norm: 1.4847391664932344e-05\n",
            "iter: 8253  x: [-99.99999266  24.999999  ]  f(x): 5.489103516328337e-11  grad at x: [ 1.46818211e-05 -2.00206650e-06]  gradient norm: 1.4817696874114191e-05\n",
            "iter: 8254  x: [-99.99999267  24.999999  ]  f(x): 5.4671690649053514e-11  grad at x: [ 1.46524575e-05 -1.99806237e-06]  gradient norm: 1.4788061488789329e-05\n",
            "iter: 8255  x: [-99.99999269  24.999999  ]  f(x): 5.4453222576322836e-11  grad at x: [ 1.46231525e-05 -1.99406624e-06]  gradient norm: 1.4758485366232245e-05\n",
            "iter: 8256  x: [-99.9999927  24.999999 ]  f(x): 5.423562747223914e-11  grad at x: [ 1.45939062e-05 -1.99007810e-06]  gradient norm: 1.472896839187852e-05\n",
            "iter: 8257  x: [-99.99999272  24.99999901]  f(x): 5.401890187596795e-11  grad at x: [ 1.45647184e-05 -1.98609795e-06]  gradient norm: 1.4699510451163732e-05\n",
            "iter: 8258  x: [-99.99999273  24.99999901]  f(x): 5.380304233161676e-11  grad at x: [ 1.45355890e-05 -1.98212575e-06]  gradient norm: 1.467011142856342e-05\n",
            "iter: 8259  x: [-99.99999275  24.99999901]  f(x): 5.358804540232736e-11  grad at x: [ 1.45065178e-05 -1.97816150e-06]  gradient norm: 1.464077120951316e-05\n",
            "iter: 8260  x: [-99.99999276  24.99999901]  f(x): 5.3373907663157734e-11  grad at x: [ 1.44775048e-05 -1.97420518e-06]  gradient norm: 1.4611489679448531e-05\n",
            "iter: 8261  x: [-99.99999278  24.99999901]  f(x): 5.3160625495722206e-11  grad at x: [ 1.44485498e-05 -1.97025677e-06]  gradient norm: 1.4582266695644022e-05\n",
            "iter: 8262  x: [-99.99999279  24.99999902]  f(x): 5.2948195697972126e-11  grad at x: [ 1.44196527e-05 -1.96631625e-06]  gradient norm: 1.4553102170736262e-05\n",
            "iter: 8263  x: [-99.9999928   24.99999902]  f(x): 5.273661467604618e-11  grad at x: [ 1.43908134e-05 -1.96238362e-06]  gradient norm: 1.4523995961999738e-05\n",
            "iter: 8264  x: [-99.99999282  24.99999902]  f(x): 5.252587925769448e-11  grad at x: [ 1.43620317e-05 -1.95845885e-06]  gradient norm: 1.449494798303112e-05\n",
            "iter: 8265  x: [-99.99999283  24.99999902]  f(x): 5.2315985873400786e-11  grad at x: [ 1.43333077e-05 -1.95454194e-06]  gradient norm: 1.4465958091104892e-05\n",
            "iter: 8266  x: [-99.99999285  24.99999902]  f(x): 5.2106931163344636e-11  grad at x: [ 1.43046411e-05 -1.95063285e-06]  gradient norm: 1.44370261706966e-05\n",
            "iter: 8267  x: [-99.99999286  24.99999903]  f(x): 5.189871179327039e-11  grad at x: [ 1.42760318e-05 -1.94673159e-06]  gradient norm: 1.4408152108201855e-05\n",
            "iter: 8268  x: [-99.99999288  24.99999903]  f(x): 5.1691324629174887e-11  grad at x: [ 1.42474797e-05 -1.94283813e-06]  gradient norm: 1.4379335816257285e-05\n",
            "iter: 8269  x: [-99.99999289  24.99999903]  f(x): 5.1484766149861854e-11  grad at x: [ 1.42189848e-05 -1.93895245e-06]  gradient norm: 1.435057715213738e-05\n",
            "iter: 8270  x: [-99.9999929   24.99999903]  f(x): 5.1279033042118906e-11  grad at x: [ 1.41905468e-05 -1.93507454e-06]  gradient norm: 1.4321876000317683e-05\n",
            "iter: 8271  x: [-99.99999292  24.99999903]  f(x): 5.10741220180552e-11  grad at x: [ 1.41621657e-05 -1.93120439e-06]  gradient norm: 1.4293232247193802e-05\n",
            "iter: 8272  x: [-99.99999293  24.99999904]  f(x): 5.08700297875419e-11  grad at x: [ 1.41338414e-05 -1.92734198e-06]  gradient norm: 1.4264645777241285e-05\n",
            "iter: 8273  x: [-99.99999295  24.99999904]  f(x): 5.066675307881728e-11  grad at x: [ 1.41055737e-05 -1.92348730e-06]  gradient norm: 1.4236116475895705e-05\n",
            "iter: 8274  x: [-99.99999296  24.99999904]  f(x): 5.046428863156543e-11  grad at x: [ 1.40773625e-05 -1.91964032e-06]  gradient norm: 1.4207644228592639e-05\n",
            "iter: 8275  x: [-99.99999298  24.99999904]  f(x): 5.02626333965348e-11  grad at x: [ 1.40492078e-05 -1.91580104e-06]  gradient norm: 1.4179228948928754e-05\n",
            "iter: 8276  x: [-99.99999299  24.99999904]  f(x): 5.006178393575318e-11  grad at x: [ 1.40211094e-05 -1.91196943e-06]  gradient norm: 1.4150870494178537e-05\n",
            "iter: 8277  x: [-99.999993    24.99999905]  f(x): 4.986173701626939e-11  grad at x: [ 1.39930672e-05 -1.90814549e-06]  gradient norm: 1.412256874881753e-05\n",
            "iter: 8278  x: [-99.99999302  24.99999905]  f(x): 4.966248943003291e-11  grad at x: [ 1.3965081e-05 -1.9043292e-06]  gradient norm: 1.4094323599241349e-05\n",
            "iter: 8279  x: [-99.99999303  24.99999905]  f(x): 4.946403816477662e-11  grad at x: [ 1.39371509e-05 -1.90052054e-06]  gradient norm: 1.406613495808662e-05\n",
            "iter: 8280  x: [-99.99999305  24.99999905]  f(x): 4.926637982936656e-11  grad at x: [ 1.39092766e-05 -1.89671950e-06]  gradient norm: 1.4038002682627832e-05\n",
            "iter: 8281  x: [-99.99999306  24.99999905]  f(x): 4.90695114400076e-11  grad at x: [ 1.38814580e-05 -1.89292606e-06]  gradient norm: 1.4009926686461652e-05\n",
            "iter: 8282  x: [-99.99999307  24.99999906]  f(x): 4.887342962876653e-11  grad at x: [ 1.38536951e-05 -1.88914021e-06]  gradient norm: 1.3981906826862569e-05\n",
            "iter: 8283  x: [-99.99999309  24.99999906]  f(x): 4.8678131426703504e-11  grad at x: [ 1.38259877e-05 -1.88536193e-06]  gradient norm: 1.395394301646721e-05\n",
            "iter: 8284  x: [-99.9999931   24.99999906]  f(x): 4.848361369176522e-11  grad at x: [ 1.37983357e-05 -1.88159121e-06]  gradient norm: 1.392603514167119e-05\n",
            "iter: 8285  x: [-99.99999311  24.99999906]  f(x): 4.828987308390212e-11  grad at x: [ 1.37707391e-05 -1.87782803e-06]  gradient norm: 1.389818305878896e-05\n",
            "iter: 8286  x: [-99.99999313  24.99999906]  f(x): 4.809690667300883e-11  grad at x: [ 1.37431976e-05 -1.87407237e-06]  gradient norm: 1.3870386681417188e-05\n",
            "iter: 8287  x: [-99.99999314  24.99999906]  f(x): 4.790471135017563e-11  grad at x: [ 1.37157112e-05 -1.87032423e-06]  gradient norm: 1.3842645895951487e-05\n",
            "iter: 8288  x: [-99.99999316  24.99999907]  f(x): 4.771328419869704e-11  grad at x: [ 1.36882798e-05 -1.86658358e-06]  gradient norm: 1.3814960615028483e-05\n",
            "iter: 8289  x: [-99.99999317  24.99999907]  f(x): 4.7522621923064683e-11  grad at x: [ 1.36609032e-05 -1.86285041e-06]  gradient norm: 1.378733069496263e-05\n",
            "iter: 8290  x: [-99.99999318  24.99999907]  f(x): 4.733272144727017e-11  grad at x: [ 1.36335814e-05 -1.85912471e-06]  gradient norm: 1.3759756022149545e-05\n",
            "iter: 8291  x: [-99.9999932   24.99999907]  f(x): 4.714357989292058e-11  grad at x: [ 1.36063142e-05 -1.85540646e-06]  gradient norm: 1.3732236510185888e-05\n",
            "iter: 8292  x: [-99.99999321  24.99999907]  f(x): 4.695519419797679e-11  grad at x: [ 1.35791016e-05 -1.85169565e-06]  gradient norm: 1.3704772044507241e-05\n",
            "iter: 8293  x: [-99.99999322  24.99999908]  f(x): 4.676756130467864e-11  grad at x: [ 1.35519434e-05 -1.84799226e-06]  gradient norm: 1.3677362509589141e-05\n",
            "iter: 8294  x: [-99.99999324  24.99999908]  f(x): 4.6580678172683575e-11  grad at x: [ 1.35248395e-05 -1.84429627e-06]  gradient norm: 1.3650007790867165e-05\n",
            "iter: 8295  x: [-99.99999325  24.99999908]  f(x): 4.6394541778969494e-11  grad at x: [ 1.34977898e-05 -1.84060768e-06]  gradient norm: 1.3622707774736928e-05\n",
            "iter: 8296  x: [-99.99999326  24.99999908]  f(x): 4.6209149098159715e-11  grad at x: [ 1.34707942e-05 -1.83692647e-06]  gradient norm: 1.3595462345673973e-05\n",
            "iter: 8297  x: [-99.99999328  24.99999908]  f(x): 4.602449731321018e-11  grad at x: [ 1.34438527e-05 -1.83325261e-06]  gradient norm: 1.3568271417274963e-05\n",
            "iter: 8298  x: [-99.99999329  24.99999909]  f(x): 4.584058342556926e-11  grad at x: [ 1.34169649e-05 -1.82958610e-06]  gradient norm: 1.3541134874975474e-05\n",
            "iter: 8299  x: [-99.9999933   24.99999909]  f(x): 4.565740444734609e-11  grad at x: [ 1.33901310e-05 -1.82592693e-06]  gradient norm: 1.3514052604211084e-05\n",
            "iter: 8300  x: [-99.99999332  24.99999909]  f(x): 4.547495740127948e-11  grad at x: [ 1.33633507e-05 -1.82227507e-06]  gradient norm: 1.3487024490417369e-05\n",
            "iter: 8301  x: [-99.99999333  24.99999909]  f(x): 4.529323951023164e-11  grad at x: [ 1.33366241e-05 -1.81863052e-06]  gradient norm: 1.3460050447190997e-05\n",
            "iter: 8302  x: [-99.99999335  24.99999909]  f(x): 4.511224781697043e-11  grad at x: [ 1.33099508e-05 -1.81499326e-06]  gradient norm: 1.3433130359967543e-05\n",
            "iter: 8303  x: [-99.99999336  24.99999909]  f(x): 4.4931979186037455e-11  grad at x: [ 1.32833309e-05 -1.81136328e-06]  gradient norm: 1.3406264086021497e-05\n",
            "iter: 8304  x: [-99.99999337  24.9999991 ]  f(x): 4.475243105916465e-11  grad at x: [ 1.32567642e-05 -1.80774055e-06]  gradient norm: 1.3379451567110612e-05\n",
            "iter: 8305  x: [-99.99999338  24.9999991 ]  f(x): 4.4573600315095705e-11  grad at x: [ 1.32302507e-05 -1.80412507e-06]  gradient norm: 1.335269265954934e-05\n",
            "iter: 8306  x: [-99.9999934  24.9999991]  f(x): 4.439548423265073e-11  grad at x: [ 1.32037902e-05 -1.80051682e-06]  gradient norm: 1.3325987277894382e-05\n",
            "iter: 8307  x: [-99.99999341  24.9999991 ]  f(x): 4.421807990586976e-11  grad at x: [ 1.31773826e-05 -1.79691579e-06]  gradient norm: 1.3299335307581316e-05\n",
            "iter: 8308  x: [-99.99999342  24.9999991 ]  f(x): 4.404138443281368e-11  grad at x: [ 1.31510279e-05 -1.79332196e-06]  gradient norm: 1.3272736633085685e-05\n",
            "iter: 8309  x: [-99.99999344  24.99999911]  f(x): 4.38653951211862e-11  grad at x: [ 1.31247258e-05 -1.78973531e-06]  gradient norm: 1.3246191168964187e-05\n",
            "iter: 8310  x: [-99.99999345  24.99999911]  f(x): 4.369010908864692e-11  grad at x: [ 1.30984764e-05 -1.78615584e-06]  gradient norm: 1.3219698799692362e-05\n",
            "iter: 8311  x: [-99.99999346  24.99999911]  f(x): 4.3515523475877235e-11  grad at x: [ 1.30722794e-05 -1.78258353e-06]  gradient norm: 1.3193259411665828e-05\n",
            "iter: 8312  x: [-99.99999348  24.99999911]  f(x): 4.3341635421141634e-11  grad at x: [ 1.30461349e-05 -1.77901836e-06]  gradient norm: 1.3166872889360121e-05\n",
            "iter: 8313  x: [-99.99999349  24.99999911]  f(x): 4.3168442270640847e-11  grad at x: [ 1.30200426e-05 -1.77546033e-06]  gradient norm: 1.3140539147331946e-05\n",
            "iter: 8314  x: [-99.9999935   24.99999911]  f(x): 4.299594118201274e-11  grad at x: [ 1.29940025e-05 -1.77190940e-06]  gradient norm: 1.311425807005684e-05\n",
            "iter: 8315  x: [-99.99999352  24.99999912]  f(x): 4.2824129335698895e-11  grad at x: [ 1.29680145e-05 -1.76836559e-06]  gradient norm: 1.3088029543930422e-05\n",
            "iter: 8316  x: [-99.99999353  24.99999912]  f(x): 4.2653004093626737e-11  grad at x: [ 1.29420785e-05 -1.76482885e-06]  gradient norm: 1.3061853481589316e-05\n",
            "iter: 8317  x: [-99.99999354  24.99999912]  f(x): 4.2482562655395256e-11  grad at x: [ 1.29161943e-05 -1.76129920e-06]  gradient norm: 1.3035729769429138e-05\n",
            "iter: 8318  x: [-99.99999355  24.99999912]  f(x): 4.231280240134905e-11  grad at x: [ 1.28903619e-05 -1.75777659e-06]  gradient norm: 1.3009658320086512e-05\n",
            "iter: 8319  x: [-99.99999357  24.99999912]  f(x): 4.2143720367321155e-11  grad at x: [ 1.28645812e-05 -1.75426104e-06]  gradient norm: 1.2983638991795968e-05\n",
            "iter: 8320  x: [-99.99999358  24.99999912]  f(x): 4.1975314135517545e-11  grad at x: [ 1.28388521e-05 -1.75075252e-06]  gradient norm: 1.2957671725355223e-05\n",
            "iter: 8321  x: [-99.99999359  24.99999913]  f(x): 4.180758076144404e-11  grad at x: [ 1.28131744e-05 -1.74725101e-06]  gradient norm: 1.29317563789988e-05\n",
            "iter: 8322  x: [-99.99999361  24.99999913]  f(x): 4.164051766925409e-11  grad at x: [ 1.27875480e-05 -1.74375651e-06]  gradient norm: 1.2905892866323366e-05\n",
            "iter: 8323  x: [-99.99999362  24.99999913]  f(x): 4.147412210406388e-11  grad at x: [ 1.27619729e-05 -1.74026900e-06]  gradient norm: 1.2880081071804459e-05\n",
            "iter: 8324  x: [-99.99999363  24.99999913]  f(x): 4.130839151430799e-11  grad at x: [ 1.27364490e-05 -1.73678846e-06]  gradient norm: 1.2854320909998784e-05\n",
            "iter: 8325  x: [-99.99999364  24.99999913]  f(x): 4.114332317003456e-11  grad at x: [ 1.27109761e-05 -1.73331489e-06]  gradient norm: 1.2828612266341915e-05\n",
            "iter: 8326  x: [-99.99999366  24.99999914]  f(x): 4.0978914525296334e-11  grad at x: [ 1.26855541e-05 -1.72984826e-06]  gradient norm: 1.280295505347048e-05\n",
            "iter: 8327  x: [-99.99999367  24.99999914]  f(x): 4.081516269504392e-11  grad at x: [ 1.26601830e-05 -1.72638856e-06]  gradient norm: 1.277734912961901e-05\n",
            "iter: 8328  x: [-99.99999368  24.99999914]  f(x): 4.065206533797661e-11  grad at x: [ 1.26348626e-05 -1.72293579e-06]  gradient norm: 1.2751794436545252e-05\n",
            "iter: 8329  x: [-99.9999937   24.99999914]  f(x): 4.048961976132413e-11  grad at x: [ 1.26095929e-05 -1.71948992e-06]  gradient norm: 1.2726290859684785e-05\n",
            "iter: 8330  x: [-99.99999371  24.99999914]  f(x): 4.032782328208495e-11  grad at x: [ 1.25843737e-05 -1.71605094e-06]  gradient norm: 1.2700838284473187e-05\n",
            "iter: 8331  x: [-99.99999372  24.99999914]  f(x): 4.016667322699708e-11  grad at x: [ 1.25592050e-05 -1.71261884e-06]  gradient norm: 1.2675436596346034e-05\n",
            "iter: 8332  x: [-99.99999373  24.99999915]  f(x): 4.000616728874904e-11  grad at x: [ 1.25340866e-05 -1.70919360e-06]  gradient norm: 1.265008573706108e-05\n",
            "iter: 8333  x: [-99.99999375  24.99999915]  f(x): 3.9846302633569085e-11  grad at x: [ 1.25090184e-05 -1.70577522e-06]  gradient norm: 1.2624785563892812e-05\n",
            "iter: 8334  x: [-99.99999376  24.99999915]  f(x): 3.968707679358496e-11  grad at x: [ 1.24840004e-05 -1.70236367e-06]  gradient norm: 1.2599535990437896e-05\n",
            "iter: 8335  x: [-99.99999377  24.99999915]  f(x): 3.952848730913258e-11  grad at x: [ 1.24590324e-05 -1.69895894e-06]  gradient norm: 1.2574336930292997e-05\n",
            "iter: 8336  x: [-99.99999378  24.99999915]  f(x): 3.9370531558058614e-11  grad at x: [ 1.24341143e-05 -1.69556102e-06]  gradient norm: 1.254918826985373e-05\n",
            "iter: 8337  x: [-99.9999938   24.99999915]  f(x): 3.921320691571607e-11  grad at x: [ 1.24092461e-05 -1.69216990e-06]  gradient norm: 1.2524089893595633e-05\n",
            "iter: 8338  x: [-99.99999381  24.99999916]  f(x): 3.905651095504833e-11  grad at x: [ 1.23844276e-05 -1.68878557e-06]  gradient norm: 1.249904171607541e-05\n",
            "iter: 8339  x: [-99.99999382  24.99999916]  f(x): 3.8900441069446464e-11  grad at x: [ 1.23596587e-05 -1.68540799e-06]  gradient norm: 1.2474043621768598e-05\n",
            "iter: 8340  x: [-99.99999383  24.99999916]  f(x): 3.8744994849087245e-11  grad at x: [ 1.23349394e-05 -1.68203718e-06]  gradient norm: 1.2449095525231902e-05\n",
            "iter: 8341  x: [-99.99999384  24.99999916]  f(x): 3.8590169886200835e-11  grad at x: [ 1.23102695e-05 -1.67867311e-06]  gradient norm: 1.2424197340061987e-05\n",
            "iter: 8342  x: [-99.99999386  24.99999916]  f(x): 3.843596360647141e-11  grad at x: [ 1.22856490e-05 -1.67531577e-06]  gradient norm: 1.2399348951694425e-05\n",
            "iter: 8343  x: [-99.99999387  24.99999916]  f(x): 3.8282373445001956e-11  grad at x: [ 1.22610777e-05 -1.67196514e-06]  gradient norm: 1.2374550245564799e-05\n",
            "iter: 8344  x: [-99.99999388  24.99999917]  f(x): 3.812939702017763e-11  grad at x: [ 1.22365555e-05 -1.66862121e-06]  gradient norm: 1.2349801135269772e-05\n",
            "iter: 8345  x: [-99.99999389  24.99999917]  f(x): 3.797703195835432e-11  grad at x: [ 1.22120824e-05 -1.66528397e-06]  gradient norm: 1.2325101534406006e-05\n",
            "iter: 8346  x: [-99.99999391  24.99999917]  f(x): 3.782527572064018e-11  grad at x: [ 1.21876583e-05 -1.66195340e-06]  gradient norm: 1.2300451328409081e-05\n",
            "iter: 8347  x: [-99.99999392  24.99999917]  f(x): 3.767412595620017e-11  grad at x: [ 1.21632829e-05 -1.65862949e-06]  gradient norm: 1.2275850431835698e-05\n",
            "iter: 8348  x: [-99.99999393  24.99999917]  f(x): 3.752358013778791e-11  grad at x: [ 1.21389564e-05 -1.65531223e-06]  gradient norm: 1.2251298729161396e-05\n",
            "iter: 8349  x: [-99.99999394  24.99999917]  f(x): 3.737363593136353e-11  grad at x: [ 1.21146785e-05 -1.65200161e-06]  gradient norm: 1.2226796134942879e-05\n",
            "iter: 8350  x: [-99.99999395  24.99999918]  f(x): 3.7224290833023005e-11  grad at x: [ 1.20904491e-05 -1.64869761e-06]  gradient norm: 1.220234253461572e-05\n",
            "iter: 8351  x: [-99.99999397  24.99999918]  f(x): 3.707554251954357e-11  grad at x: [ 1.20662682e-05 -1.64540021e-06]  gradient norm: 1.217793784177659e-05\n",
            "iter: 8352  x: [-99.99999398  24.99999918]  f(x): 3.6927388675511864e-11  grad at x: [ 1.20421357e-05 -1.64210941e-06]  gradient norm: 1.215358197002215e-05\n",
            "iter: 8353  x: [-99.99999399  24.99999918]  f(x): 3.677982682251609e-11  grad at x: [ 1.20180514e-05 -1.63882519e-06]  gradient norm: 1.2129274804787973e-05\n",
            "iter: 8354  x: [-99.999994    24.99999918]  f(x): 3.6632854667533837e-11  grad at x: [ 1.19940153e-05 -1.63554754e-06]  gradient norm: 1.210501626063077e-05\n",
            "iter: 8355  x: [-99.99999401  24.99999918]  f(x): 3.648646974353597e-11  grad at x: [ 1.19700273e-05 -1.63227644e-06]  gradient norm: 1.2080806222026073e-05\n",
            "iter: 8356  x: [-99.99999403  24.99999919]  f(x): 3.634066977396081e-11  grad at x: [ 1.19460872e-05 -1.62901189e-06]  gradient norm: 1.2056644603530588e-05\n",
            "iter: 8357  x: [-99.99999404  24.99999919]  f(x): 3.619545248412062e-11  grad at x: [ 1.19221950e-05 -1.62575387e-06]  gradient norm: 1.2032531318740977e-05\n",
            "iter: 8358  x: [-99.99999405  24.99999919]  f(x): 3.605081543791726e-11  grad at x: [ 1.18983506e-05 -1.62250236e-06]  gradient norm: 1.2008466253092817e-05\n",
            "iter: 8359  x: [-99.99999406  24.99999919]  f(x): 3.5906756377007355e-11  grad at x: [ 1.18745539e-05 -1.61925735e-06]  gradient norm: 1.1984449320182776e-05\n",
            "iter: 8360  x: [-99.99999407  24.99999919]  f(x): 3.576327305641832e-11  grad at x: [ 1.18508048e-05 -1.61601884e-06]  gradient norm: 1.1960480434567555e-05\n",
            "iter: 8361  x: [-99.99999409  24.99999919]  f(x): 3.5620363059207687e-11  grad at x: [ 1.18271032e-05 -1.61278680e-06]  gradient norm: 1.193655948072269e-05\n",
            "iter: 8362  x: [-99.9999941  24.9999992]  f(x): 3.547802415658535e-11  grad at x: [ 1.18034490e-05 -1.60956123e-06]  gradient norm: 1.1912686373204887e-05\n",
            "iter: 8363  x: [-99.99999411  24.9999992 ]  f(x): 3.533625395417029e-11  grad at x: [ 1.17798421e-05 -1.60634211e-06]  gradient norm: 1.188886099744972e-05\n",
            "iter: 8364  x: [-99.99999412  24.9999992 ]  f(x): 3.519505023353036e-11  grad at x: [ 1.17562824e-05 -1.60312943e-06]  gradient norm: 1.1865083267053857e-05\n",
            "iter: 8365  x: [-99.99999413  24.9999992 ]  f(x): 3.505441078375157e-11  grad at x: [ 1.17327699e-05 -1.59992317e-06]  gradient norm: 1.184135309561396e-05\n",
            "iter: 8366  x: [-99.99999415  24.9999992 ]  f(x): 3.491433340709032e-11  grad at x: [ 1.17093043e-05 -1.59672332e-06]  gradient norm: 1.1817670397686732e-05\n",
            "iter: 8367  x: [-99.99999416  24.9999992 ]  f(x): 3.477481573584551e-11  grad at x: [ 1.16858857e-05 -1.59352987e-06]  gradient norm: 1.179403505774771e-05\n",
            "iter: 8368  x: [-99.99999417  24.9999992 ]  f(x): 3.463585558818103e-11  grad at x: [ 1.16625140e-05 -1.59034281e-06]  gradient norm: 1.17704469903536e-05\n",
            "iter: 8369  x: [-99.99999418  24.99999921]  f(x): 3.4497450784008976e-11  grad at x: [ 1.16391890e-05 -1.58716212e-06]  gradient norm: 1.1746906109101065e-05\n",
            "iter: 8370  x: [-99.99999419  24.99999921]  f(x): 3.435959899120878e-11  grad at x: [ 1.16159106e-05 -1.58398780e-06]  gradient norm: 1.1723412300385718e-05\n",
            "iter: 8371  x: [-99.9999942   24.99999921]  f(x): 3.422229803983646e-11  grad at x: [ 1.15926788e-05 -1.58081983e-06]  gradient norm: 1.1699965476844186e-05\n",
            "iter: 8372  x: [-99.99999422  24.99999921]  f(x): 3.408554577857023e-11  grad at x: [ 1.15694934e-05 -1.57765819e-06]  gradient norm: 1.1676565553033173e-05\n",
            "iter: 8373  x: [-99.99999423  24.99999921]  f(x): 3.394933989371315e-11  grad at x: [ 1.15463544e-05 -1.57450287e-06]  gradient norm: 1.1653212414388257e-05\n",
            "iter: 8374  x: [-99.99999424  24.99999921]  f(x): 3.3813678407718566e-11  grad at x: [ 1.15232617e-05 -1.57135387e-06]  gradient norm: 1.1629906002667186e-05\n",
            "iter: 8375  x: [-99.99999425  24.99999922]  f(x): 3.367855902217673e-11  grad at x: [ 1.15002152e-05 -1.56821116e-06]  gradient norm: 1.160664620330554e-05\n",
            "iter: 8376  x: [-99.99999426  24.99999922]  f(x): 3.354397945283205e-11  grad at x: [ 1.14772148e-05 -1.56507474e-06]  gradient norm: 1.1583432902698932e-05\n",
            "iter: 8377  x: [-99.99999427  24.99999922]  f(x): 3.340993774395223e-11  grad at x: [ 1.14542603e-05 -1.56194459e-06]  gradient norm: 1.1560266042605116e-05\n",
            "iter: 8378  x: [-99.99999428  24.99999922]  f(x): 3.3276431620842756e-11  grad at x: [ 1.14313518e-05 -1.55882071e-06]  gradient norm: 1.1537145508459665e-05\n",
            "iter: 8379  x: [-99.9999943   24.99999922]  f(x): 3.314345897945554e-11  grad at x: [ 1.14084891e-05 -1.55570307e-06]  gradient norm: 1.1514071213859247e-05\n",
            "iter: 8380  x: [-99.99999431  24.99999922]  f(x): 3.3011017728456946e-11  grad at x: [ 1.13856721e-05 -1.55259166e-06]  gradient norm: 1.1491043073360563e-05\n",
            "iter: 8381  x: [-99.99999432  24.99999923]  f(x): 3.287910577263802e-11  grad at x: [ 1.13629008e-05 -1.54948648e-06]  gradient norm: 1.146806099960024e-05\n",
            "iter: 8382  x: [-99.99999433  24.99999923]  f(x): 3.27477208738268e-11  grad at x: [ 1.1340175e-05 -1.5463875e-06]  gradient norm: 1.1445124878973894e-05\n",
            "iter: 8383  x: [-99.99999434  24.99999923]  f(x): 3.2616860963066635e-11  grad at x: [ 1.13174947e-05 -1.54329473e-06]  gradient norm: 1.1422234626038223e-05\n",
            "iter: 8384  x: [-99.99999435  24.99999923]  f(x): 3.248652396752839e-11  grad at x: [ 1.12948597e-05 -1.54020814e-06]  gradient norm: 1.1399390153429856e-05\n",
            "iter: 8385  x: [-99.99999436  24.99999923]  f(x): 3.235670783244417e-11  grad at x: [ 1.12722699e-05 -1.53712772e-06]  gradient norm: 1.1376591375705496e-05\n",
            "iter: 8386  x: [-99.99999438  24.99999923]  f(x): 3.222741050462796e-11  grad at x: [ 1.12497254e-05 -1.53405347e-06]  gradient norm: 1.1353838206461806e-05\n",
            "iter: 8387  x: [-99.99999439  24.99999923]  f(x): 3.209862977840134e-11  grad at x: [ 1.12272260e-05 -1.53098536e-06]  gradient norm: 1.1331130531134365e-05\n",
            "iter: 8388  x: [-99.9999944   24.99999924]  f(x): 3.197036362105626e-11  grad at x: [ 1.12047715e-05 -1.52792339e-06]  gradient norm: 1.1308468264279873e-05\n",
            "iter: 8389  x: [-99.99999441  24.99999924]  f(x): 3.1842610001436726e-11  grad at x: [ 1.11823620e-05 -1.52486754e-06]  gradient norm: 1.1285851319494994e-05\n",
            "iter: 8390  x: [-99.99999442  24.99999924]  f(x): 3.171536689538016e-11  grad at x: [ 1.11599972e-05 -1.52181781e-06]  gradient norm: 1.1263279610376396e-05\n",
            "iter: 8391  x: [-99.99999443  24.99999924]  f(x): 3.1588632285697814e-11  grad at x: [ 1.11376772e-05 -1.51877417e-06]  gradient norm: 1.1240753050520737e-05\n",
            "iter: 8392  x: [-99.99999444  24.99999924]  f(x): 3.1462404167540275e-11  grad at x: [ 1.11154019e-05 -1.51573662e-06]  gradient norm: 1.1218271554484725e-05\n",
            "iter: 8393  x: [-99.99999445  24.99999924]  f(x): 3.133668037993223e-11  grad at x: [ 1.10931711e-05 -1.51270515e-06]  gradient norm: 1.1195835007703934e-05\n",
            "iter: 8394  x: [-99.99999446  24.99999925]  f(x): 3.121145892740319e-11  grad at x: [ 1.10709847e-05 -1.50967973e-06]  gradient norm: 1.1173443323775029e-05\n",
            "iter: 8395  x: [-99.99999448  24.99999925]  f(x): 3.108673798374121e-11  grad at x: [ 1.10488428e-05 -1.50666038e-06]  gradient norm: 1.1151096445415799e-05\n",
            "iter: 8396  x: [-99.99999449  24.99999925]  f(x): 3.0962515409567436e-11  grad at x: [ 1.10267451e-05 -1.50364706e-06]  gradient norm: 1.1128794258061821e-05\n",
            "iter: 8397  x: [-99.9999945   24.99999925]  f(x): 3.0838789230000186e-11  grad at x: [ 1.10046916e-05 -1.50063976e-06]  gradient norm: 1.1106536675309758e-05\n",
            "iter: 8398  x: [-99.99999451  24.99999925]  f(x): 3.071555747699228e-11  grad at x: [ 1.09826822e-05 -1.49763848e-06]  gradient norm: 1.1084323610756279e-05\n",
            "iter: 8399  x: [-99.99999452  24.99999925]  f(x): 3.0592818038860615e-11  grad at x: [ 1.09607168e-05 -1.49464320e-06]  gradient norm: 1.1062154950796995e-05\n",
            "iter: 8400  x: [-99.99999453  24.99999925]  f(x): 3.0470569117521574e-11  grad at x: [ 1.09387954e-05 -1.49165391e-06]  gradient norm: 1.1040030637189659e-05\n",
            "iter: 8401  x: [-99.99999454  24.99999926]  f(x): 3.034880877057122e-11  grad at x: [ 1.09169178e-05 -1.48867061e-06]  gradient norm: 1.1017950584490969e-05\n",
            "iter: 8402  x: [-99.99999455  24.99999926]  f(x): 3.022753489693623e-11  grad at x: [ 1.08950840e-05 -1.48569326e-06]  gradient norm: 1.099591467717647e-05\n",
            "iter: 8403  x: [-99.99999456  24.99999926]  f(x): 3.01067457284036e-11  grad at x: [ 1.08732938e-05 -1.48272188e-06]  gradient norm: 1.0973922858923986e-05\n",
            "iter: 8404  x: [-99.99999457  24.99999926]  f(x): 2.998643917796486e-11  grad at x: [ 1.08515472e-05 -1.47975643e-06]  gradient norm: 1.0951975014209055e-05\n",
            "iter: 8405  x: [-99.99999459  24.99999926]  f(x): 2.9866613330982815e-11  grad at x: [ 1.08298441e-05 -1.47679692e-06]  gradient norm: 1.0930071057588384e-05\n",
            "iter: 8406  x: [-99.9999946   24.99999926]  f(x): 2.974726627422016e-11  grad at x: [ 1.08081844e-05 -1.47384333e-06]  gradient norm: 1.0908210902658632e-05\n",
            "iter: 8407  x: [-99.99999461  24.99999926]  f(x): 2.962839625438525e-11  grad at x: [ 1.07865681e-05 -1.47089565e-06]  gradient norm: 1.0886394491177553e-05\n",
            "iter: 8408  x: [-99.99999462  24.99999927]  f(x): 2.951000122285611e-11  grad at x: [ 1.07649950e-05 -1.46795386e-06]  gradient norm: 1.0864621709540762e-05\n",
            "iter: 8409  x: [-99.99999463  24.99999927]  f(x): 2.93920792862871e-11  grad at x: [ 1.07434650e-05 -1.46501795e-06]  gradient norm: 1.0842892471344923e-05\n",
            "iter: 8410  x: [-99.99999464  24.99999927]  f(x): 2.927462855793534e-11  grad at x: [ 1.07219780e-05 -1.46208792e-06]  gradient norm: 1.0821206690186698e-05\n",
            "iter: 8411  x: [-99.99999465  24.99999927]  f(x): 2.9157647162825926e-11  grad at x: [ 1.07005341e-05 -1.45916374e-06]  gradient norm: 1.0799564280622794e-05\n",
            "iter: 8412  x: [-99.99999466  24.99999927]  f(x): 2.9041133227334066e-11  grad at x: [ 1.06791330e-05 -1.45624541e-06]  gradient norm: 1.0777965156249869e-05\n",
            "iter: 8413  x: [-99.99999467  24.99999927]  f(x): 2.892508488438161e-11  grad at x: [ 1.06577748e-05 -1.45333292e-06]  gradient norm: 1.0756409230664592e-05\n",
            "iter: 8414  x: [-99.99999468  24.99999927]  f(x): 2.8809500278571296e-11  grad at x: [ 1.06364592e-05 -1.45042625e-06]  gradient norm: 1.0734896418423663e-05\n",
            "iter: 8415  x: [-99.99999469  24.99999928]  f(x): 2.8694377555831302e-11  grad at x: [ 1.06151863e-05 -1.44752540e-06]  gradient norm: 1.0713426633123746e-05\n",
            "iter: 8416  x: [-99.9999947   24.99999928]  f(x): 2.8579714873712985e-11  grad at x: [ 1.05939559e-05 -1.44463035e-06]  gradient norm: 1.0691999789321544e-05\n",
            "iter: 8417  x: [-99.99999471  24.99999928]  f(x): 2.84655103910768e-11  grad at x: [ 1.05727680e-05 -1.44174109e-06]  gradient norm: 1.067061580061372e-05\n",
            "iter: 8418  x: [-99.99999472  24.99999928]  f(x): 2.8351762273237037e-11  grad at x: [ 1.05516225e-05 -1.43885761e-06]  gradient norm: 1.064927458059694e-05\n",
            "iter: 8419  x: [-99.99999473  24.99999928]  f(x): 2.8238468697044964e-11  grad at x: [ 1.05305193e-05 -1.43597990e-06]  gradient norm: 1.0627976043827905e-05\n",
            "iter: 8420  x: [-99.99999475  24.99999928]  f(x): 2.8125627840636495e-11  grad at x: [ 1.05094583e-05 -1.43310794e-06]  gradient norm: 1.060672010390328e-05\n",
            "iter: 8421  x: [-99.99999476  24.99999928]  f(x): 2.8013237888546143e-11  grad at x: [ 1.04884394e-05 -1.43024172e-06]  gradient norm: 1.0585506674419727e-05\n",
            "iter: 8422  x: [-99.99999477  24.99999929]  f(x): 2.7901297036759694e-11  grad at x: [ 1.04674625e-05 -1.42738124e-06]  gradient norm: 1.0564335669933949e-05\n",
            "iter: 8423  x: [-99.99999478  24.99999929]  f(x): 2.7789803482523294e-11  grad at x: [ 1.04465276e-05 -1.42452648e-06]  gradient norm: 1.054320700404261e-05\n",
            "iter: 8424  x: [-99.99999479  24.99999929]  f(x): 2.7678755429426757e-11  grad at x: [ 1.04256345e-05 -1.42167742e-06]  gradient norm: 1.0522120590342377e-05\n",
            "iter: 8425  x: [-99.9999948   24.99999929]  f(x): 2.7568151092426015e-11  grad at x: [ 1.04047832e-05 -1.41883407e-06]  gradient norm: 1.0501076343389952e-05\n",
            "iter: 8426  x: [-99.99999481  24.99999929]  f(x): 2.7457988692743823e-11  grad at x: [ 1.03839737e-05 -1.41599640e-06]  gradient norm: 1.0480074177742031e-05\n",
            "iter: 8427  x: [-99.99999482  24.99999929]  f(x): 2.7348266595080665e-11  grad at x: [ 1.03632057e-05 -1.41316441e-06]  gradient norm: 1.0459114034196332e-05\n",
            "iter: 8428  x: [-99.99999483  24.99999929]  f(x): 2.7238982890350717e-11  grad at x: [ 1.03424793e-05 -1.41033808e-06]  gradient norm: 1.0438195800108507e-05\n",
            "iter: 8429  x: [-99.99999484  24.9999993 ]  f(x): 2.713013595518164e-11  grad at x: [ 1.03217943e-05 -1.40751740e-06]  gradient norm: 1.041731941627627e-05\n",
            "iter: 8430  x: [-99.99999485  24.9999993 ]  f(x): 2.70217238885073e-11  grad at x: [ 1.03011508e-05 -1.40470237e-06]  gradient norm: 1.0396484769095234e-05\n",
            "iter: 8431  x: [-99.99999486  24.9999993 ]  f(x): 2.6913745083823282e-11  grad at x: [ 1.02805485e-05 -1.40189296e-06]  gradient norm: 1.0375691800323154e-05\n",
            "iter: 8432  x: [-99.99999487  24.9999993 ]  f(x): 2.680619779882267e-11  grad at x: [ 1.02599874e-05 -1.39908917e-06]  gradient norm: 1.0354940424516729e-05\n",
            "iter: 8433  x: [-99.99999488  24.9999993 ]  f(x): 2.6699080292382576e-11  grad at x: [ 1.02394674e-05 -1.39629099e-06]  gradient norm: 1.0334230555272624e-05\n",
            "iter: 8434  x: [-99.99999489  24.9999993 ]  f(x): 2.65923908344974e-11  grad at x: [ 1.02189885e-05 -1.39349841e-06]  gradient norm: 1.031356210714754e-05\n",
            "iter: 8435  x: [-99.9999949  24.9999993]  f(x): 2.6486127696329797e-11  grad at x: [ 1.01985505e-05 -1.39071141e-06]  gradient norm: 1.0292934993738141e-05\n",
            "iter: 8436  x: [-99.99999491  24.99999931]  f(x): 2.6380289160104277e-11  grad at x: [ 1.01781534e-05 -1.38792999e-06]  gradient norm: 1.027234912960113e-05\n",
            "iter: 8437  x: [-99.99999492  24.99999931]  f(x): 2.6274873504276852e-11  grad at x: [ 1.01577971e-05 -1.38515413e-06]  gradient norm: 1.0251804427373135e-05\n",
            "iter: 8438  x: [-99.99999493  24.99999931]  f(x): 2.6169879172234225e-11  grad at x: [ 1.01374815e-05 -1.38238382e-06]  gradient norm: 1.023130083073198e-05\n",
            "iter: 8439  x: [-99.99999494  24.99999931]  f(x): 2.6065304314858442e-11  grad at x: [ 1.01172066e-05 -1.37961906e-06]  gradient norm: 1.0210838225113244e-05\n",
            "iter: 8440  x: [-99.99999495  24.99999931]  f(x): 2.5961147377218425e-11  grad at x: [ 1.00969721e-05 -1.37685982e-06]  gradient norm: 1.0190416552274676e-05\n",
            "iter: 8441  x: [-99.99999496  24.99999931]  f(x): 2.5857406666077902e-11  grad at x: [ 1.00767782e-05 -1.37410610e-06]  gradient norm: 1.0170035725812944e-05\n",
            "iter: 8442  x: [-99.99999497  24.99999931]  f(x): 2.5754080503972795e-11  grad at x: [ 1.00566247e-05 -1.37135789e-06]  gradient norm: 1.0149695661244783e-05\n",
            "iter: 8443  x: [-99.99999498  24.99999932]  f(x): 2.5651167204784676e-11  grad at x: [ 1.00365114e-05 -1.36861517e-06]  gradient norm: 1.0129396271206823e-05\n",
            "iter: 8444  x: [-99.99999499  24.99999932]  f(x): 2.5548665098132284e-11  grad at x: [ 1.00164384e-05 -1.36587794e-06]  gradient norm: 1.0109137470255766e-05\n",
            "iter: 8445  x: [-99.999995    24.99999932]  f(x): 2.544657266162847e-11  grad at x: [ 9.99640551e-06 -1.36314619e-06]  gradient norm: 1.0088919201109397e-05\n",
            "iter: 8446  x: [-99.99999501  24.99999932]  f(x): 2.5344888084455875e-11  grad at x: [ 9.97641268e-06 -1.36041989e-06]  gradient norm: 1.006874134824326e-05\n",
            "iter: 8447  x: [-99.99999502  24.99999932]  f(x): 2.524360986036426e-11  grad at x: [ 9.95645985e-06 -1.35769906e-06]  gradient norm: 1.004860385533518e-05\n",
            "iter: 8448  x: [-99.99999503  24.99999932]  f(x): 2.5142736336982847e-11  grad at x: [ 9.93654692e-06 -1.35498366e-06]  gradient norm: 1.0028506635981819e-05\n",
            "iter: 8449  x: [-99.99999504  24.99999932]  f(x): 2.5042266008774646e-11  grad at x: [ 9.91667383e-06 -1.35227369e-06]  gradient norm: 1.0008449631940932e-05\n",
            "iter: 8450  x: [-99.99999505  24.99999933]  f(x): 2.494219709847674e-11  grad at x: [ 9.89684048e-06 -1.34956915e-06]  gradient norm: 9.988432729608131e-06\n",
            "Optimizer: [-99.99999505  24.99999933]\n",
            "for step length = 0.001, the minimum value of function is 2.494219709847674e-11 and number of iterations are= 8450\n",
            "\n",
            "For step length = 0.01\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [ 7.8 10.3]  f(x): 11836.93  grad at x: [215.6 -29.4]  gradient norm: 217.5953124495103\n",
            "iter: 2  x: [ 5.644 10.594]  f(x): 11368.187572  grad at x: [211.288 -28.812]  gradient norm: 213.24340620052007\n",
            "iter: 3  x: [ 3.53112 10.88212]  f(x): 10918.0073441488  grad at x: [207.06224 -28.23576]  gradient norm: 208.97853807650966\n",
            "iter: 4  x: [ 1.4604976 11.1644776]  f(x): 10485.654253320508  grad at x: [202.9209952 -27.6710448]  gradient norm: 204.79896731497948\n",
            "iter: 5  x: [-0.56871235 11.44118805]  f(x): 10070.422344889015  grad at x: [198.8625753 -27.1176239]  gradient norm: 200.70298796867988\n",
            "iter: 6  x: [-2.5573381  11.71236429]  f(x): 9671.63362003141  grad at x: [194.88532379 -26.57527143]  gradient norm: 196.68892820930628\n",
            "iter: 7  x: [-4.50619134 11.978117  ]  f(x): 9288.636928678166  grad at x: [190.98761731 -26.043766  ]  gradient norm: 192.75514964512016\n",
            "iter: 8  x: [-6.41606752 12.23855466]  f(x): 8920.806906302512  grad at x: [187.16786497 -25.52289068]  gradient norm: 188.90004665221778\n",
            "iter: 9  x: [-8.28774617 12.49378357]  f(x): 8567.542952812932  grad at x: [183.42450767 -25.01243286]  gradient norm: 185.1220457191734\n",
            "iter: 10  x: [-10.12199124  12.7439079 ]  f(x): 8228.26825188154  grad at x: [179.75601752 -24.51218421]  gradient norm: 181.41960480478994\n",
            "iter: 11  x: [-11.91955142  12.98902974]  f(x): 7902.428829107032  grad at x: [176.16089716 -24.02194052]  gradient norm: 177.79121270869413\n",
            "iter: 12  x: [-13.68116039  13.22924914]  f(x): 7589.492647474393  grad at x: [172.63767922 -23.54150171]  gradient norm: 174.23538845452026\n",
            "iter: 13  x: [-15.40753718  13.46466416]  f(x): 7288.948738634405  grad at x: [169.18492564 -23.07067168]  gradient norm: 170.75068068542984\n",
            "iter: 14  x: [-17.09938644  13.69537088]  f(x): 7000.3063685844845  grad at x: [165.80122712 -22.60925824]  gradient norm: 167.33566707172125\n",
            "iter: 15  x: [-18.75739871  13.92146346]  f(x): 6723.094236388538  grad at x: [162.48520258 -22.15707308]  gradient norm: 163.98895373028682\n",
            "iter: 16  x: [-20.38225073  14.14303419]  f(x): 6456.859704627552  grad at x: [159.23549853 -21.71393162]  gradient norm: 160.7091746556811\n",
            "iter: 17  x: [-21.97460572  14.36017351]  f(x): 6201.168060324301  grad at x: [156.05078856 -21.27965299]  gradient norm: 157.49499116256746\n",
            "iter: 18  x: [-23.53511361  14.57297004]  f(x): 5955.6018051354595  grad at x: [152.92977279 -20.85405993]  gradient norm: 154.34509133931613\n",
            "iter: 19  x: [-25.06441133  14.78151064]  f(x): 5719.759973652094  grad at x: [149.87117733 -20.43697873]  gradient norm: 151.25818951252978\n",
            "iter: 20  x: [-26.56312311  14.98588042]  f(x): 5493.257478695472  grad at x: [146.87375379 -20.02823915]  gradient norm: 148.2330257222792\n",
            "iter: 21  x: [-28.03186064  15.18616282]  f(x): 5275.724482539131  grad at x: [143.93627871 -19.62767437]  gradient norm: 145.26836520783363\n",
            "iter: 22  x: [-29.47122343  15.38243956]  f(x): 5066.80579303058  grad at x: [141.05755314 -19.23512088]  gradient norm: 142.36299790367693\n",
            "iter: 23  x: [-30.88179896  15.57479077]  f(x): 4866.160283626569  grad at x: [138.23640207 -18.85041846]  gradient norm: 139.51573794560338\n",
            "iter: 24  x: [-32.26416298  15.76329495]  f(x): 4673.46033639496  grad at x: [135.47167403 -18.4734101 ]  gradient norm: 136.72542318669136\n",
            "iter: 25  x: [-33.61887972  15.94802905]  f(x): 4488.391307073717  grad at x: [132.76224055 -18.10394189]  gradient norm: 133.9909147229575\n",
            "iter: 26  x: [-34.94650213  16.12906847]  f(x): 4310.651011313599  grad at x: [130.10699574 -17.74186306]  gradient norm: 131.31109642849836\n",
            "iter: 27  x: [-36.24757209  16.3064871 ]  f(x): 4139.94923126558  grad at x: [127.50485583 -17.38702579]  gradient norm: 128.68487449992838\n",
            "iter: 28  x: [-37.52262065  16.48035736]  f(x): 3976.0072417074634  grad at x: [124.95475871 -17.03928528]  gradient norm: 126.11117700992983\n",
            "iter: 29  x: [-38.77216823  16.65075021]  f(x): 3818.557354935848  grad at x: [122.45566353 -16.69849957]  gradient norm: 123.58895346973124\n",
            "iter: 30  x: [-39.99672487  16.81773521]  f(x): 3667.342483680388  grad at x: [120.00655026 -16.36452958]  gradient norm: 121.1171744003366\n",
            "iter: 31  x: [-41.19679037  16.98138051]  f(x): 3522.115721326645  grad at x: [117.60641926 -16.03723899]  gradient norm: 118.69483091232988\n",
            "iter: 32  x: [-42.37285456  17.14175289]  f(x): 3382.63993876211  grad at x: [115.25429087 -15.71649421]  gradient norm: 116.32093429408329\n",
            "iter: 33  x: [-43.52539747  17.29891784]  f(x): 3248.6873971871305  grad at x: [112.94920506 -15.40216433]  gradient norm: 113.99451560820161\n",
            "iter: 34  x: [-44.65488952  17.45293948]  f(x): 3120.03937625852  grad at x: [110.69022096 -15.09412104]  gradient norm: 111.71462529603758\n",
            "iter: 35  x: [-45.76179173  17.60388069]  f(x): 2996.485816958683  grad at x: [108.47641654 -14.79223862]  gradient norm: 109.48033279011683\n",
            "iter: 36  x: [-46.8465559   17.75180308]  f(x): 2877.824978607119  grad at x: [106.30688821 -14.49639385]  gradient norm: 107.2907261343145\n",
            "iter: 37  x: [-47.90962478  17.89676702]  f(x): 2763.863109454277  grad at x: [104.18075044 -14.20646597]  gradient norm: 105.1449116116282\n",
            "iter: 38  x: [-48.95143228  18.03883168]  f(x): 2654.414130319888  grad at x: [102.09713543 -13.92233665]  gradient norm: 103.04201337939566\n",
            "iter: 39  x: [-49.97240364  18.17805504]  f(x): 2549.2993307592205  grad at x: [100.05519272 -13.64388992]  gradient norm: 100.98117311180773\n",
            "iter: 40  x: [-50.97295557  18.31449394]  f(x): 2448.347077261156  grad at x: [ 98.05408887 -13.37101212]  gradient norm: 98.9615496495716\n",
            "iter: 41  x: [-51.95349645  18.44820406]  f(x): 2351.392533001614  grad at x: [ 96.09300709 -13.10359188]  gradient norm: 96.98231865658016\n",
            "iter: 42  x: [-52.91442653  18.57923998]  f(x): 2258.27738869475  grad at x: [ 94.17114695 -12.84152004]  gradient norm: 95.04267228344855\n",
            "iter: 43  x: [-53.85613799  18.70765518]  f(x): 2168.8496041024378  grad at x: [ 92.28772401 -12.58468964]  gradient norm: 93.14181883777958\n",
            "iter: 44  x: [-54.77901523  18.83350208]  f(x): 2082.963159779981  grad at x: [ 90.44196953 -12.33299585]  gradient norm: 91.27898246102399\n",
            "iter: 45  x: [-55.68343493  18.95683204]  f(x): 2000.4778186526942  grad at x: [ 88.63313014 -12.08633593]  gradient norm: 89.45340281180351\n",
            "iter: 46  x: [-56.56976623  19.0776954 ]  f(x): 1921.2588970340478  grad at x: [ 86.86046754 -11.84460921]  gradient norm: 87.66433475556745\n",
            "iter: 47  x: [-57.43837091  19.19614149]  f(x): 1845.1770447114993  grad at x: [ 85.12325819 -11.60771703]  gradient norm: 85.9110480604561\n",
            "iter: 48  x: [-58.28960349  19.31221866]  f(x): 1772.1080337409242  grad at x: [ 83.42079302 -11.37556268]  gradient norm: 84.19282709924698\n",
            "iter: 49  x: [-59.12381142  19.42597428]  f(x): 1701.9325556047834  grad at x: [ 81.75237716 -11.14805143]  gradient norm: 82.50897055726203\n",
            "iter: 50  x: [-59.94133519  19.5374548 ]  f(x): 1634.5360264028343  grad at x: [ 80.11732962 -10.9250904 ]  gradient norm: 80.8587911461168\n",
            "iter: 51  x: [-60.74250849  19.6467057 ]  f(x): 1569.8083997572821  grad at x: [ 78.51498303 -10.70658859]  gradient norm: 79.24161532319447\n",
            "iter: 52  x: [-61.52765832  19.75377159]  f(x): 1507.6439871268938  grad at x: [ 76.94468337 -10.49245682]  gradient norm: 77.65678301673059\n",
            "iter: 53  x: [-62.29710515  19.85869616]  f(x): 1447.9412852366686  grad at x: [ 75.4057897  -10.28260769]  gradient norm: 76.10364735639597\n",
            "iter: 54  x: [-63.05116305  19.96152223]  f(x): 1390.6028103412966  grad at x: [ 73.8976739  -10.07695553]  gradient norm: 74.58157440926804\n",
            "iter: 55  x: [-63.79013979  20.06229179]  f(x): 1335.5349390517813  grad at x: [72.41972043 -9.87541642]  gradient norm: 73.08994292108268\n",
            "iter: 56  x: [-64.51433699  20.16104595]  f(x): 1282.6477554653304  grad at x: [70.97132602 -9.67790809]  gradient norm: 71.62814406266102\n",
            "iter: 57  x: [-65.22405025  20.25782503]  f(x): 1231.8549043489031  grad at x: [69.5518995  -9.48434993]  gradient norm: 70.1955811814078\n",
            "iter: 58  x: [-65.91956925  20.35266853]  f(x): 1183.0734501366871  grad at x: [68.16086151 -9.29466293]  gradient norm: 68.79166955777966\n",
            "iter: 59  x: [-66.60117786  20.44561516]  f(x): 1136.2237415112743  grad at x: [66.79764428 -9.10876967]  gradient norm: 67.41583616662406\n",
            "iter: 60  x: [-67.2691543   20.53670286]  f(x): 1091.2292813474282  grad at x: [65.46169139 -8.92659428]  gradient norm: 66.0675194432916\n",
            "iter: 61  x: [-67.92377122  20.6259688 ]  f(x): 1048.0166018060702  grad at x: [64.15245756 -8.7480624 ]  gradient norm: 64.74616905442576\n",
            "iter: 62  x: [-68.56529579  20.71344943]  f(x): 1006.5151443745498  grad at x: [62.86940841 -8.57310115]  gradient norm: 63.45124567333725\n",
            "iter: 63  x: [-69.19398988  20.79918044]  f(x): 966.6571446573179  grad at x: [61.61202024 -8.40163912]  gradient norm: 62.18222075987051\n",
            "iter: 64  x: [-69.81011008  20.88319683]  f(x): 928.3775217288882  grad at x: [60.37977984 -8.23360634]  gradient norm: 60.93857634467311\n",
            "iter: 65  x: [-70.41390788  20.96553289]  f(x): 891.6137718684245  grad at x: [59.17218424 -8.06893421]  gradient norm: 59.719804817779654\n",
            "iter: 66  x: [-71.00562972  21.04622223]  f(x): 856.3058665024345  grad at x: [57.98874056 -7.90755553]  gradient norm: 58.52540872142405\n",
            "iter: 67  x: [-71.58551713  21.12529779]  f(x): 822.3961541889385  grad at x: [56.82896575 -7.74940442]  gradient norm: 57.35490054699558\n",
            "iter: 68  x: [-72.15380678  21.20279183]  f(x): 789.8292664830562  grad at x: [55.69238643 -7.59441633]  gradient norm: 56.20780253605566\n",
            "iter: 69  x: [-72.71073065  21.278736  ]  f(x): 758.5520275303277  grad at x: [54.5785387  -7.44252801]  gradient norm: 55.08364648533456\n",
            "iter: 70  x: [-73.25651604  21.35316128]  f(x): 728.5133672401269  grad at x: [53.48696793 -7.29367744]  gradient norm: 53.981973555627874\n",
            "iter: 71  x: [-73.79138571  21.42609805]  f(x): 699.6642378974177  grad at x: [52.41722857 -7.1478039 ]  gradient norm: 52.90233408451532\n",
            "iter: 72  x: [-74.315558    21.49757609]  f(x): 671.9575340766801  grad at x: [51.368884   -7.00484782]  gradient norm: 51.84428740282501\n",
            "iter: 73  x: [-74.82924684  21.56762457]  f(x): 645.3480157272434  grad at x: [50.34150632 -6.86475086]  gradient norm: 50.807401654768505\n",
            "iter: 74  x: [-75.3326619   21.63627208]  f(x): 619.7922343044448  grad at x: [49.33467619 -6.72745584]  gradient norm: 49.79125362167315\n",
            "iter: 75  x: [-75.82600867  21.70354664]  f(x): 595.2484618259886  grad at x: [48.34798267 -6.59290673]  gradient norm: 48.79542854923967\n",
            "iter: 76  x: [-76.30948849  21.7694757 ]  f(x): 571.676622737679  grad at x: [47.38102302 -6.46104859]  gradient norm: 47.81951997825487\n",
            "iter: 77  x: [-76.78329872  21.83408619]  f(x): 549.0382284772669  grad at x: [46.43340256 -6.33182762]  gradient norm: 46.863129578689765\n",
            "iter: 78  x: [-77.24763275  21.89740447]  f(x): 527.2963146295673  grad at x: [45.5047345  -6.20519107]  gradient norm: 45.925866987115974\n",
            "iter: 79  x: [-77.70268009  21.95945638]  f(x): 506.4153805702368  grad at x: [44.59463981 -6.08108725]  gradient norm: 45.00734964737367\n",
            "iter: 80  x: [-78.14862649  22.02026725]  f(x): 486.36133149965514  grad at x: [43.70274702 -5.9594655 ]  gradient norm: 44.10720265442619\n",
            "iter: 81  x: [-78.58565396  22.0798619 ]  f(x): 467.10142277226856  grad at x: [42.82869208 -5.84027619]  gradient norm: 43.22505860133765\n",
            "iter: 82  x: [-79.01394088  22.13826467]  f(x): 448.60420643048656  grad at x: [41.97211824 -5.72347067]  gradient norm: 42.360557429310894\n",
            "iter: 83  x: [-79.43366206  22.19549937]  f(x): 430.83947985583933  grad at x: [41.13267587 -5.60900126]  gradient norm: 41.513346280724676\n",
            "iter: 84  x: [-79.84498882  22.25158938]  f(x): 413.7782364535483  grad at x: [40.31002235 -5.49682123]  gradient norm: 40.683079355110195\n",
            "iter: 85  x: [-80.24808905  22.3065576 ]  f(x): 397.39261828998764  grad at x: [39.50382191 -5.38688481]  gradient norm: 39.869417768007985\n",
            "iter: 86  x: [-80.64312727  22.36042645]  f(x): 381.65587060570414  grad at x: [38.71374547 -5.27914711]  gradient norm: 39.07202941264782\n",
            "iter: 87  x: [-81.03026472  22.41321792]  f(x): 366.5422981297181  grad at x: [37.93947056 -5.17356417]  gradient norm: 38.29058882439486\n",
            "iter: 88  x: [-81.40965943  22.46495356]  f(x): 352.027223123781  grad at x: [37.18068115 -5.07009288]  gradient norm: 37.52477704790695\n",
            "iter: 89  x: [-81.78146624  22.51565449]  f(x): 338.086945088079  grad at x: [36.43706753 -4.96869103]  gradient norm: 36.774281506948796\n",
            "iter: 90  x: [-82.14583691  22.5653414 ]  f(x): 324.698702062591  grad at x: [35.70832617 -4.86931721]  gradient norm: 36.038795876809814\n",
            "iter: 91  x: [-82.50292017  22.61403457]  f(x): 311.84063346091244  grad at x: [34.99415965 -4.77193086]  gradient norm: 35.31801995927362\n",
            "iter: 92  x: [-82.85286177  22.66175388]  f(x): 299.49174437586015  grad at x: [34.29427646 -4.67649224]  gradient norm: 34.611659560088135\n",
            "iter: 93  x: [-83.19580454  22.7085188 ]  f(x): 287.6318712985759  grad at x: [33.60839093 -4.5829624 ]  gradient norm: 33.91942636888636\n",
            "iter: 94  x: [-83.53188844  22.75434842]  f(x): 276.2416491951525  grad at x: [32.93622311 -4.49130315]  gradient norm: 33.24103784150865\n",
            "iter: 95  x: [-83.86125068  22.79926146]  f(x): 265.3024798870242  grad at x: [32.27749865 -4.40147709]  gradient norm: 32.57621708467846\n",
            "iter: 96  x: [-84.18402566  22.84327623]  f(x): 254.79650168349792  grad at x: [31.63194868 -4.31344755]  gradient norm: 31.924692742984885\n",
            "iter: 97  x: [-84.50034515  22.8864107 ]  f(x): 244.70656021683135  grad at x: [30.9993097 -4.2271786]  gradient norm: 31.28619888812518\n",
            "iter: 98  x: [-84.81033825  22.92868249]  f(x): 235.01618043224494  grad at x: [30.37932351 -4.14263502]  gradient norm: 30.660474910362687\n",
            "iter: 99  x: [-85.11413148  22.97010884]  f(x): 225.70953968712823  grad at x: [29.77173704 -4.05978232]  gradient norm: 30.047265412155443\n",
            "iter: 100  x: [-85.41184885  23.01070666]  f(x): 216.7714419155179  grad at x: [29.1763023  -3.97858668]  gradient norm: 29.44632010391233\n",
            "iter: 101  x: [-85.70361187  23.05049253]  f(x): 208.18729281566348  grad at x: [28.59277625 -3.89901494]  gradient norm: 28.85739370183409\n",
            "iter: 102  x: [-85.98953964  23.08948268]  f(x): 199.94307602016315  grad at x: [28.02092073 -3.82103464]  gradient norm: 28.280245827797405\n",
            "iter: 103  x: [-86.26974884  23.12769302]  f(x): 192.0253302097647  grad at x: [27.46050231 -3.74461395]  gradient norm: 27.714640911241457\n",
            "iter: 104  x: [-86.54435387  23.16513916]  f(x): 184.42112713345816  grad at x: [26.91129227 -3.66972167]  gradient norm: 27.16034809301664\n",
            "iter: 105  x: [-86.81346679  23.20183638]  f(x): 177.11805049897305  grad at x: [26.37306642 -3.59632724]  gradient norm: 26.617141131156295\n",
            "iter: 106  x: [-87.07719745  23.23779965]  f(x): 170.10417569921384  grad at x: [25.84560509 -3.52440069]  gradient norm: 26.084798308533177\n",
            "iter: 107  x: [-87.33565351  23.27304366]  f(x): 163.36805034152485  grad at x: [25.32869299 -3.45391268]  gradient norm: 25.563102342362505\n",
            "iter: 108  x: [-87.58894044  23.30758279]  f(x): 156.89867554800054  grad at x: [24.82211913 -3.38483443]  gradient norm: 25.051840295515262\n",
            "iter: 109  x: [-87.83716163  23.34143113]  f(x): 150.68548799629977  grad at x: [24.32567675 -3.31713774]  gradient norm: 24.55080348960496\n",
            "iter: 110  x: [-88.08041839  23.37460251]  f(x): 144.71834267164638  grad at x: [23.83916321 -3.25079498]  gradient norm: 24.059787419812867\n",
            "iter: 111  x: [-88.31881003  23.40711046]  f(x): 138.98749630184932  grad at x: [23.36237995 -3.18577908]  gradient norm: 23.57859167141662\n",
            "iter: 112  x: [-88.55243383  23.43896825]  f(x): 133.48359144829624  grad at x: [22.89513235 -3.1220635 ]  gradient norm: 23.107019837988304\n",
            "iter: 113  x: [-88.78138515  23.47018888]  f(x): 128.19764122694355  grad at x: [22.4372297  -3.05962223]  gradient norm: 22.64487944122852\n",
            "iter: 114  x: [-89.00575745  23.50078511]  f(x): 123.12101463435668  grad at x: [21.98848511 -2.99842979]  gradient norm: 22.19198185240396\n",
            "iter: 115  x: [-89.2256423  23.5307694]  f(x): 118.24542245483613  grad at x: [21.54871541 -2.93846119]  gradient norm: 21.74814221535588\n",
            "iter: 116  x: [-89.44112945  23.56015402]  f(x): 113.5629037256247  grad at x: [21.1177411  -2.87969197]  gradient norm: 21.31317937104877\n",
            "iter: 117  x: [-89.65230686  23.58895094]  f(x): 109.06581273809007  grad at x: [20.69538628 -2.82209813]  gradient norm: 20.886915783627803\n",
            "iter: 118  x: [-89.85926072  23.61717192]  f(x): 104.74680655366156  grad at x: [20.28147855 -2.76565617]  gradient norm: 20.469177467955234\n",
            "iter: 119  x: [-90.06207551  23.64482848]  f(x): 100.59883301413663  grad at x: [19.87584898 -2.71034304]  gradient norm: 20.059793918596135\n",
            "iter: 120  x: [-90.260834    23.67193191]  f(x): 96.61511922677684  grad at x: [19.478332   -2.65613618]  gradient norm: 19.658598040224216\n",
            "iter: 121  x: [-90.45561732  23.69849327]  f(x): 92.78916050539662  grad at x: [19.08876536 -2.60301346]  gradient norm: 19.265426079419747\n",
            "iter: 122  x: [-90.64650497  23.72452341]  f(x): 89.11470974938302  grad at x: [18.70699005 -2.55095319]  gradient norm: 18.880117557831362\n",
            "iter: 123  x: [-90.83357487  23.75003294]  f(x): 85.58576724330756  grad at x: [18.33285025 -2.49993413]  gradient norm: 18.502515206674747\n",
            "iter: 124  x: [-91.01690338  23.77503228]  f(x): 82.19657086047268  grad at x: [17.96619325 -2.44993544]  gradient norm: 18.132464902541262\n",
            "iter: 125  x: [-91.19656531  23.79953163]  f(x): 78.94158665439788  grad at x: [17.60686938 -2.40093673]  gradient norm: 17.76981560449043\n",
            "iter: 126  x: [-91.372634  23.823541]  f(x): 75.81549982288365  grad at x: [17.25473199 -2.352918  ]  gradient norm: 17.414419292400613\n",
            "iter: 127  x: [-91.54518132  23.84707018]  f(x): 72.81320602989751  grad at x: [16.90963735 -2.30585964]  gradient norm: 17.066130906552605\n",
            "iter: 128  x: [-91.7142777   23.87012878]  f(x): 69.92980307111354  grad at x: [16.57144461 -2.25974245]  gradient norm: 16.72480828842155\n",
            "iter: 129  x: [-91.87999214  23.8927262 ]  f(x): 67.16058286949736  grad at x: [16.24001571 -2.2145476 ]  gradient norm: 16.390312122653107\n",
            "iter: 130  x: [-92.0423923   23.91487168]  f(x): 64.50102378786522  grad at x: [15.9152154  -2.17025665]  gradient norm: 16.062505880200042\n",
            "iter: 131  x: [-92.20154445  23.93657424]  f(x): 61.94678324586572  grad at x: [15.59691109 -2.12685151]  gradient norm: 15.741255762596035\n",
            "iter: 132  x: [-92.35751356  23.95784276]  f(x): 59.49369062932941  grad at x: [15.28497287 -2.08431448]  gradient norm: 15.42643064734411\n",
            "iter: 133  x: [-92.51036329  23.9786859 ]  f(x): 57.137740480408  grad at x: [14.97927341 -2.04262819]  gradient norm: 15.117902034397233\n",
            "iter: 134  x: [-92.66015603  23.99911219]  f(x): 54.87508595738384  grad at x: [14.67968794 -2.00177563]  gradient norm: 14.815543993709287\n",
            "iter: 135  x: [-92.80695291  24.01912994]  f(x): 52.70203255347134  grad at x: [14.38609419 -1.96174012]  gradient norm: 14.519233113835089\n",
            "iter: 136  x: [-92.95081385  24.03874734]  f(x): 50.61503206435392  grad at x: [14.0983723  -1.92250531]  gradient norm: 14.228848451558394\n",
            "iter: 137  x: [-93.09179757  24.0579724 ]  f(x): 48.61067679460558  grad at x: [13.81640486 -1.88405521]  gradient norm: 13.944271482527236\n",
            "iter: 138  x: [-93.22996162  24.07681295]  f(x): 46.685693993539296  grad at x: [13.54007676 -1.8463741 ]  gradient norm: 13.665386052876705\n",
            "iter: 139  x: [-93.36536239  24.09527669]  f(x): 44.83694051139516  grad at x: [13.26927522 -1.80944662]  gradient norm: 13.392078331819175\n",
            "iter: 140  x: [-93.49805514  24.11337116]  f(x): 43.06139766714387  grad at x: [13.00388972 -1.77325769]  gradient norm: 13.124236765182784\n",
            "iter: 141  x: [-93.62809404  24.13110373]  f(x): 41.356166319524974  grad at x: [12.74381193 -1.73779254]  gradient norm: 12.86175202987913\n",
            "iter: 142  x: [-93.75553216  24.14848166]  f(x): 39.718462133271736  grad at x: [12.48893569 -1.70303668]  gradient norm: 12.604516989281539\n",
            "iter: 143  x: [-93.88042151  24.16551202]  f(x): 38.14561103279427  grad at x: [12.23915697 -1.66897595]  gradient norm: 12.352426649495923\n",
            "iter: 144  x: [-94.00281308  24.18220178]  f(x): 36.635044835895634  grad at x: [11.99437383 -1.63559643]  gradient norm: 12.105378116506008\n",
            "iter: 145  x: [-94.12275682  24.19855775]  f(x): 35.184297060394194  grad at x: [11.75448636 -1.6028845 ]  gradient norm: 11.863270554175893\n",
            "iter: 146  x: [-94.24030169  24.21458659]  f(x): 33.790998896802606  grad at x: [11.51939663 -1.57082681]  gradient norm: 11.626005143092378\n",
            "iter: 147  x: [-94.35549565  24.23029486]  f(x): 32.452875340489165  grad at x: [11.2890087  -1.53941028]  gradient norm: 11.39348504023052\n",
            "iter: 148  x: [-94.46838574  24.24568896]  f(x): 31.16774147700572  grad at x: [11.06322852 -1.50862207]  gradient norm: 11.165615339425896\n",
            "iter: 149  x: [-94.57901802  24.26077519]  f(x): 29.933498914516363  grad at x: [10.84196395 -1.47844963]  gradient norm: 10.942303032637392\n",
            "iter: 150  x: [-94.68743766  24.27555968]  f(x): 28.748132357501447  grad at x: [10.62512467 -1.44888064]  gradient norm: 10.72345697198463\n",
            "iter: 151  x: [-94.79368891  24.29004849]  f(x): 27.60970631614446  grad at x: [10.41262218 -1.41990302]  gradient norm: 10.508987832544952\n",
            "iter: 152  x: [-94.89781513  24.30424752]  f(x): 26.516361946025206  grad at x: [10.20436974 -1.39150496]  gradient norm: 10.298808075894065\n",
            "iter: 153  x: [-94.99985883  24.31816257]  f(x): 25.46631401296254  grad at x: [10.00028234 -1.36367486]  gradient norm: 10.09283191437617\n",
            "iter: 154  x: [-95.09986165  24.33179932]  f(x): 24.457847978049244  grad at x: [ 9.80027669 -1.33640137]  gradient norm: 9.890975276088652\n",
            "iter: 155  x: [-95.19786442  24.34516333]  f(x): 23.48931719811846  grad at x: [ 9.60427116 -1.30967334]  gradient norm: 9.693155770566872\n",
            "iter: 156  x: [-95.29390713  24.35826006]  f(x): 22.559140237072917  grad at x: [ 9.41218574 -1.28347987]  gradient norm: 9.499292655155523\n",
            "iter: 157  x: [-95.38802899  24.37109486]  f(x): 21.66579828368482  grad at x: [ 9.22394202 -1.25781028]  gradient norm: 9.30930680205241\n",
            "iter: 158  x: [-95.48026841  24.38367296]  f(x): 20.807832671650846  grad at x: [ 9.03946318 -1.23265407]  gradient norm: 9.123120666011351\n",
            "iter: 159  x: [-95.57066304  24.39599951]  f(x): 19.98384249785342  grad at x: [ 8.85867392 -1.20800099]  gradient norm: 8.940658252691112\n",
            "iter: 160  x: [-95.65924978  24.40807952]  f(x): 19.192482334938394  grad at x: [ 8.68150044 -1.18384097]  gradient norm: 8.761845087637282\n",
            "iter: 161  x: [-95.74606478  24.41991793]  f(x): 18.432460034474797  grad at x: [ 8.50787043 -1.16016415]  gradient norm: 8.586608185884527\n",
            "iter: 162  x: [-95.83114349  24.43151957]  f(x): 17.702534617109553  grad at x: [ 8.33771302 -1.13696087]  gradient norm: 8.414876022166828\n",
            "iter: 163  x: [-95.91452062  24.44288918]  f(x): 17.001514246272006  grad at x: [ 8.17095876 -1.11422165]  gradient norm: 8.246578501723489\n",
            "iter: 164  x: [-95.99623021  24.45403139]  f(x): 16.32825428211962  grad at x: [ 8.00753959 -1.09193722]  gradient norm: 8.081646931689015\n",
            "iter: 165  x: [-96.0763056   24.46495076]  f(x): 15.681655412547654  grad at x: [ 7.8473888  -1.07009847]  gradient norm: 7.920013993055228\n",
            "iter: 166  x: [-96.15477949  24.47565175]  f(x): 15.06066185821078  grad at x: [ 7.69044102 -1.0486965 ]  gradient norm: 7.7616137131941265\n",
            "iter: 167  x: [-96.2316839   24.48613871]  f(x): 14.464259648625667  grad at x: [ 7.5366322  -1.02772257]  gradient norm: 7.606381438930253\n",
            "iter: 168  x: [-96.30705022  24.49641594]  f(x): 13.891474966540095  grad at x: [ 7.38589956 -1.00716812]  gradient norm: 7.454253810151649\n",
            "iter: 169  x: [-96.38090922  24.50648762]  f(x): 13.341372557865071  grad at x: [ 7.23818156 -0.98702476]  gradient norm: 7.305168733948606\n",
            "iter: 170  x: [-96.45329103  24.51635787]  f(x): 12.813054204573625  grad at x: [ 7.09341793 -0.96728426]  gradient norm: 7.159065359269637\n",
            "iter: 171  x: [-96.52422521  24.52603071]  f(x): 12.305657258072497  grad at x: [ 6.95154957 -0.94793858]  gradient norm: 7.015884052084241\n",
            "iter: 172  x: [-96.59374071  24.5355101 ]  f(x): 11.81835323065281  grad at x: [ 6.81251858 -0.92897981]  gradient norm: 6.875566371042551\n",
            "iter: 173  x: [-96.66186589  24.54479989]  f(x): 11.350346442718987  grad at x: [ 6.67626821 -0.91040021]  gradient norm: 6.738055043621709\n",
            "iter: 174  x: [-96.72862858  24.5539039 ]  f(x): 10.900872723587344  grad at x: [ 6.54274285 -0.89219221]  gradient norm: 6.603293942749284\n",
            "iter: 175  x: [-96.794056    24.56282582]  f(x): 10.469198163733264  grad at x: [ 6.41188799 -0.87434836]  gradient norm: 6.471228063894291\n",
            "iter: 176  x: [-96.85817488  24.5715693 ]  f(x): 10.054617916449462  grad at x: [ 6.28365023 -0.8568614 ]  gradient norm: 6.341803502616417\n",
            "iter: 177  x: [-96.92101139  24.58013792]  f(x): 9.656455046958028  grad at x: [ 6.15797723 -0.83972417]  gradient norm: 6.214967432564077\n",
            "iter: 178  x: [-96.98259116  24.58853516]  f(x): 9.274059427098523  grad at x: [ 6.03481768 -0.82292968]  gradient norm: 6.0906680839128065\n",
            "iter: 179  x: [-97.04293934  24.59676445]  f(x): 8.906806673785457  grad at x: [ 5.91412133 -0.80647109]  gradient norm: 5.968854722234562\n",
            "iter: 180  x: [-97.10208055  24.60482917]  f(x): 8.554097129503516  grad at x: [ 5.7958389  -0.79034167]  gradient norm: 5.849477627789858\n",
            "iter: 181  x: [-97.16003894  24.61273258]  f(x): 8.215354883175175  grad at x: [ 5.67992212 -0.77453483]  gradient norm: 5.73248807523406\n",
            "iter: 182  x: [-97.21683816  24.62047793]  f(x): 7.890026829801464  grad at x: [ 5.56632368 -0.75904414]  gradient norm: 5.617838313729388\n",
            "iter: 183  x: [-97.2725014   24.62806837]  f(x): 7.577581767341348  grad at x: [ 5.45499721 -0.74386326]  gradient norm: 5.505481547454808\n",
            "iter: 184  x: [-97.32705137  24.635507  ]  f(x): 7.277509529354595  grad at x: [ 5.34589726 -0.72898599]  gradient norm: 5.395371916505699\n",
            "iter: 185  x: [-97.38051034  24.64279686]  f(x): 6.989320151992158  grad at x: [ 5.23897932 -0.71440627]  gradient norm: 5.287464478175587\n",
            "iter: 186  x: [-97.43290013  24.64994093]  f(x): 6.712543073973295  grad at x: [ 5.13419973 -0.70011815]  gradient norm: 5.181715188612086\n",
            "iter: 187  x: [-97.48424213  24.65694211]  f(x): 6.446726368243966  grad at x: [ 5.03151574 -0.68611578]  gradient norm: 5.078080884839849\n",
            "iter: 188  x: [-97.53455729  24.66380327]  f(x): 6.191436004061531  grad at x: [ 4.93088542 -0.67239347]  gradient norm: 4.976519267143063\n",
            "iter: 189  x: [-97.58386614  24.6705272 ]  f(x): 5.946255138300712  grad at x: [ 4.83226771 -0.6589456 ]  gradient norm: 4.876988881800209\n",
            "iter: 190  x: [-97.63218882  24.67711666]  f(x): 5.710783434824001  grad at x: [ 4.73562236 -0.64576669]  gradient norm: 4.779449104164203\n",
            "iter: 191  x: [-97.67954504  24.68357432]  f(x): 5.484636410804961  grad at x: [ 4.64090991 -0.63285135]  gradient norm: 4.683860122080915\n",
            "iter: 192  x: [-97.72595414  24.68990284]  f(x): 5.2674448089370705  grad at x: [ 4.54809171 -0.62019432]  gradient norm: 4.5901829196392905\n",
            "iter: 193  x: [-97.77143506  24.69610478]  f(x): 5.058853994503162  grad at x: [ 4.45712988 -0.60779044]  gradient norm: 4.498379261246504\n",
            "iter: 194  x: [-97.81600636  24.70218269]  f(x): 4.8585233763208056  grad at x: [ 4.36798728 -0.59563463]  gradient norm: 4.408411676021561\n",
            "iter: 195  x: [-97.85968623  24.70813903]  f(x): 4.666125850618515  grad at x: [ 4.28062754 -0.58372194]  gradient norm: 4.320243442501136\n",
            "iter: 196  x: [-97.90249251  24.71397625]  f(x): 4.481347266934019  grad at x: [ 4.19501499 -0.5720475 ]  gradient norm: 4.233838573651111\n",
            "iter: 197  x: [-97.94444266  24.71969673]  f(x): 4.303885915163433  grad at x: [ 4.11111469 -0.56060655]  gradient norm: 4.14916180217809\n",
            "iter: 198  x: [-97.9855538   24.72530279]  f(x): 4.133452032922971  grad at x: [ 4.02889239 -0.54939442]  gradient norm: 4.066178566134533\n",
            "iter: 199  x: [-98.02584273  24.73079674]  f(x): 3.9697673324192  grad at x: [ 3.94831454 -0.53840653]  gradient norm: 3.9848549948118315\n",
            "iter: 200  x: [-98.06532587  24.7361808 ]  f(x): 3.8125645460554267  grad at x: [ 3.86934825 -0.5276384 ]  gradient norm: 3.9051578949156087\n",
            "iter: 201  x: [-98.10401936  24.74145718]  f(x): 3.6615869900316085  grad at x: [ 3.79196129 -0.51708563]  gradient norm: 3.827054737017284\n",
            "iter: 202  x: [-98.14193897  24.74662804]  f(x): 3.5165881452263554  grad at x: [ 3.71612206 -0.50674392]  gradient norm: 3.7505136422769376\n",
            "iter: 203  x: [-98.17910019  24.75169548]  f(x): 3.377331254675385  grad at x: [ 3.64179962 -0.49660904]  gradient norm: 3.675503369431395\n",
            "iter: 204  x: [-98.21551819  24.75666157]  f(x): 3.2435889369902418  grad at x: [ 3.56896363 -0.48667686]  gradient norm: 3.6019933020427684\n",
            "iter: 205  x: [-98.25120782  24.76152834]  f(x): 3.1151428150854397  grad at x: [ 3.49758436 -0.47694332]  gradient norm: 3.52995343600192\n",
            "iter: 206  x: [-98.28618367  24.76629777]  f(x): 2.991783159608054  grad at x: [ 3.42763267 -0.46740445]  gradient norm: 3.4593543672818803\n",
            "iter: 207  x: [-98.32045999  24.77097182]  f(x): 2.8733085464875714  grad at x: [ 3.35908002 -0.45805637]  gradient norm: 3.3901672799362403\n",
            "iter: 208  x: [-98.35405079  24.77555238]  f(x): 2.7595255280466695  grad at x: [ 3.29189842 -0.44889524]  gradient norm: 3.322363934337519\n",
            "iter: 209  x: [-98.38696978  24.78004133]  f(x): 2.6502483171360236  grad at x: [ 3.22606045 -0.43991733]  gradient norm: 3.25591665565077\n",
            "iter: 210  x: [-98.41923038  24.78444051]  f(x): 2.545298483777442  grad at x: [ 3.16153924 -0.43111899]  gradient norm: 3.1907983225377574\n",
            "iter: 211  x: [-98.45084577  24.7887517 ]  f(x): 2.444504663819855  grad at x: [ 3.09830845 -0.42249661]  gradient norm: 3.126982356087002\n",
            "iter: 212  x: [-98.48182886  24.79297666]  f(x): 2.347702279132592  grad at x: [ 3.03634228 -0.41404668]  gradient norm: 3.0644427089652644\n",
            "iter: 213  x: [-98.51219228  24.79711713]  f(x): 2.2547332688789243  grad at x: [ 2.97561544 -0.40576574]  gradient norm: 3.0031538547859475\n",
            "iter: 214  x: [-98.54194844  24.80117479]  f(x): 2.165445831431312  grad at x: [ 2.91610313 -0.39765043]  gradient norm: 2.943090777690224\n",
            "iter: 215  x: [-98.57110947  24.80515129]  f(x): 2.079694176506624  grad at x: [ 2.85778107 -0.38969742]  gradient norm: 2.884228962136414\n",
            "iter: 216  x: [-98.59968728  24.80904827]  f(x): 1.997338287116966  grad at x: [ 2.80062545 -0.38190347]  gradient norm: 2.826544382893689\n",
            "iter: 217  x: [-98.62769353  24.8128673 ]  f(x): 1.9182436909471312  grad at x: [ 2.74461294 -0.3742654 ]  gradient norm: 2.770013495235813\n",
            "iter: 218  x: [-98.65513966  24.81660995]  f(x): 1.842281240785629  grad at x: [ 2.68972068 -0.36678009]  gradient norm: 2.7146132253311\n",
            "iter: 219  x: [-98.68203687  24.82027775]  f(x): 1.7693269036505352  grad at x: [ 2.63592626 -0.35944449]  gradient norm: 2.6603209608244907\n",
            "iter: 220  x: [-98.70839613  24.8238722 ]  f(x): 1.6992615582659858  grad at x: [ 2.58320774 -0.3522556 ]  gradient norm: 2.6071145416080097\n",
            "iter: 221  x: [-98.73422821  24.82739476]  f(x): 1.6319708005586686  grad at x: [ 2.53154358 -0.34521049]  gradient norm: 2.554972250775862\n",
            "iter: 222  x: [-98.75954364  24.83084686]  f(x): 1.5673447568565286  grad at x: [ 2.48091271 -0.33830628]  gradient norm: 2.5038728057603317\n",
            "iter: 223  x: [-98.78435277  24.83422992]  f(x): 1.5052779044850053  grad at x: [ 2.43129446 -0.33154015]  gradient norm: 2.453795349645121\n",
            "iter: 224  x: [-98.80866572  24.83754532]  f(x): 1.4456688994673892  grad at x: [ 2.38266857 -0.32490935]  gradient norm: 2.40471944265221\n",
            "iter: 225  x: [-98.8324924   24.84079442]  f(x): 1.3884204110484697  grad at x: [ 2.3350152  -0.31841116]  gradient norm: 2.356625053799157\n",
            "iter: 226  x: [-98.85584255  24.84397853]  f(x): 1.3334389627709413  grad at x: [ 2.28831489 -0.31204294]  gradient norm: 2.309492552723166\n",
            "iter: 227  x: [-98.8787257   24.84709896]  f(x): 1.2806347798452082  grad at x: [ 2.2425486  -0.30580208]  gradient norm: 2.263302701668699\n",
            "iter: 228  x: [-98.90115119  24.85015698]  f(x): 1.229921642563324  grad at x: [ 2.19769762 -0.29968604]  gradient norm: 2.2180366476353126\n",
            "iter: 229  x: [-98.92312816  24.85315384]  f(x): 1.1812167455178073  grad at x: [ 2.15374367 -0.29369232]  gradient norm: 2.173675914682598\n",
            "iter: 230  x: [-98.9446656   24.85609076]  f(x): 1.1344405623952958  grad at x: [ 2.1106688  -0.28781847]  gradient norm: 2.13020239638894\n",
            "iter: 231  x: [-98.96577229  24.85896895]  f(x): 1.089516716124457  grad at x: [ 2.06845542 -0.2820621 ]  gradient norm: 2.0875983484611758\n",
            "iter: 232  x: [-98.98645684  24.86178957]  f(x): 1.046371854165931  grad at x: [ 2.02708631 -0.27642086]  gradient norm: 2.0458463814919545\n",
            "iter: 233  x: [-99.00672771  24.86455378]  f(x): 1.004935528740947  grad at x: [ 1.98654459 -0.27089244]  gradient norm: 2.0049294538621023\n",
            "iter: 234  x: [-99.02659315  24.8672627 ]  f(x): 0.9651400818028  grad at x: [ 1.9468137  -0.26547459]  gradient norm: 1.9648308647848547\n",
            "iter: 235  x: [-99.04606129  24.86991745]  f(x): 0.9269205345634113  grad at x: [ 1.90787742 -0.2601651 ]  gradient norm: 1.92553424748916\n",
            "iter: 236  x: [-99.06514006  24.8725191 ]  f(x): 0.8902144813946994  grad at x: [ 1.86971987 -0.2549618 ]  gradient norm: 1.8870235625393759\n",
            "iter: 237  x: [-99.08383726  24.87506872]  f(x): 0.8549619879314568  grad at x: [ 1.83232548 -0.24986256]  gradient norm: 1.8492830912885747\n",
            "iter: 238  x: [-99.10216052  24.87756734]  f(x): 0.8211054932093722  grad at x: [ 1.79567897 -0.24486531]  gradient norm: 1.8122974294628045\n",
            "iter: 239  x: [-99.12011731  24.880016  ]  f(x): 0.7885897156782913  grad at x: [ 1.75976539 -0.23996801]  gradient norm: 1.77605148087356\n",
            "iter: 240  x: [-99.13771496  24.88241568]  f(x): 0.7573615629374189  grad at x: [ 1.72457008 -0.23516865]  gradient norm: 1.7405304512560749\n",
            "iter: 241  x: [-99.15496066  24.88476736]  f(x): 0.727370045045095  grad at x: [ 1.69007868 -0.23046527]  gradient norm: 1.7057198422309507\n",
            "iter: 242  x: [-99.17186145  24.88707202]  f(x): 0.698566191261302  grad at x: [ 1.6562771  -0.22585597]  gradient norm: 1.6716054453863232\n",
            "iter: 243  x: [-99.18842422  24.88933058]  f(x): 0.6709029700873644  grad at x: [ 1.62315156 -0.22133885]  gradient norm: 1.6381733364786089\n",
            "iter: 244  x: [-99.20465573  24.89154396]  f(x): 0.6443352124718976  grad at x: [ 1.59068853 -0.21691207]  gradient norm: 1.6054098697490278\n",
            "iter: 245  x: [-99.22056262  24.89371308]  f(x): 0.6188195380580171  grad at x: [ 1.55887476 -0.21257383]  gradient norm: 1.5733016723540558\n",
            "iter: 246  x: [-99.23615137  24.89583882]  f(x): 0.5943142843509224  grad at x: [ 1.52769727 -0.20832235]  gradient norm: 1.541835638906978\n",
            "iter: 247  x: [-99.25142834  24.89792205]  f(x): 0.5707794386906292  grad at x: [ 1.49714332 -0.20415591]  gradient norm: 1.510998926128843\n",
            "iter: 248  x: [-99.26639977  24.89996361]  f(x): 0.5481765729184817  grad at x: [ 1.46720045 -0.20007279]  gradient norm: 1.480778947606268\n",
            "iter: 249  x: [-99.28107178  24.90196433]  f(x): 0.526468780630913  grad at x: [ 1.43785644 -0.19607133]  gradient norm: 1.451163368654147\n",
            "iter: 250  x: [-99.29545034  24.90392505]  f(x): 0.5056206169179349  grad at x: [ 1.40909932 -0.19214991]  gradient norm: 1.4221401012810726\n",
            "iter: 251  x: [-99.30954134  24.90584655]  f(x): 0.48559804048798333  grad at x: [ 1.38091733 -0.18830691]  gradient norm: 1.3936972992554493\n",
            "iter: 252  x: [-99.32335051  24.90772961]  f(x): 0.46636835808465493  grad at x: [ 1.35329898 -0.18454077]  gradient norm: 1.365823353270334\n",
            "iter: 253  x: [-99.3368835   24.90957502]  f(x): 0.44790017110449604  grad at x: [ 1.326233   -0.18084995]  gradient norm: 1.3385068862049176\n",
            "iter: 254  x: [-99.35014583  24.91138352]  f(x): 0.43016332432874993  grad at x: [ 1.29970834 -0.17723296]  gradient norm: 1.311736748480807\n",
            "iter: 255  x: [-99.36314291  24.91315585]  f(x): 0.4131288566853339  grad at x: [ 1.27371418 -0.1736883 ]  gradient norm: 1.2855020135111945\n",
            "iter: 256  x: [-99.37588005  24.91489273]  f(x): 0.39676895396060047  grad at x: [ 1.24823989 -0.17021453]  gradient norm: 1.25979197324098\n",
            "iter: 257  x: [-99.38836245  24.91659488]  f(x): 0.3810569033837623  grad at x: [ 1.22327509 -0.16681024]  gradient norm: 1.2345961337761628\n",
            "iter: 258  x: [-99.4005952   24.91826298]  f(x): 0.3659670500097637  grad at x: [ 1.19880959 -0.16347404]  gradient norm: 1.209904211100637\n",
            "iter: 259  x: [-99.4125833   24.91989772]  f(x): 0.35147475482938334  grad at x: [ 1.1748334  -0.16020455]  gradient norm: 1.1857061268786349\n",
            "iter: 260  x: [-99.42433163  24.92149977]  f(x): 0.33755635453813443  grad at x: [ 1.15133673 -0.15700046]  gradient norm: 1.161992004341053\n",
            "iter: 261  x: [-99.435845    24.92306977]  f(x): 0.3241891228984267  grad at x: [ 1.12831    -0.15386045]  gradient norm: 1.1387521642542362\n",
            "iter: 262  x: [-99.4471281   24.92460838]  f(x): 0.311351233631646  grad at x: [ 1.1057438  -0.15078325]  gradient norm: 1.115977120969146\n",
            "iter: 263  x: [-99.45818554  24.92611621]  f(x): 0.2990217247798255  grad at x: [ 1.08362892 -0.14776758]  gradient norm: 1.0936575785497498\n",
            "iter: 264  x: [-99.46902183  24.92759389]  f(x): 0.2871804644785409  grad at x: [ 1.06195634 -0.14481223]  gradient norm: 1.0717844269787482\n",
            "iter: 265  x: [-99.47964139  24.92904201]  f(x): 0.27580811808518874  grad at x: [ 1.04071722 -0.14191598]  gradient norm: 1.0503487384391694\n",
            "iter: 266  x: [-99.49004856  24.93046117]  f(x): 0.2648861166090103  grad at x: [ 1.01990287 -0.13907766]  gradient norm: 1.0293417636703766\n",
            "iter: 267  x: [-99.50024759  24.93185194]  f(x): 0.25439662639128774  grad at x: [ 0.99950482 -0.13629611]  gradient norm: 1.0087549283969575\n",
            "iter: 268  x: [-99.51024264  24.93321491]  f(x): 0.2443225199861918  grad at x: [ 0.97951472 -0.13357019]  gradient norm: 0.9885798298290165\n",
            "iter: 269  x: [-99.52003779  24.93455061]  f(x): 0.23464734819473393  grad at x: [ 0.95992442 -0.13089879]  gradient norm: 0.9688082332324265\n",
            "iter: 270  x: [-99.52963703  24.9358596 ]  f(x): 0.22535531320622143  grad at x: [ 0.94072594 -0.12828081]  gradient norm: 0.9494320685677758\n",
            "iter: 271  x: [-99.53904429  24.9371424 ]  f(x): 0.21643124280325376  grad at x: [ 0.92191142 -0.12571519]  gradient norm: 0.9304434271964175\n",
            "iter: 272  x: [-99.54826341  24.93839956]  f(x): 0.20786056558824387  grad at x: [ 0.90347319 -0.12320089]  gradient norm: 0.9118345586524869\n",
            "iter: 273  x: [-99.55729814  24.93963156]  f(x): 0.19962928719094802  grad at x: [ 0.88540373 -0.12073687]  gradient norm: 0.893597867479434\n",
            "iter: 274  x: [-99.56615217  24.94083893]  f(x): 0.19172396741818104  grad at x: [ 0.86769565 -0.11832213]  gradient norm: 0.8757259101298329\n",
            "iter: 275  x: [-99.57482913  24.94202215]  f(x): 0.1841316983084217  grad at x: [ 0.85034174 -0.11595569]  gradient norm: 0.8582113919272377\n",
            "iter: 276  x: [-99.58333255  24.94318171]  f(x): 0.1768400830554022  grad at x: [ 0.8333349  -0.11363658]  gradient norm: 0.8410471640886786\n",
            "iter: 277  x: [-99.5916659   24.94431808]  f(x): 0.16983721576640817  grad at x: [ 0.8166682  -0.11136385]  gradient norm: 0.8242262208069049\n",
            "iter: 278  x: [-99.59983258  24.94543172]  f(x): 0.16311166202206206  grad at x: [ 0.80033484 -0.10913657]  gradient norm: 0.8077416963907759\n",
            "iter: 279  x: [-99.60783593  24.94652308]  f(x): 0.1566524402059887  grad at x: [ 0.78432814 -0.10695384]  gradient norm: 0.7915868624629611\n",
            "iter: 280  x: [-99.61567921  24.94759262]  f(x): 0.15044900357382823  grad at x: [ 0.76864158 -0.10481476]  gradient norm: 0.7757551252136933\n",
            "iter: 281  x: [-99.62336563  24.94864077]  f(x): 0.1444912230323047  grad at x: [ 0.75326875 -0.10271847]  gradient norm: 0.7602400227094196\n",
            "iter: 282  x: [-99.63089831  24.94966795]  f(x): 0.13876937060022965  grad at x: [ 0.73820337 -0.1006641 ]  gradient norm: 0.7450352222552425\n",
            "iter: 283  x: [-99.63828035  24.95067459]  f(x): 0.13327410352445987  grad at x: [ 0.72343931 -0.09865081]  gradient norm: 0.7301345178101358\n",
            "iter: 284  x: [-99.64551474  24.9516611 ]  f(x): 0.12799644902489257  grad at x: [ 0.70897052 -0.0966778 ]  gradient norm: 0.7155318274539367\n",
            "iter: 285  x: [-99.65260444  24.95262788]  f(x): 0.12292778964350684  grad at x: [ 0.69479111 -0.09474424]  gradient norm: 0.701221190904858\n",
            "iter: 286  x: [-99.65955236  24.95357532]  f(x): 0.11805984917362349  grad at x: [ 0.68089529 -0.09284936]  gradient norm: 0.6871967670867595\n",
            "iter: 287  x: [-99.66636131  24.95450381]  f(x): 0.11338467914634522  grad at x: [ 0.66727738 -0.09099237]  gradient norm: 0.673452831745016\n",
            "iter: 288  x: [-99.67303408  24.95541374]  f(x): 0.10889464585215092  grad at x: [ 0.65393183 -0.08917252]  gradient norm: 0.6599837751101186\n",
            "iter: 289  x: [-99.6795734   24.95630546]  f(x): 0.10458241787640514  grad at x: [ 0.6408532  -0.08738907]  gradient norm: 0.6467840996079144\n",
            "iter: 290  x: [-99.68598193  24.95717935]  f(x): 0.10044095412850296  grad at x: [ 0.62803613 -0.08564129]  gradient norm: 0.633848417615767\n",
            "iter: 291  x: [-99.69226229  24.95803577]  f(x): 0.09646349234501303  grad at x: [ 0.61547541 -0.08392847]  gradient norm: 0.6211714492634478\n",
            "iter: 292  x: [-99.69841705  24.95887505]  f(x): 0.09264353804815327  grad at x: [ 0.6031659 -0.0822499]  gradient norm: 0.6087480202781879\n",
            "iter: 293  x: [-99.70444871  24.95969755]  f(x): 0.08897485394144646  grad at x: [ 0.59110259 -0.0806049 ]  gradient norm: 0.5965730598726243\n",
            "iter: 294  x: [-99.71035973  24.9605036 ]  f(x): 0.08545144972536377  grad at x: [ 0.57928053 -0.0789928 ]  gradient norm: 0.5846415986751671\n",
            "iter: 295  x: [-99.71615254  24.96129353]  f(x): 0.08206757231624084  grad at x: [ 0.56769492 -0.07741294]  gradient norm: 0.5729487667016688\n",
            "iter: 296  x: [-99.72182949  24.96206766]  f(x): 0.07881769645251724  grad at x: [ 0.55634102 -0.07586469]  gradient norm: 0.5614897913676338\n",
            "iter: 297  x: [-99.7273929  24.9628263]  f(x): 0.07569651567300041  grad at x: [ 0.5452142  -0.07434739]  gradient norm: 0.5502599955402915\n",
            "iter: 298  x: [-99.73284504  24.96356978]  f(x): 0.07269893365234924  grad at x: [ 0.53430992 -0.07286044]  gradient norm: 0.5392547956294844\n",
            "iter: 299  x: [-99.73818814  24.96429838]  f(x): 0.06982005587971943  grad at x: [ 0.52362372 -0.07140323]  gradient norm: 0.5284696997169068\n",
            "iter: 300  x: [-99.74342438  24.96501241]  f(x): 0.06705518166688414  grad at x: [ 0.51315125 -0.06997517]  gradient norm: 0.517900305722575\n",
            "iter: 301  x: [-99.74855589  24.96571217]  f(x): 0.0643997964728773  grad at x: [ 0.50288822 -0.06857567]  gradient norm: 0.5075422996081304\n",
            "iter: 302  x: [-99.75358477  24.96639792]  f(x): 0.0618495645325516  grad at x: [ 0.49283046 -0.06720415]  gradient norm: 0.49739145361596876\n",
            "iter: 303  x: [-99.75851308  24.96706996]  f(x): 0.05940032177706336  grad at x: [ 0.48297385 -0.06586007]  gradient norm: 0.48744362454365264\n",
            "iter: 304  x: [-99.76334281  24.96772857]  f(x): 0.05704806903469008  grad at x: [ 0.47331437 -0.06454287]  gradient norm: 0.477694752052773\n",
            "iter: 305  x: [-99.76807596  24.96837399]  f(x): 0.05478896550091642  grad at x: [ 0.46384808 -0.06325201]  gradient norm: 0.46814085701171787\n",
            "iter: 306  x: [-99.77271444  24.96900651]  f(x): 0.05261932246707985  grad at x: [ 0.45457112 -0.06198697]  gradient norm: 0.4587780398714823\n",
            "iter: 307  x: [-99.77726015  24.96962638]  f(x): 0.05053559729738085  grad at x: [ 0.4454797  -0.06074723]  gradient norm: 0.44960247907404094\n",
            "iter: 308  x: [-99.78171495  24.97023386]  f(x): 0.048534387644403806  grad at x: [ 0.43657011 -0.05953229]  gradient norm: 0.44061042949255663\n",
            "iter: 309  x: [-99.78608065  24.97082918]  f(x): 0.046612425893686776  grad at x: [ 0.4278387  -0.05834164]  gradient norm: 0.4317982209027118\n",
            "iter: 310  x: [-99.79035904  24.9714126 ]  f(x): 0.044766573828295284  grad at x: [ 0.41928193 -0.05717481]  gradient norm: 0.4231622564846505\n",
            "iter: 311  x: [-99.79455185  24.97198434]  f(x): 0.042993817504692895  grad at x: [ 0.41089629 -0.05603131]  gradient norm: 0.41469901135494835\n",
            "iter: 312  x: [-99.79866082  24.97254466]  f(x): 0.04129126233150554  grad at x: [ 0.40267837 -0.05491069]  gradient norm: 0.40640503112784193\n",
            "iter: 313  x: [-99.8026876   24.97309376]  f(x): 0.039656128343179144  grad at x: [ 0.3946248  -0.05381247]  gradient norm: 0.3982769305052912\n",
            "iter: 314  x: [-99.80663385  24.97363189]  f(x): 0.038085745660787575  grad at x: [ 0.3867323  -0.05273622]  gradient norm: 0.39031139189517683\n",
            "iter: 315  x: [-99.81050117  24.97415925]  f(x): 0.036577550132618424  grad at x: [ 0.37899766 -0.0516815 ]  gradient norm: 0.38250516405726304\n",
            "iter: 316  x: [-99.81429115  24.97467607]  f(x): 0.035129079147368764  grad at x: [ 0.3714177  -0.05064787]  gradient norm: 0.37485506077612857\n",
            "iter: 317  x: [-99.81800533  24.97518254]  f(x): 0.0337379676131323  grad at x: [ 0.36398935 -0.04963491]  gradient norm: 0.36735795956060247\n",
            "iter: 318  x: [-99.82164522  24.97567889]  f(x): 0.03240194409565086  grad at x: [ 0.35670956 -0.04864221]  gradient norm: 0.36001080036938254\n",
            "iter: 319  x: [-99.82521231  24.97616532]  f(x): 0.031118827109462326  grad at x: [ 0.34957537 -0.04766937]  gradient norm: 0.35281058436199064\n",
            "iter: 320  x: [-99.82870807  24.97664201]  f(x): 0.02988652155592981  grad at x: [ 0.34258386 -0.04671598]  gradient norm: 0.3457543726747635\n",
            "iter: 321  x: [-99.83213391  24.97710917]  f(x): 0.02870301530231343  grad at x: [ 0.33573219 -0.04578166]  gradient norm: 0.338839285221259\n",
            "iter: 322  x: [-99.83549123  24.97756699]  f(x): 0.02756637589634262  grad at x: [ 0.32901754 -0.04486603]  gradient norm: 0.33206249951683864\n",
            "iter: 323  x: [-99.8387814   24.97801565]  f(x): 0.02647474741084557  grad at x: [ 0.32243719 -0.04396871]  gradient norm: 0.3254212495264903\n",
            "iter: 324  x: [-99.84200578  24.97845533]  f(x): 0.025426347413376967  grad at x: [ 0.31598845 -0.04308933]  gradient norm: 0.31891282453596603\n",
            "iter: 325  x: [-99.84516566  24.97888623]  f(x): 0.02441946405580659  grad at x: [ 0.30966868 -0.04222755]  gradient norm: 0.3125345680452426\n",
            "iter: 326  x: [-99.84826235  24.9793085 ]  f(x): 0.023452453279198524  grad at x: [ 0.30347531 -0.041383  ]  gradient norm: 0.30628387668435\n",
            "iter: 327  x: [-99.8512971   24.97972233]  f(x): 0.022523736129341974  grad at x: [ 0.2974058  -0.04055534]  gradient norm: 0.30015819915066105\n",
            "iter: 328  x: [-99.85427116  24.98012789]  f(x): 0.02163179617861804  grad at x: [ 0.29145768 -0.03974423]  gradient norm: 0.29415503516763425\n",
            "iter: 329  x: [-99.85718574  24.98052533]  f(x): 0.020775177049942688  grad at x: [ 0.28562853 -0.03894934]  gradient norm: 0.2882719344642672\n",
            "iter: 330  x: [-99.86004202  24.98091482]  f(x): 0.01995248003876412  grad at x: [ 0.27991596 -0.03817036]  gradient norm: 0.2825064957749759\n",
            "iter: 331  x: [-99.86284118  24.98129652]  f(x): 0.019162361829227568  grad at x: [ 0.27431764 -0.03740695]  gradient norm: 0.2768563658594656\n",
            "iter: 332  x: [-99.86558436  24.98167059]  f(x): 0.018403532300790382  grad at x: [ 0.26883129 -0.03665881]  gradient norm: 0.27131923854227796\n",
            "iter: 333  x: [-99.86827267  24.98203718]  f(x): 0.017674752421680748  grad at x: [ 0.26345466 -0.03592564]  gradient norm: 0.2658928537714449\n",
            "iter: 334  x: [-99.87090722  24.98239644]  f(x): 0.01697483222578236  grad at x: [ 0.25818557 -0.03520712]  gradient norm: 0.2605749966960173\n",
            "iter: 335  x: [-99.87348907  24.98274851]  f(x): 0.016302628869640786  grad at x: [ 0.25302186 -0.03450298]  gradient norm: 0.25536349676209236\n",
            "iter: 336  x: [-99.87601929  24.98309354]  f(x): 0.01565704476640352  grad at x: [ 0.24796142 -0.03381292]  gradient norm: 0.25025622682685456\n",
            "iter: 337  x: [-99.8784989   24.98343167]  f(x): 0.015037025793654435  grad at x: [ 0.24300219 -0.03313666]  gradient norm: 0.2452511022903215\n",
            "iter: 338  x: [-99.88092893  24.98376304]  f(x): 0.014441559572226613  grad at x: [ 0.23814215 -0.03247393]  gradient norm: 0.2403460802445225\n",
            "iter: 339  x: [-99.88331035  24.98408777]  f(x): 0.013869673813166944  grad at x: [ 0.2333793  -0.03182445]  gradient norm: 0.23553915863963634\n",
            "iter: 340  x: [-99.88564414  24.98440602]  f(x): 0.013320434730165632  grad at x: [ 0.22871172 -0.03118796]  gradient norm: 0.23082837546684448\n",
            "iter: 341  x: [-99.88793126  24.9847179 ]  f(x): 0.012792945514851713  grad at x: [ 0.22413748 -0.0305642 ]  gradient norm: 0.22621180795751325\n",
            "iter: 342  x: [-99.89017263  24.98502354]  f(x): 0.012286344872462242  grad at x: [ 0.21965473 -0.02995292]  gradient norm: 0.22168757179835086\n",
            "iter: 343  x: [-99.89236918  24.98532307]  f(x): 0.011799805615514183  grad at x: [ 0.21526164 -0.02935386]  gradient norm: 0.21725382036239715\n",
            "iter: 344  x: [-99.8945218   24.98561661]  f(x): 0.011332533313140858  grad at x: [ 0.21095641 -0.02876678]  gradient norm: 0.21290874395515894\n",
            "iter: 345  x: [-99.89663136  24.98590428]  f(x): 0.010883764993941624  grad at x: [ 0.20673728 -0.02819145]  gradient norm: 0.20865056907606674\n",
            "iter: 346  x: [-99.89869873  24.98618619]  f(x): 0.010452767900182562  grad at x: [ 0.20260253 -0.02762762]  gradient norm: 0.20447755769455545\n",
            "iter: 347  x: [-99.90072476  24.98646247]  f(x): 0.010038838291335178  grad at x: [ 0.19855048 -0.02707507]  gradient norm: 0.20038800654066277\n",
            "iter: 348  x: [-99.90271026  24.98673322]  f(x): 0.00964130029499869  grad at x: [ 0.19457947 -0.02653356]  gradient norm: 0.19638024640985347\n",
            "iter: 349  x: [-99.90465606  24.98699855]  f(x): 0.009259504803315593  grad at x: [ 0.19068788 -0.02600289]  gradient norm: 0.19245264148164445\n",
            "iter: 350  x: [-99.90656294  24.98725858]  f(x): 0.008892828413105146  grad at x: [ 0.18687413 -0.02548284]  gradient norm: 0.18860358865202057\n",
            "iter: 351  x: [-99.90843168  24.98751341]  f(x): 0.008540672407945163  grad at x: [ 0.18313664 -0.02497318]  gradient norm: 0.18483151687896912\n",
            "iter: 352  x: [-99.91026304  24.98776314]  f(x): 0.008202461780591391  grad at x: [ 0.17947391 -0.02447372]  gradient norm: 0.1811348865413992\n",
            "iter: 353  x: [-99.91205778  24.98800788]  f(x): 0.007877644294080334  grad at x: [ 0.17588443 -0.02398424]  gradient norm: 0.17751218881057532\n",
            "iter: 354  x: [-99.91381663  24.98824772]  f(x): 0.007565689580035495  grad at x: [ 0.17236674 -0.02350456]  gradient norm: 0.17396194503437232\n",
            "iter: 355  x: [-99.9155403   24.98848277]  f(x): 0.0072660882726663335  grad at x: [ 0.16891941 -0.02303446]  gradient norm: 0.17048270613368774\n",
            "iter: 356  x: [-99.91722949  24.98871311]  f(x): 0.006978351177069392  grad at x: [ 0.16554102 -0.02257378]  gradient norm: 0.1670730520110217\n",
            "iter: 357  x: [-99.9188849   24.98893885]  f(x): 0.0067020084704585104  grad at x: [ 0.1622302 -0.0221223]  gradient norm: 0.1637315909708143\n",
            "iter: 358  x: [-99.9205072   24.98916007]  f(x): 0.0064366089350288425  grad at x: [ 0.1589856  -0.02167985]  gradient norm: 0.16045695915140412\n",
            "iter: 359  x: [-99.92209706  24.98937687]  f(x): 0.006181719221201094  grad at x: [ 0.15580588 -0.02124626]  gradient norm: 0.15724781996836834\n",
            "iter: 360  x: [-99.92365512  24.98958933]  f(x): 0.005936923140041502  grad at x: [ 0.15268977 -0.02082133]  gradient norm: 0.15410286356900058\n",
            "iter: 361  x: [-99.92518201  24.98979755]  f(x): 0.005701820983696095  grad at x: [ 0.14963597 -0.02040491]  gradient norm: 0.1510208062976237\n",
            "iter: 362  x: [-99.92667837  24.9900016 ]  f(x): 0.005476028872741241  grad at x: [ 0.14664325 -0.01999681]  gradient norm: 0.14800039017166464\n",
            "iter: 363  x: [-99.92814481  24.99020156]  f(x): 0.005259178129380311  grad at x: [ 0.14371039 -0.01959687]  gradient norm: 0.14504038236822614\n",
            "iter: 364  x: [-99.92958191  24.99039753]  f(x): 0.00505091467545734  grad at x: [ 0.14083618 -0.01920493]  gradient norm: 0.1421395747208685\n",
            "iter: 365  x: [-99.93099027  24.99058958]  f(x): 0.004850898454308272  grad at x: [ 0.13801946 -0.01882083]  gradient norm: 0.13929678322643738\n",
            "iter: 366  x: [-99.93237047  24.99077779]  f(x): 0.004658802875518515  grad at x: [ 0.13525907 -0.01844442]  gradient norm: 0.1365108475619211\n",
            "iter: 367  x: [-99.93372306  24.99096224]  f(x): 0.004474314281648937  grad at x: [ 0.13255388 -0.01807553]  gradient norm: 0.13378063061069695\n",
            "iter: 368  x: [-99.9350486   24.99114299]  f(x): 0.004297131436096554  grad at x: [ 0.12990281 -0.01771402]  gradient norm: 0.13110501799849697\n",
            "iter: 369  x: [-99.93634762  24.99132013]  f(x): 0.004126965031227007  grad at x: [ 0.12730475 -0.01735974]  gradient norm: 0.12848291763852512\n",
            "iter: 370  x: [-99.93762067  24.99149373]  f(x): 0.003963537215990374  grad at x: [ 0.12475866 -0.01701254]  gradient norm: 0.12591325928575392\n",
            "iter: 371  x: [-99.93886826  24.99166385]  f(x): 0.0038065811422364304  grad at x: [ 0.12226348 -0.01667229]  gradient norm: 0.12339499410002709\n",
            "iter: 372  x: [-99.94009089  24.99183058]  f(x): 0.0036558405290038304  grad at x: [ 0.11981821 -0.01633885]  gradient norm: 0.12092709421802593\n",
            "iter: 373  x: [-99.94128908  24.99199396]  f(x): 0.003511069244055539  grad at x: [ 0.11742185 -0.01601207]  gradient norm: 0.11850855233366982\n",
            "iter: 374  x: [-99.94246329  24.99215409]  f(x): 0.0033720309019913807  grad at x: [ 0.11507341 -0.01569183]  gradient norm: 0.116138381287004\n",
            "iter: 375  x: [-99.94361403  24.992311  ]  f(x): 0.0032384984782726956  grad at x: [ 0.11277194 -0.01537799]  gradient norm: 0.11381561366126698\n",
            "iter: 376  x: [-99.94474175  24.99246478]  f(x): 0.0031102539385333425  grad at x: [ 0.1105165  -0.01507043]  gradient norm: 0.11153930138804605\n",
            "iter: 377  x: [-99.94584691  24.99261549]  f(x): 0.0029870878825675453  grad at x: [ 0.10830617 -0.01476902]  gradient norm: 0.10930851536028738\n",
            "iter: 378  x: [-99.94692997  24.99276318]  f(x): 0.002868799202418183  grad at x: [ 0.10614005 -0.01447364]  gradient norm: 0.10712234505308746\n",
            "iter: 379  x: [-99.94799137  24.99290791]  f(x): 0.002755194754002351  grad at x: [ 0.10401725 -0.01418417]  gradient norm: 0.10497989815202434\n",
            "iter: 380  x: [-99.94903155  24.99304976]  f(x): 0.0026460890417434133  grad at x: [ 0.10193691 -0.01390049]  gradient norm: 0.10288030018897522\n",
            "iter: 381  x: [-99.95005092  24.99318876]  f(x): 0.0025413039156901075  grad at x: [ 0.09989817 -0.01362248]  gradient norm: 0.10082269418519042\n",
            "iter: 382  x: [-99.9510499   24.99332499]  f(x): 0.0024406682806288223  grad at x: [ 0.0979002  -0.01335003]  gradient norm: 0.09880624030148748\n",
            "iter: 383  x: [-99.9520289   24.99345849]  f(x): 0.00234401781671622  grad at x: [ 0.0959422  -0.01308303]  gradient norm: 0.09683011549546391\n",
            "iter: 384  x: [-99.95298832  24.99358932]  f(x): 0.002251194711173784  grad at x: [ 0.09402336 -0.01282137]  gradient norm: 0.09489351318554465\n",
            "iter: 385  x: [-99.95392856  24.99371753]  f(x): 0.00216204740061084  grad at x: [ 0.09214289 -0.01256494]  gradient norm: 0.09299564292182382\n",
            "iter: 386  x: [-99.95484998  24.99384318]  f(x): 0.0020764303235466745  grad at x: [ 0.09030003 -0.01231364]  gradient norm: 0.09113573006338786\n",
            "iter: 387  x: [-99.95575298  24.99396632]  f(x): 0.001994203682734539  grad at x: [ 0.08849403 -0.01206737]  gradient norm: 0.08931301546212711\n",
            "iter: 388  x: [-99.95663793  24.99408699]  f(x): 0.0019152332168980621  grad at x: [ 0.08672415 -0.01182602]  gradient norm: 0.08752675515288023\n",
            "iter: 389  x: [-99.95750517  24.99420525]  f(x): 0.001839389981508306  grad at x: [ 0.08498967 -0.0115895 ]  gradient norm: 0.08577622004980881\n",
            "iter: 390  x: [-99.95835506  24.99432115]  f(x): 0.0017665501382408601  grad at x: [ 0.08328987 -0.01135771]  gradient norm: 0.08406069564881938\n",
            "iter: 391  x: [-99.95918796  24.99443472]  f(x): 0.0016965947527664343  grad at x: [ 0.08162408 -0.01113056]  gradient norm: 0.08237948173584085\n",
            "iter: 392  x: [-99.9600042   24.99454603]  f(x): 0.0016294096005569304  grad at x: [ 0.07999159 -0.01090794]  gradient norm: 0.0807318921011252\n",
            "iter: 393  x: [-99.96080412  24.99465511]  f(x): 0.0015648849803751718  grad at x: [ 0.07839176 -0.01068979]  gradient norm: 0.07911725425911018\n",
            "iter: 394  x: [-99.96158804  24.994762  ]  f(x): 0.0015029155351526423  grad at x: [ 0.07682393 -0.01047599]  gradient norm: 0.07753490917393642\n",
            "iter: 395  x: [-99.96235628  24.99486676]  f(x): 0.0014434000799610797  grad at x: [ 0.07528745 -0.01026647]  gradient norm: 0.07598421099047037\n",
            "iter: 396  x: [-99.96310915  24.99496943]  f(x): 0.0013862414367947346  grad at x: [ 0.0737817  -0.01006114]  gradient norm: 0.07446452677066402\n",
            "iter: 397  x: [-99.96384697  24.99507004]  f(x): 0.0013313462758974782  grad at x: [ 0.07230607 -0.00985992]  gradient norm: 0.07297523623524567\n",
            "iter: 398  x: [-99.96457003  24.99516864]  f(x): 0.0012786249633722887  grad at x: [ 0.07085994 -0.00966272]  gradient norm: 0.07151573151055056\n",
            "iter: 399  x: [-99.96527863  24.99526527]  f(x): 0.0012279914148228203  grad at x: [ 0.06944275 -0.00946947]  gradient norm: 0.07008541688034167\n",
            "iter: 400  x: [-99.96597305  24.99535996]  f(x): 0.0011793629547957546  grad at x: [ 0.06805389 -0.00928008]  gradient norm: 0.06868370854273245\n",
            "iter: 401  x: [-99.96665359  24.99545276]  f(x): 0.0011326601817857685  grad at x: [ 0.06669281 -0.00909447]  gradient norm: 0.06731003437187559\n",
            "iter: 402  x: [-99.96732052  24.99554371]  f(x): 0.0010878068385871173  grad at x: [ 0.06535896 -0.00891258]  gradient norm: 0.06596383368444006\n",
            "iter: 403  x: [-99.96797411  24.99563283]  f(x): 0.0010447296877794752  grad at x: [ 0.06405178 -0.00873433]  gradient norm: 0.06464455701076387\n",
            "iter: 404  x: [-99.96861463  24.99572018]  f(x): 0.0010033583921431617  grad at x: [ 0.06277074 -0.00855965]  gradient norm: 0.06335166587054082\n",
            "iter: 405  x: [-99.96924234  24.99580577]  f(x): 0.0009636253998140354  grad at x: [ 0.06151533 -0.00838845]  gradient norm: 0.06208463255312172\n",
            "iter: 406  x: [-99.96985749  24.99588966]  f(x): 0.0009254658339816063  grad at x: [ 0.06028502 -0.00822068]  gradient norm: 0.06084293990206609\n",
            "iter: 407  x: [-99.97046034  24.99597186]  f(x): 0.0008888173869557953  grad at x: [ 0.05907932 -0.00805627]  gradient norm: 0.05962608110402009\n",
            "iter: 408  x: [-99.97105113  24.99605243]  f(x): 0.000853620218432044  grad at x: [ 0.05789773 -0.00789515]  gradient norm: 0.058433559481929354\n",
            "iter: 409  x: [-99.97163011  24.99613138]  f(x): 0.0008198168577822424  grad at x: [ 0.05673978 -0.00773724]  gradient norm: 0.057264888292294515\n",
            "iter: 410  x: [-99.97219751  24.99620875]  f(x): 0.0007873521102139674  grad at x: [ 0.05560498 -0.0075825 ]  gradient norm: 0.05611959052644513\n",
            "iter: 411  x: [-99.97275356  24.99628458]  f(x): 0.000756172966649316  grad at x: [ 0.05449288 -0.00743085]  gradient norm: 0.05499719871590974\n",
            "iter: 412  x: [-99.97329849  24.99635888]  f(x): 0.00072622851716972  grad at x: [ 0.05340303 -0.00728223]  gradient norm: 0.05389725474158104\n",
            "iter: 413  x: [-99.97383252  24.99643171]  f(x): 0.0006974698678897103  grad at x: [ 0.05233497 -0.00713659]  gradient norm: 0.05281930964674606\n",
            "iter: 414  x: [-99.97435587  24.99650307]  f(x): 0.0006698500611214879  grad at x: [ 0.05128827 -0.00699385]  gradient norm: 0.051762923453819255\n",
            "iter: 415  x: [-99.97486875  24.99657301]  f(x): 0.0006433239987012167  grad at x: [ 0.0502625  -0.00685398]  gradient norm: 0.05072766498474838\n",
            "iter: 416  x: [-99.97537137  24.99664155]  f(x): 0.0006178483683528406  grad at x: [ 0.04925725 -0.0067169 ]  gradient norm: 0.04971311168506114\n",
            "iter: 417  x: [-99.97586395  24.99670872]  f(x): 0.0005933815729659833  grad at x: [ 0.04827211 -0.00658256]  gradient norm: 0.04871884945135643\n",
            "iter: 418  x: [-99.97634667  24.99677455]  f(x): 0.0005698836626763296  grad at x: [ 0.04730666 -0.00645091]  gradient norm: 0.04774447246232089\n",
            "iter: 419  x: [-99.97681973  24.99683905]  f(x): 0.0005473162696343918  grad at x: [ 0.04636053 -0.00632189]  gradient norm: 0.0467895830130764\n",
            "iter: 420  x: [-99.97728334  24.99690227]  f(x): 0.0005256425453570507  grad at x: [ 0.04543332 -0.00619545]  gradient norm: 0.045853791352822755\n",
            "iter: 421  x: [-99.97773767  24.99696423]  f(x): 0.0005048271005606382  grad at x: [ 0.04452465 -0.00607154]  gradient norm: 0.04493671552575414\n",
            "iter: 422  x: [-99.97818292  24.99702494]  f(x): 0.0004848359473784706  grad at x: [ 0.04363416 -0.00595011]  gradient norm: 0.044037981215240586\n",
            "iter: 423  x: [-99.97861926  24.99708444]  f(x): 0.000465636443862385  grad at x: [ 0.04276148 -0.00583111]  gradient norm: 0.043157221590940494\n",
            "iter: 424  x: [-99.97904688  24.99714276]  f(x): 0.00044719724068547923  grad at x: [ 0.04190625 -0.00571449]  gradient norm: 0.04229407715912379\n",
            "iter: 425  x: [-99.97946594  24.9971999 ]  f(x): 0.0004294882299544642  grad at x: [ 0.04106812 -0.0056002 ]  gradient norm: 0.04144819561594759\n",
            "iter: 426  x: [-99.97987662  24.9972559 ]  f(x): 0.0004124804960483492  grad at x: [ 0.04024676 -0.00548819]  gradient norm: 0.040619231703632665\n",
            "iter: 427  x: [-99.98027909  24.99731078]  f(x): 0.0003961462684049197  grad at x: [ 0.03944182 -0.00537843]  gradient norm: 0.039806847069564284\n",
            "iter: 428  x: [-99.98067351  24.99736457]  f(x): 0.0003804588761760966  grad at x: [ 0.03865299 -0.00527086]  gradient norm: 0.0390107101281736\n",
            "iter: 429  x: [-99.98106004  24.99741728]  f(x): 0.00036539270467939543  grad at x: [ 0.03787993 -0.00516544]  gradient norm: 0.03823049592560345\n",
            "iter: 430  x: [-99.98143883  24.99746893]  f(x): 0.0003509231535742235  grad at x: [ 0.03712233 -0.00506214]  gradient norm: 0.03746588600709843\n",
            "iter: 431  x: [-99.98181006  24.99751955]  f(x): 0.000337026596692589  grad at x: [ 0.03637988 -0.00496089]  gradient norm: 0.03671656828695127\n",
            "iter: 432  x: [-99.98217386  24.99756916]  f(x): 0.00032368034346375604  grad at x: [ 0.03565229 -0.00486168]  gradient norm: 0.03598223692122301\n",
            "iter: 433  x: [-99.98253038  24.99761778]  f(x): 0.0003108626018625817  grad at x: [ 0.03493924 -0.00476444]  gradient norm: 0.035262592182798\n",
            "iter: 434  x: [-99.98287977  24.99766542]  f(x): 0.0002985524428290405  grad at x: [ 0.03424046 -0.00466915]  gradient norm: 0.034557340339154606\n",
            "iter: 435  x: [-99.98322218  24.99771212]  f(x): 0.0002867297660931276  grad at x: [ 0.03355565 -0.00457577]  gradient norm: 0.03386619353237843\n",
            "iter: 436  x: [-99.98355773  24.99775787]  f(x): 0.0002753752673557611  grad at x: [ 0.03288453 -0.00448425]  gradient norm: 0.03318886966172612\n",
            "iter: 437  x: [-99.98388658  24.99780272]  f(x): 0.0002644704067683858  grad at x: [ 0.03222684 -0.00439457]  gradient norm: 0.03252509226848624\n",
            "iter: 438  x: [-99.98420885  24.99784666]  f(x): 0.0002539973786601296  grad at x: [ 0.03158231 -0.00430668]  gradient norm: 0.0318745904231022\n",
            "iter: 439  x: [-99.98452467  24.99788973]  f(x): 0.000243939082465382  grad at x: [ 0.03095066 -0.00422054]  gradient norm: 0.031237098614652545\n",
            "iter: 440  x: [-99.98483418  24.99793193]  f(x): 0.0002342790947998243  grad at x: [ 0.03033165 -0.00413613]  gradient norm: 0.030612356642364162\n",
            "iter: 441  x: [-99.98513749  24.99797329]  f(x): 0.00022500164264561848  grad at x: [ 0.02972501 -0.00405341]  gradient norm: 0.030000109509508027\n",
            "iter: 442  x: [-99.98543474  24.99801383]  f(x): 0.00021609157759702922  grad at x: [ 0.02913051 -0.00397234]  gradient norm: 0.029400107319329922\n",
            "iter: 443  x: [-99.98572605  24.99805355]  f(x): 0.0002075343511240458  grad at x: [ 0.0285479 -0.0038929]  gradient norm: 0.028812105172933533\n",
            "iter: 444  x: [-99.98601153  24.99809248]  f(x): 0.00019931599081967526  grad at x: [ 0.02797694 -0.00381504]  gradient norm: 0.028235863069484897\n",
            "iter: 445  x: [-99.9862913   24.99813063]  f(x): 0.00019142307758305896  grad at x: [ 0.02741741 -0.00373874]  gradient norm: 0.02767114580808384\n",
            "iter: 446  x: [-99.98656547  24.99816802]  f(x): 0.00018384272371084375  grad at x: [ 0.02686906 -0.00366396]  gradient norm: 0.027117722891927615\n",
            "iter: 447  x: [-99.98683416  24.99820466]  f(x): 0.0001765625518517793  grad at x: [ 0.02633168 -0.00359068]  gradient norm: 0.026575368434080406\n",
            "iter: 448  x: [-99.98709748  24.99824057]  f(x): 0.00016957067479842682  grad at x: [ 0.02580504 -0.00351887]  gradient norm: 0.026043861065397106\n",
            "iter: 449  x: [-99.98735553  24.99827575]  f(x): 0.0001628556760762822  grad at x: [ 0.02528894 -0.00344849]  gradient norm: 0.02552298384407922\n",
            "iter: 450  x: [-99.98760842  24.99831024]  f(x): 0.00015640659130381976  grad at x: [ 0.02478316 -0.00337952]  gradient norm: 0.025012524167210295\n",
            "iter: 451  x: [-99.98785625  24.99834403]  f(x): 0.00015021289028826034  grad at x: [ 0.0242875  -0.00331193]  gradient norm: 0.02451227368387195\n",
            "iter: 452  x: [-99.98809912  24.99837715]  f(x): 0.00014426445983298938  grad at x: [ 0.02380175 -0.00324569]  gradient norm: 0.024022028210206513\n",
            "iter: 453  x: [-99.98833714  24.99840961]  f(x): 0.00013855158722360917  grad at x: [ 0.02332572 -0.00318078]  gradient norm: 0.023541587646002907\n",
            "iter: 454  x: [-99.9885704   24.99844142]  f(x): 0.00013306494436960718  grad at x: [ 0.0228592  -0.00311716]  gradient norm: 0.02307075589308744\n",
            "iter: 455  x: [-99.98879899  24.99847259]  f(x): 0.00012779557257270777  grad at x: [ 0.02240202 -0.00305482]  gradient norm: 0.02260934077523781\n",
            "iter: 456  x: [-99.98902301  24.99850314]  f(x): 0.00012273486789878614  grad at x: [ 0.02195398 -0.00299372]  gradient norm: 0.02215715395972923\n",
            "iter: 457  x: [-99.98924255  24.99853308]  f(x): 0.00011787456713009121  grad at x: [ 0.0215149  -0.00293385]  gradient norm: 0.021714010880543578\n",
            "iter: 458  x: [-99.9894577   24.99856241]  f(x): 0.00011320673427173177  grad at x: [ 0.0210846  -0.00287517]  gradient norm: 0.02127973066293197\n",
            "iter: 459  x: [-99.98966855  24.99859117]  f(x): 0.00010872374759448103  grad at x: [ 0.02066291 -0.00281767]  gradient norm: 0.020854136049664684\n",
            "iter: 460  x: [-99.98987518  24.99861934]  f(x): 0.00010441828718960087  grad at x: [ 0.02024965 -0.00276132]  gradient norm: 0.020437053328657813\n",
            "iter: 461  x: [-99.99007767  24.99864696]  f(x): 0.00010028332301700606  grad at x: [ 0.01984466 -0.00270609]  gradient norm: 0.02002831226209598\n",
            "iter: 462  x: [-99.99027612  24.99867402]  f(x): 9.631210342541848e-05  grad at x: [ 0.01944776 -0.00265197]  gradient norm: 0.01962774601684243\n",
            "iter: 463  x: [-99.9904706   24.99870054]  f(x): 9.249814412984546e-05  grad at x: [ 0.01905881 -0.00259893]  gradient norm: 0.01923519109651323\n",
            "iter: 464  x: [-99.99066118  24.99872653]  f(x): 8.883521762230836e-05  grad at x: [ 0.01867763 -0.00254695]  gradient norm: 0.018850487274583473\n",
            "iter: 465  x: [-99.99084796  24.99875199]  f(x): 8.531734300456095e-05  grad at x: [ 0.01830408 -0.00249601]  gradient norm: 0.018473477529102198\n",
            "iter: 466  x: [-99.991031    24.99877695]  f(x): 8.193877622162239e-05  grad at x: [ 0.017938   -0.00244609]  gradient norm: 0.018104007978524798\n",
            "iter: 467  x: [-99.99121038  24.99880142]  f(x): 7.869400068321049e-05  grad at x: [ 0.01757924 -0.00239717]  gradient norm: 0.017741927818950284\n",
            "iter: 468  x: [-99.99138617  24.99882539]  f(x): 7.557771825607824e-05  grad at x: [ 0.01722765 -0.00234923]  gradient norm: 0.017387089262562407\n",
            "iter: 469  x: [-99.99155845  24.99884888]  f(x): 7.2584840613116e-05  grad at x: [ 0.0168831  -0.00230224]  gradient norm: 0.01703934747730863\n",
            "iter: 470  x: [-99.99172728  24.9988719 ]  f(x): 6.971048092472456e-05  grad at x: [ 0.01654544 -0.0022562 ]  gradient norm: 0.016698560527749037\n",
            "iter: 471  x: [-99.99189274  24.99889446]  f(x): 6.694994588015685e-05  grad at x: [ 0.01621453 -0.00221107]  gradient norm: 0.016364589317200338\n",
            "iter: 472  x: [-99.99205488  24.99891657]  f(x): 6.429872802330437e-05  grad at x: [ 0.01589024 -0.00216685]  gradient norm: 0.01603729753085655\n",
            "iter: 473  x: [-99.99221378  24.99893824]  f(x): 6.175249839367647e-05  grad at x: [ 0.01557243 -0.00212351]  gradient norm: 0.0157165515802515\n",
            "iter: 474  x: [-99.99236951  24.99895948]  f(x): 5.930709945727959e-05  grad at x: [ 0.01526098 -0.00208104]  gradient norm: 0.015402220548645523\n",
            "iter: 475  x: [-99.99252212  24.99898029]  f(x): 5.695853831876204e-05  grad at x: [ 0.01495577 -0.00203942]  gradient norm: 0.015094176137671382\n",
            "iter: 476  x: [-99.99267168  24.99900068]  f(x): 5.470298020139047e-05  grad at x: [ 0.01465665 -0.00199863]  gradient norm: 0.014792292614924905\n",
            "iter: 477  x: [-99.99281824  24.99902067]  f(x): 5.253674218539412e-05  grad at x: [ 0.01436352 -0.00195866]  gradient norm: 0.014496446762623471\n",
            "iter: 478  x: [-99.99296188  24.99904026]  f(x): 5.045628719488284e-05  grad at x: [ 0.01407625 -0.00191949]  gradient norm: 0.014206517827375269\n",
            "iter: 479  x: [-99.99310264  24.99905945]  f(x): 4.845821822188002e-05  grad at x: [ 0.01379472 -0.0018811 ]  gradient norm: 0.013922387470815487\n",
            "iter: 480  x: [-99.99324059  24.99907826]  f(x): 4.653927278019848e-05  grad at x: [ 0.01351883 -0.00184348]  gradient norm: 0.013643939721385239\n",
            "iter: 481  x: [-99.99337577  24.9990967 ]  f(x): 4.4696317578027996e-05  grad at x: [ 0.01324845 -0.00180661]  gradient norm: 0.013371060926946373\n",
            "iter: 482  x: [-99.99350826  24.99911476]  f(x): 4.2926343401871536e-05  grad at x: [ 0.01298348 -0.00177047]  gradient norm: 0.013103639708397287\n",
            "iter: 483  x: [-99.99363809  24.99913247]  f(x): 4.1226460203195926e-05  grad at x: [ 0.01272381 -0.00173507]  gradient norm: 0.012841566914235339\n",
            "iter: 484  x: [-99.99376533  24.99914982]  f(x): 3.959389237916548e-05  grad at x: [ 0.01246934 -0.00170036]  gradient norm: 0.012584735575953193\n",
            "iter: 485  x: [-99.99389003  24.99916682]  f(x): 3.802597424096911e-05  grad at x: [ 0.01221995 -0.00166636]  gradient norm: 0.012333040864437142\n",
            "iter: 486  x: [-99.99401223  24.99918349]  f(x): 3.652014566099448e-05  grad at x: [ 0.01197555 -0.00163303]  gradient norm: 0.012086380047143061\n",
            "iter: 487  x: [-99.99413198  24.99919982]  f(x): 3.507394789290165e-05  grad at x: [ 0.01173604 -0.00160037]  gradient norm: 0.01184465244621414\n",
            "iter: 488  x: [-99.99424934  24.99921582]  f(x): 3.368501955634275e-05  grad at x: [ 0.01150132 -0.00156836]  gradient norm: 0.011607759397289858\n",
            "iter: 489  x: [-99.99436435  24.9992315 ]  f(x): 3.235109278193917e-05  grad at x: [ 0.01127129 -0.00153699]  gradient norm: 0.011375604209348912\n",
            "iter: 490  x: [-99.99447707  24.99924687]  f(x): 3.106998950771462e-05  grad at x: [ 0.01104587 -0.00150625]  gradient norm: 0.011148092125151212\n",
            "iter: 491  x: [-99.99458753  24.99926194]  f(x): 2.9839617923260062e-05  grad at x: [ 0.01082495 -0.00147613]  gradient norm: 0.010925130282657514\n",
            "iter: 492  x: [-99.99469578  24.9992767 ]  f(x): 2.8657969053452164e-05  grad at x: [ 0.01060845 -0.00144661]  gradient norm: 0.01070662767699562\n",
            "iter: 493  x: [-99.99480186  24.99929116]  f(x): 2.752311347891561e-05  grad at x: [ 0.01039628 -0.00141767]  gradient norm: 0.010492495123451925\n",
            "iter: 494  x: [-99.99490582  24.99930534]  f(x): 2.6433198185187112e-05  grad at x: [ 0.01018836 -0.00138932]  gradient norm: 0.010282645220989999\n",
            "iter: 495  x: [-99.99500771  24.99931923]  f(x): 2.5386443537024515e-05  grad at x: [ 0.00998459 -0.00136153]  gradient norm: 0.010076992316564405\n",
            "iter: 496  x: [-99.99510755  24.99933285]  f(x): 2.4381140372979742e-05  grad at x: [ 0.0097849 -0.0013343]  gradient norm: 0.009875452470237451\n",
            "iter: 497  x: [-99.9952054   24.99934619]  f(x): 2.341564721414867e-05  grad at x: [ 0.0095892  -0.00130762]  gradient norm: 0.00967794342082008\n",
            "iter: 498  x: [-99.99530129  24.99935927]  f(x): 2.248838758441545e-05  grad at x: [ 0.00939741 -0.00128147]  gradient norm: 0.009484384552392517\n",
            "iter: 499  x: [-99.99539527  24.99937208]  f(x): 2.159784743608857e-05  grad at x: [ 0.00920947 -0.00125584]  gradient norm: 0.009294696861348104\n",
            "iter: 500  x: [-99.99548736  24.99938464]  f(x): 2.0742572677617454e-05  grad at x: [ 0.00902528 -0.00123072]  gradient norm: 0.0091088029241207\n",
            "iter: 501  x: [-99.99557761  24.99939695]  f(x): 1.9921166799566205e-05  grad at x: [ 0.00884477 -0.00120611]  gradient norm: 0.008926626865634344\n",
            "iter: 502  x: [-99.99566606  24.99940901]  f(x): 1.913228859430022e-05  grad at x: [ 0.00866788 -0.00118198]  gradient norm: 0.008748094328320933\n",
            "iter: 503  x: [-99.99575274  24.99942083]  f(x): 1.8374649965919567e-05  grad at x: [ 0.00849452 -0.00115834]  gradient norm: 0.008573132441743698\n",
            "iter: 504  x: [-99.99583769  24.99943241]  f(x): 1.7647013827273964e-05  grad at x: [ 0.00832463 -0.00113518]  gradient norm: 0.008401669792909969\n",
            "iter: 505  x: [-99.99592093  24.99944376]  f(x): 1.694819207966667e-05  grad at x: [ 0.00815814 -0.00111247]  gradient norm: 0.008233636397040294\n",
            "iter: 506  x: [-99.99600251  24.99945489]  f(x): 1.6277043673367416e-05  grad at x: [ 0.00799497 -0.00109022]  gradient norm: 0.008068963669113256\n",
            "iter: 507  x: [-99.99608246  24.99946579]  f(x): 1.563247274388164e-05  grad at x: [ 0.00783507 -0.00106842]  gradient norm: 0.007907584395725826\n",
            "iter: 508  x: [-99.99616081  24.99947647]  f(x): 1.5013426823234024e-05  grad at x: [ 0.00767837 -0.00104705]  gradient norm: 0.007749432707813914\n",
            "iter: 509  x: [-99.9962376   24.99948695]  f(x): 1.4418895121047662e-05  grad at x: [ 0.0075248  -0.00102611]  gradient norm: 0.007594444053661245\n",
            "iter: 510  x: [-99.99631285  24.99949721]  f(x): 1.3847906874271753e-05  grad at x: [ 0.00737431 -0.00100559]  gradient norm: 0.007442555172592744\n",
            "iter: 511  x: [-99.99638659  24.99950726]  f(x): 1.3299529762024939e-05  grad at x: [ 0.00722682 -0.00098548]  gradient norm: 0.007293704069133855\n",
            "iter: 512  x: [-99.99645886  24.99951712]  f(x): 1.2772868383488187e-05  grad at x: [ 0.00708229 -0.00096577]  gradient norm: 0.007147829987762212\n",
            "iter: 513  x: [-99.99652968  24.99952677]  f(x): 1.2267062795531846e-05  grad at x: [ 0.00694064 -0.00094645]  gradient norm: 0.007004873388015474\n",
            "iter: 514  x: [-99.99659909  24.99953624]  f(x): 1.1781287108806904e-05  grad at x: [ 0.00680183 -0.00092752]  gradient norm: 0.0068647759202487895\n",
            "iter: 515  x: [-99.9966671   24.99954551]  f(x): 1.1314748139333632e-05  grad at x: [ 0.00666579 -0.00090897]  gradient norm: 0.0067274804018543615\n",
            "iter: 516  x: [-99.99673376  24.9995546 ]  f(x): 1.0866684113059481e-05  grad at x: [ 0.00653247 -0.00089079]  gradient norm: 0.006592930793830459\n",
            "iter: 517  x: [-99.99679909  24.99956351]  f(x): 1.0436363422152758e-05  grad at x: [ 0.00640183 -0.00087298]  gradient norm: 0.006461072177944697\n",
            "iter: 518  x: [-99.99686311  24.99957224]  f(x): 1.0023083430644162e-05  grad at x: [ 0.00627379 -0.00085552]  gradient norm: 0.006331850734388536\n",
            "iter: 519  x: [-99.99692584  24.9995808 ]  f(x): 9.626169326763288e-06  grad at x: [ 0.00614831 -0.00083841]  gradient norm: 0.006205213719691945\n",
            "iter: 520  x: [-99.99698733  24.99958918]  f(x): 9.244973021412254e-06  grad at x: [ 0.00602535 -0.00082164]  gradient norm: 0.006081109445294421\n",
            "iter: 521  x: [-99.99704758  24.9995974 ]  f(x): 8.878872089733185e-06  grad at x: [ 0.00590484 -0.00080521]  gradient norm: 0.0059594872563780805\n",
            "iter: 522  x: [-99.99710663  24.99960545]  f(x): 8.527268754948092e-06  grad at x: [ 0.00578674 -0.0007891 ]  gradient norm: 0.005840297511239678\n",
            "iter: 523  x: [-99.9971645   24.99961334]  f(x): 8.189588912244033e-06  grad at x: [ 0.00567101 -0.00077332]  gradient norm: 0.005723491561012048\n",
            "iter: 524  x: [-99.99722121  24.99962107]  f(x): 7.865281191307125e-06  grad at x: [ 0.00555759 -0.00075785]  gradient norm: 0.005609021729787512\n",
            "iter: 525  x: [-99.99727678  24.99962865]  f(x): 7.55381605613949e-06  grad at x: [ 0.00544644 -0.0007427 ]  gradient norm: 0.005496841295194719\n",
            "iter: 526  x: [-99.99733125  24.99963608]  f(x): 7.254684940337604e-06  grad at x: [ 0.00533751 -0.00072784]  gradient norm: 0.00538690446929871\n",
            "iter: 527  x: [-99.99738462  24.99964336]  f(x): 6.96739941672838e-06  grad at x: [ 0.00523076 -0.00071329]  gradient norm: 0.005279166379923398\n",
            "iter: 528  x: [-99.99743693  24.99965049]  f(x): 6.691490399824728e-06  grad at x: [ 0.00512614 -0.00069902]  gradient norm: 0.005173583052324463\n",
            "iter: 529  x: [-99.99748819  24.99965748]  f(x): 6.426507380018342e-06  grad at x: [ 0.00502362 -0.00068504]  gradient norm: 0.005070111391288496\n",
            "iter: 530  x: [-99.99753843  24.99966433]  f(x): 6.172017687773178e-06  grad at x: [ 0.00492315 -0.00067134]  gradient norm: 0.00496870916346416\n",
            "iter: 531  x: [-99.99758766  24.99967104]  f(x): 5.9276057873691324e-06  grad at x: [ 0.00482468 -0.00065791]  gradient norm: 0.0048693349802079264\n",
            "iter: 532  x: [-99.9976359   24.99967762]  f(x): 5.692872598191163e-06  grad at x: [ 0.00472819 -0.00064475]  gradient norm: 0.004771948280604543\n",
            "iter: 533  x: [-99.99768319  24.99968407]  f(x): 5.467434843318551e-06  grad at x: [ 0.00463363 -0.00063186]  gradient norm: 0.004676509314999191\n",
            "iter: 534  x: [-99.99772952  24.99969039]  f(x): 5.2509244235416015e-06  grad at x: [ 0.00454095 -0.00061922]  gradient norm: 0.004582979128707265\n",
            "iter: 535  x: [-99.99777493  24.99969658]  f(x): 5.04298781634275e-06  grad at x: [ 0.00445013 -0.00060684]  gradient norm: 0.004491319546121273\n",
            "iter: 536  x: [-99.99781943  24.99970265]  f(x): 4.843285498801562e-06  grad at x: [ 0.00436113 -0.0005947 ]  gradient norm: 0.00440149315519248\n",
            "iter: 537  x: [-99.99786305  24.9997086 ]  f(x): 4.6514913930419526e-06  grad at x: [ 0.00427391 -0.00058281]  gradient norm: 0.0043134632920853525\n",
            "iter: 538  x: [-99.99790578  24.99971443]  f(x): 4.467292333856997e-06  grad at x: [ 0.00418843 -0.00057115]  gradient norm: 0.004227194026233949\n",
            "iter: 539  x: [-99.99794767  24.99972014]  f(x): 4.290387557433767e-06  grad at x: [ 0.00410466 -0.00055973]  gradient norm: 0.0041426501457080675\n",
            "iter: 540  x: [-99.99798872  24.99972573]  f(x): 4.120488210153349e-06  grad at x: [ 0.00402257 -0.00054853]  gradient norm: 0.00405979714279093\n",
            "iter: 541  x: [-99.99802894  24.99973122]  f(x): 3.957316877055493e-06  grad at x: [ 0.00394212 -0.00053756]  gradient norm: 0.0039786011999472844\n",
            "iter: 542  x: [-99.99806836  24.99973659]  f(x): 3.800607128713427e-06  grad at x: [ 0.00386328 -0.00052681]  gradient norm: 0.0038990291759428666\n",
            "iter: 543  x: [-99.99810699  24.99974186]  f(x): 3.6501030864091e-06  grad at x: [ 0.00378601 -0.00051627]  gradient norm: 0.003821048592420201\n",
            "iter: 544  x: [-99.99814485  24.99974703]  f(x): 3.5055590042140582e-06  grad at x: [ 0.00371029 -0.00050595]  gradient norm: 0.003744627620586089\n",
            "iter: 545  x: [-99.99818196  24.99975209]  f(x): 3.366738867626513e-06  grad at x: [ 0.00363608 -0.00049583]  gradient norm: 0.0036697350681631027\n",
            "iter: 546  x: [-99.99821832  24.99975704]  f(x): 3.2334160084589736e-06  grad at x: [ 0.00356336 -0.00048591]  gradient norm: 0.003596340366794541\n",
            "iter: 547  x: [-99.99825395  24.9997619 ]  f(x): 3.1053727345161255e-06  grad at x: [ 0.0034921  -0.00047619]  gradient norm: 0.0035244135594541827\n",
            "iter: 548  x: [-99.99828887  24.99976666]  f(x): 2.9823999742086955e-06  grad at x: [ 0.00342225 -0.00046667]  gradient norm: 0.0034539252882531757\n",
            "iter: 549  x: [-99.9983231   24.99977133]  f(x): 2.8642969352161233e-06  grad at x: [ 0.00335381 -0.00045734]  gradient norm: 0.003384846782479894\n",
            "iter: 550  x: [-99.99835663  24.9997759 ]  f(x): 2.7508707765941543e-06  grad at x: [ 0.00328673 -0.00044819]  gradient norm: 0.003317149846837887\n",
            "iter: 551  x: [-99.9983895   24.99978039]  f(x): 2.6419362938191485e-06  grad at x: [ 0.003221   -0.00043923]  gradient norm: 0.0032508068498876695\n",
            "iter: 552  x: [-99.99842171  24.99978478]  f(x): 2.5373156165786494e-06  grad at x: [ 0.00315658 -0.00043044]  gradient norm: 0.0031857907128866134\n",
            "iter: 553  x: [-99.99845328  24.99978908]  f(x): 2.4368379181665612e-06  grad at x: [ 0.00309345 -0.00042183]  gradient norm: 0.0031220748986317166\n",
            "iter: 554  x: [-99.99848421  24.9997933 ]  f(x): 2.340339136585781e-06  grad at x: [ 0.00303158 -0.0004134 ]  gradient norm: 0.003059633400645104\n",
            "iter: 555  x: [-99.99851453  24.99979744]  f(x): 2.247661706756805e-06  grad at x: [ 0.00297095 -0.00040513]  gradient norm: 0.0029984407326187424\n",
            "iter: 556  x: [-99.99854424  24.99980149]  f(x): 2.1586543031757993e-06  grad at x: [ 0.00291153 -0.00039703]  gradient norm: 0.002938471917970835\n",
            "iter: 557  x: [-99.99857335  24.99980546]  f(x): 2.0731715927664803e-06  grad at x: [ 0.0028533  -0.00038909]  gradient norm: 0.0028797024796089477\n",
            "iter: 558  x: [-99.99860188  24.99980935]  f(x): 1.991073997711749e-06  grad at x: [ 0.00279623 -0.0003813 ]  gradient norm: 0.0028221084300301073\n",
            "iter: 559  x: [-99.99862985  24.99981316]  f(x): 1.9122274673997524e-06  grad at x: [ 0.00274031 -0.00037368]  gradient norm: 0.0027656662614276165\n",
            "iter: 560  x: [-99.99865725  24.9998169 ]  f(x): 1.836503259683931e-06  grad at x: [ 0.0026855 -0.0003662]  gradient norm: 0.002710352936194053\n",
            "iter: 561  x: [-99.99868411  24.99982056]  f(x): 1.7637777305832432e-06  grad at x: [ 0.00263179 -0.00035888]  gradient norm: 0.0026561458774572175\n",
            "iter: 562  x: [-99.99871042  24.99982415]  f(x): 1.6939321324411514e-06  grad at x: [ 0.00257915 -0.0003517 ]  gradient norm: 0.002603022959899625\n",
            "iter: 563  x: [-99.99873621  24.99982767]  f(x): 1.6268524199873468e-06  grad at x: [ 0.00252757 -0.00034467]  gradient norm: 0.0025509625006944706\n",
            "iter: 564  x: [-99.99876149  24.99983111]  f(x): 1.5624290641447357e-06  grad at x: [ 0.00247702 -0.00033778]  gradient norm: 0.0024999432506716914\n",
            "iter: 565  x: [-99.99878626  24.99983449]  f(x): 1.5005568731993278e-06  grad at x: [ 0.00242748 -0.00033102]  gradient norm: 0.00244994438565395\n",
            "iter: 566  x: [-99.99881054  24.9998378 ]  f(x): 1.4411348210174612e-06  grad at x: [ 0.00237893 -0.0003244 ]  gradient norm: 0.002400945497938228\n",
            "iter: 567  x: [-99.99883432  24.99984104]  f(x): 1.3840658821005088e-06  grad at x: [ 0.00233135 -0.00031791]  gradient norm: 0.0023529265879755016\n",
            "iter: 568  x: [-99.99885764  24.99984422]  f(x): 1.3292568731809951e-06  grad at x: [ 0.00228472 -0.00031155]  gradient norm: 0.0023058680562261106\n",
            "iter: 569  x: [-99.99888049  24.99984734]  f(x): 1.276618301000786e-06  grad at x: [ 0.00223903 -0.00030532]  gradient norm: 0.0022597506950996042\n",
            "iter: 570  x: [-99.99890288  24.99985039]  f(x): 1.2260642162918204e-06  grad at x: [ 0.00219425 -0.00029922]  gradient norm: 0.0022145556812072443\n",
            "iter: 571  x: [-99.99892482  24.99985338]  f(x): 1.1775120733353944e-06  grad at x: [ 0.00215036 -0.00029323]  gradient norm: 0.0021702645675911445\n",
            "iter: 572  x: [-99.99894632  24.99985632]  f(x): 1.1308825952298019e-06  grad at x: [ 0.00210736 -0.00028737]  gradient norm: 0.0021268592762379007\n",
            "iter: 573  x: [-99.9989674   24.99985919]  f(x): 1.086099644448076e-06  grad at x: [ 0.00206521 -0.00028162]  gradient norm: 0.002084322090702947\n",
            "iter: 574  x: [-99.99898805  24.99986201]  f(x): 1.043090098524285e-06  grad at x: [ 0.00202391 -0.00027599]  gradient norm: 0.0020426356488853167\n",
            "iter: 575  x: [-99.99900829  24.99986477]  f(x): 1.0017837306282451e-06  grad at x: [ 0.00198343 -0.00027047]  gradient norm: 0.0020017829359131275\n",
            "iter: 576  x: [-99.99902812  24.99986747]  f(x): 9.621130949022408e-07  grad at x: [ 0.00194376 -0.00026506]  gradient norm: 0.0019617472772018732\n",
            "iter: 577  x: [-99.99904756  24.99987012]  f(x): 9.240134163418113e-07  grad at x: [ 0.00190488 -0.00025976]  gradient norm: 0.0019225123316554422\n",
            "iter: 578  x: [-99.99906661  24.99987272]  f(x): 8.874224850438289e-07  grad at x: [ 0.00186679 -0.00025456]  gradient norm: 0.0018840620850108193\n",
            "iter: 579  x: [-99.99908528  24.99987526]  f(x): 8.522805546265213e-07  grad at x: [ 0.00182945 -0.00024947]  gradient norm: 0.0018463808433002344\n",
            "iter: 580  x: [-99.99910357  24.99987776]  f(x): 8.185302446540868e-07  grad at x: [ 0.00179286 -0.00024448]  gradient norm: 0.0018094532264240342\n",
            "iter: 581  x: [-99.9991215  24.9998802]  f(x): 7.861164469673511e-07  grad at x: [ 0.001757   -0.00023959]  gradient norm: 0.0017732641618973199\n",
            "iter: 582  x: [-99.99913907  24.9998826 ]  f(x): 7.549862356634956e-07  grad at x: [ 0.00172186 -0.0002348 ]  gradient norm: 0.0017377988786548295\n",
            "iter: 583  x: [-99.99915629  24.99988495]  f(x): 7.250887807331395e-07  grad at x: [ 0.00168743 -0.0002301 ]  gradient norm: 0.0017030429010839857\n",
            "iter: 584  x: [-99.99917316  24.99988725]  f(x): 6.963752650132713e-07  grad at x: [ 0.00165368 -0.0002255 ]  gradient norm: 0.0016689820430589076\n",
            "iter: 585  x: [-99.9991897  24.9998895]  f(x): 6.687988045227184e-07  grad at x: [ 0.0016206  -0.00022099]  gradient norm: 0.001635602402202587\n",
            "iter: 586  x: [-99.9992059   24.99989171]  f(x): 6.423143718583405e-07  grad at x: [ 0.00158819 -0.00021657]  gradient norm: 0.0016028903541519496\n",
            "iter: 587  x: [-99.99922179  24.99989388]  f(x): 6.168787227378474e-07  grad at x: [ 0.00155643 -0.00021224]  gradient norm: 0.0015708325470754003\n",
            "iter: 588  x: [-99.99923735  24.999896  ]  f(x): 5.924503253227245e-07  grad at x: [ 0.0015253 -0.000208 ]  gradient norm: 0.0015394158961407726\n",
            "iter: 589  x: [-99.9992526   24.99989808]  f(x): 5.689892924321332e-07  grad at x: [ 0.00149479 -0.00020384]  gradient norm: 0.0015086275782076016\n",
            "iter: 590  x: [-99.99926755  24.99990012]  f(x): 5.464573164433897e-07  grad at x: [ 0.0014649  -0.00019976]  gradient norm: 0.0014784550266320443\n",
            "iter: 591  x: [-99.9992822   24.99990212]  f(x): 5.248176067168078e-07  grad at x: [ 0.0014356  -0.00019576]  gradient norm: 0.0014488859261057206\n",
            "iter: 592  x: [-99.99929656  24.99990408]  f(x): 5.040348294896226e-07  grad at x: [ 0.00140689 -0.00019185]  gradient norm: 0.0014199082075819164\n",
            "iter: 593  x: [-99.99931062  24.99990599]  f(x): 4.840750502362673e-07  grad at x: [ 0.00137875 -0.00018801]  gradient norm: 0.0013915100434222778\n",
            "iter: 594  x: [-99.99932441  24.99990787]  f(x): 4.6490567824575903e-07  grad at x: [ 0.00135118 -0.00018425]  gradient norm: 0.0013636798425521424\n",
            "iter: 595  x: [-99.99933792  24.99990972]  f(x): 4.4649541338544787e-07  grad at x: [ 0.00132415 -0.00018057]  gradient norm: 0.0013364062456984371\n",
            "iter: 596  x: [-99.99935117  24.99991152]  f(x): 4.288141950067337e-07  grad at x: [ 0.00129767 -0.00017695]  gradient norm: 0.0013096781207712583\n",
            "iter: 597  x: [-99.99936414  24.99991329]  f(x): 4.118331528865371e-07  grad at x: [ 0.00127172 -0.00017342]  gradient norm: 0.0012834845583590587\n",
            "iter: 598  x: [-99.99937686  24.99991503]  f(x): 3.9552456003703224e-07  grad at x: [ 0.00124628 -0.00016995]  gradient norm: 0.0012578148671995133\n",
            "iter: 599  x: [-99.99938932  24.99991673]  f(x): 3.798617874540353e-07  grad at x: [ 0.00122136 -0.00016655]  gradient norm: 0.0012326585698465497\n",
            "iter: 600  x: [-99.99940154  24.99991839]  f(x): 3.648192606722627e-07  grad at x: [ 0.00119693 -0.00016322]  gradient norm: 0.0012080053984519486\n",
            "iter: 601  x: [-99.99941351  24.99992002]  f(x): 3.503724179559072e-07  grad at x: [ 0.00117299 -0.00015995]  gradient norm: 0.0011838452904934956\n",
            "iter: 602  x: [-99.99942524  24.99992162]  f(x): 3.364976701987273e-07  grad at x: [ 0.00114953 -0.00015675]  gradient norm: 0.0011601683846730652\n",
            "iter: 603  x: [-99.99943673  24.99992319]  f(x): 3.231723624622597e-07  grad at x: [ 0.00112654 -0.00015362]  gradient norm: 0.0011369650169855883\n",
            "iter: 604  x: [-99.999448    24.99992473]  f(x): 3.103747369094994e-07  grad at x: [ 0.00110401 -0.00015055]  gradient norm: 0.001114225716647214\n",
            "iter: 605  x: [-99.99945904  24.99992623]  f(x): 2.9808389732391713e-07  grad at x: [ 0.00108193 -0.00014754]  gradient norm: 0.0010919412023070055\n",
            "iter: 606  x: [-99.99946986  24.99992771]  f(x): 2.862797749907496e-07  grad at x: [ 0.00106029 -0.00014458]  gradient norm: 0.0010701023782624718\n",
            "iter: 607  x: [-99.99948046  24.99992915]  f(x): 2.7494309590277375e-07  grad at x: [ 0.00103908 -0.00014169]  gradient norm: 0.001048700330700384\n",
            "iter: 608  x: [-99.99949085  24.99993057]  f(x): 2.640553493033893e-07  grad at x: [ 0.0010183  -0.00013886]  gradient norm: 0.0010277263240831955\n",
            "iter: 609  x: [-99.99950103  24.99993196]  f(x): 2.535987574670333e-07  grad at x: [ 0.00099794 -0.00013608]  gradient norm: 0.001007171797593704\n",
            "iter: 610  x: [-99.99951101  24.99993332]  f(x): 2.435562466665851e-07  grad at x: [ 0.00097798 -0.00013336]  gradient norm: 0.0009870283616321977\n",
            "iter: 611  x: [-99.99952079  24.99993465]  f(x): 2.3391141930112656e-07  grad at x: [ 0.00095842 -0.00013069]  gradient norm: 0.0009672877944048019\n",
            "iter: 612  x: [-99.99953038  24.99993596]  f(x): 2.2464852709449038e-07  grad at x: [ 0.00093925 -0.00012808]  gradient norm: 0.0009479420385118288\n",
            "iter: 613  x: [-99.99953977  24.99993724]  f(x): 2.1575244541678308e-07  grad at x: [ 0.00092046 -0.00012552]  gradient norm: 0.0009289831977313327\n",
            "iter: 614  x: [-99.99954897  24.9999385 ]  f(x): 2.072086485794613e-07  grad at x: [ 0.00090206 -0.00012301]  gradient norm: 0.0009104035337793045\n",
            "iter: 615  x: [-99.99955799  24.99993973]  f(x): 1.9900318609317644e-07  grad at x: [ 0.00088401 -0.00012055]  gradient norm: 0.0008921954630980286\n",
            "iter: 616  x: [-99.99956683  24.99994093]  f(x): 1.9112265992712956e-07  grad at x: [ 0.00086633 -0.00011814]  gradient norm: 0.0008743515538434859\n",
            "iter: 617  x: [-99.9995755   24.99994211]  f(x): 1.8355420259081797e-07  grad at x: [ 0.00084901 -0.00011577]  gradient norm: 0.0008568645227591535\n",
            "iter: 618  x: [-99.99958399  24.99994327]  f(x): 1.762854561687268e-07  grad at x: [ 0.00083203 -0.00011346]  gradient norm: 0.0008397272323051738\n",
            "iter: 619  x: [-99.99959231  24.99994441]  f(x): 1.6930455210054764e-07  grad at x: [ 0.00081539 -0.00011119]  gradient norm: 0.0008229326876495978\n",
            "iter: 620  x: [-99.99960046  24.99994552]  f(x): 1.6260009183265337e-07  grad at x: [ 0.00079908 -0.00010897]  gradient norm: 0.000806474033884919\n",
            "iter: 621  x: [-99.99960845  24.99994661]  f(x): 1.561611281997249e-07  grad at x: [ 0.0007831  -0.00010679]  gradient norm: 0.0007903445532164435\n",
            "iter: 622  x: [-99.99961628  24.99994767]  f(x): 1.4997714752245312e-07  grad at x: [ 0.00076744 -0.00010465]  gradient norm: 0.0007745376621506616\n",
            "iter: 623  x: [-99.99962396  24.99994872]  f(x): 1.44038052483593e-07  grad at x: [ 0.00075209 -0.00010256]  gradient norm: 0.0007590469089156296\n",
            "iter: 624  x: [-99.99963148  24.99994975]  f(x): 1.3833414560394298e-07  grad at x: [ 0.00073704 -0.00010051]  gradient norm: 0.0007438659707338224\n",
            "iter: 625  x: [-99.99963885  24.99995075]  f(x): 1.3285611344123918e-07  grad at x: [ 7.22303948e-04 -9.84959929e-05]  gradient norm: 0.0007289886513279591\n",
            "iter: 626  x: [-99.99964607  24.99995174]  f(x): 1.2759501135181703e-07  grad at x: [ 7.07857869e-04 -9.65260731e-05]  gradient norm: 0.0007144088783093811\n",
            "iter: 627  x: [-99.99965315  24.9999527 ]  f(x): 1.225422489015726e-07  grad at x: [ 6.93700712e-04 -9.45955516e-05]  gradient norm: 0.0007001207007411582\n",
            "iter: 628  x: [-99.99966009  24.99995365]  f(x): 1.1768957584577731e-07  grad at x: [ 6.79826697e-04 -9.27036406e-05]  gradient norm: 0.0006861182867283959\n",
            "iter: 629  x: [-99.99966688  24.99995458]  f(x): 1.1302906864690865e-07  grad at x: [ 6.66230163e-04 -9.08495677e-05]  gradient norm: 0.0006723959210075822\n",
            "iter: 630  x: [-99.99967355  24.99995548]  f(x): 1.0855311753134626e-07  grad at x: [ 6.52905560e-04 -8.90325764e-05]  gradient norm: 0.0006589480025960964\n",
            "iter: 631  x: [-99.99968008  24.99995637]  f(x): 1.042544140800849e-07  grad at x: [ 6.39847449e-04 -8.72519249e-05]  gradient norm: 0.0006457690425534035\n",
            "iter: 632  x: [-99.99968647  24.99995725]  f(x): 1.0012593928683939e-07  grad at x: [ 6.27050500e-04 -8.55068864e-05]  gradient norm: 0.0006328536617160065\n",
            "iter: 633  x: [-99.99969275  24.9999581 ]  f(x): 9.616095209296403e-08  grad at x: [ 6.14509490e-04 -8.37967486e-05]  gradient norm: 0.0006201965884877602\n",
            "iter: 634  x: [-99.99969889  24.99995894]  f(x): 9.235297838965282e-08  grad at x: [ 6.02219300e-04 -8.21208137e-05]  gradient norm: 0.0006077926567165905\n",
            "iter: 635  x: [-99.99970491  24.99995976]  f(x): 8.869580044327627e-08  grad at x: [ 5.90174914e-04 -8.04783974e-05]  gradient norm: 0.0005956368035750521\n",
            "iter: 636  x: [-99.99971081  24.99996057]  f(x): 8.518344674219017e-08  grad at x: [ 5.78371416e-04 -7.88688294e-05]  gradient norm: 0.0005837240674914481\n",
            "iter: 637  x: [-99.9997166   24.99996135]  f(x): 8.1810182251604e-08  grad at x: [ 5.66803988e-04 -7.72914529e-05]  gradient norm: 0.0005720495861430336\n",
            "iter: 638  x: [-99.99972227  24.99996213]  f(x): 7.857049903441896e-08  grad at x: [ 5.55467908e-04 -7.57456238e-05]  gradient norm: 0.0005606085944200961\n",
            "iter: 639  x: [-99.99972782  24.99996288]  f(x): 7.545910727294957e-08  grad at x: [ 5.44358550e-04 -7.42307113e-05]  gradient norm: 0.0005493964225327631\n",
            "iter: 640  x: [-99.99973326  24.99996363]  f(x): 7.2470926626712e-08  grad at x: [ 5.33471379e-04 -7.27460971e-05]  gradient norm: 0.0005384084940886873\n",
            "iter: 641  x: [-99.9997386   24.99996435]  f(x): 6.96010779301785e-08  grad at x: [ 5.22801951e-04 -7.12911752e-05]  gradient norm: 0.000527640324198894\n",
            "iter: 642  x: [-99.99974383  24.99996507]  f(x): 6.684487524056421e-08  grad at x: [ 5.12345912e-04 -6.98653516e-05]  gradient norm: 0.0005170875177010724\n",
            "iter: 643  x: [-99.99974895  24.99996577]  f(x): 6.419781818224761e-08  grad at x: [ 5.02098994e-04 -6.84680446e-05]  gradient norm: 0.0005067457673518255\n",
            "iter: 644  x: [-99.99975397  24.99996645]  f(x): 6.165558458550121e-08  grad at x: [ 4.92057014e-04 -6.70986837e-05]  gradient norm: 0.0004966108520179607\n",
            "iter: 645  x: [-99.99975889  24.99996712]  f(x): 5.921402343847111e-08  grad at x: [ 4.82215874e-04 -6.57567101e-05]  gradient norm: 0.0004866786349881043\n",
            "iter: 646  x: [-99.99976371  24.99996778]  f(x): 5.6869148107287144e-08  grad at x: [ 4.72571556e-04 -6.44415759e-05]  gradient norm: 0.00047694506227567615\n",
            "iter: 647  x: [-99.99976844  24.99996842]  f(x): 5.46171298450491e-08  grad at x: [ 4.63120125e-04 -6.31527443e-05]  gradient norm: 0.00046740616104218866\n",
            "iter: 648  x: [-99.99977307  24.99996906]  f(x): 5.2454291503660095e-08  grad at x: [ 4.53857723e-04 -6.18896894e-05]  gradient norm: 0.0004580580378234186\n",
            "iter: 649  x: [-99.99977761  24.99996967]  f(x): 5.0377101558463146e-08  grad at x: [ 4.44780568e-04 -6.06518957e-05]  gradient norm: 0.00044889687705958995\n",
            "iter: 650  x: [-99.99978206  24.99997028]  f(x): 4.838216833765322e-08  grad at x: [ 4.35884957e-04 -5.94388577e-05]  gradient norm: 0.00043991893952251347\n",
            "iter: 651  x: [-99.99978642  24.99997087]  f(x): 4.646623447233201e-08  grad at x: [ 4.27167258e-04 -5.82500806e-05]  gradient norm: 0.00043112056073600576\n",
            "iter: 652  x: [-99.99979069  24.99997146]  f(x): 4.46261715856363e-08  grad at x: [ 4.18623912e-04 -5.70850790e-05]  gradient norm: 0.00042249814951375255\n",
            "iter: 653  x: [-99.99979487  24.99997203]  f(x): 4.285897519103061e-08  grad at x: [ 4.10251434e-04 -5.59433774e-05]  gradient norm: 0.00041404818652437356\n",
            "iter: 654  x: [-99.99979898  24.99997259]  f(x): 4.116175977473963e-08  grad at x: [ 4.02046406e-04 -5.48245098e-05]  gradient norm: 0.00040576722280016473\n",
            "iter: 655  x: [-99.999803    24.99997314]  f(x): 3.953175408548136e-08  grad at x: [ 3.94005477e-04 -5.37280197e-05]  gradient norm: 0.0003976518783332042\n",
            "iter: 656  x: [-99.99980694  24.99997367]  f(x): 3.796629662618298e-08  grad at x: [ 3.86125368e-04 -5.26534593e-05]  gradient norm: 0.0003896988407793022\n",
            "iter: 657  x: [-99.9998108  24.9999742]  f(x): 3.646283128164746e-08  grad at x: [ 3.78402861e-04 -5.16003901e-05]  gradient norm: 0.0003819048639734637\n",
            "iter: 658  x: [-99.99981458  24.99997472]  f(x): 3.501890316235646e-08  grad at x: [ 3.70834803e-04 -5.05683823e-05]  gradient norm: 0.0003742667666911208\n",
            "iter: 659  x: [-99.99981829  24.99997522]  f(x): 3.3632154597579034e-08  grad at x: [ 3.63418107e-04 -4.95570146e-05]  gradient norm: 0.00036678143135976247\n",
            "iter: 660  x: [-99.99982193  24.99997572]  f(x): 3.2300321276592704e-08  grad at x: [ 3.56149745e-04 -4.85658743e-05]  gradient norm: 0.0003594458027385642\n",
            "iter: 661  x: [-99.99982549  24.9999762 ]  f(x): 3.1021228554319196e-08  grad at x: [ 3.49026750e-04 -4.75945569e-05]  gradient norm: 0.0003522568866853802\n",
            "iter: 662  x: [-99.99982898  24.99997668]  f(x): 2.9792787903441112e-08  grad at x: [ 3.42046215e-04 -4.66426657e-05]  gradient norm: 0.00034521174895093653\n",
            "iter: 663  x: [-99.9998324   24.99997715]  f(x): 2.8612993503301714e-08  grad at x: [ 3.35205291e-04 -4.57098124e-05]  gradient norm: 0.00033830751397686523\n",
            "iter: 664  x: [-99.99983575  24.9999776 ]  f(x): 2.7479918958861736e-08  grad at x: [ 3.28501185e-04 -4.47956162e-05]  gradient norm: 0.0003315413636870171\n",
            "iter: 665  x: [-99.99983903  24.99997805]  f(x): 2.6391714166099684e-08  grad at x: [ 3.21931161e-04 -4.38997038e-05]  gradient norm: 0.0003249105364010203\n",
            "iter: 666  x: [-99.99984225  24.99997849]  f(x): 2.5346602286847235e-08  grad at x: [ 3.15492538e-04 -4.30217098e-05]  gradient norm: 0.00031841232568383555\n",
            "iter: 667  x: [-99.99984541  24.99997892]  f(x): 2.4342876836567685e-08  grad at x: [ 3.09182687e-04 -4.21612756e-05]  gradient norm: 0.0003120440791719509\n",
            "iter: 668  x: [-99.9998485   24.99997934]  f(x): 2.3378898915239016e-08  grad at x: [ 3.02999034e-04 -4.13180500e-05]  gradient norm: 0.0003058031975976642\n",
            "iter: 669  x: [-99.99985153  24.99997975]  f(x): 2.2453094517567382e-08  grad at x: [ 2.96939053e-04 -4.04916890e-05]  gradient norm: 0.0002996871336415188\n",
            "iter: 670  x: [-99.9998545   24.99998016]  f(x): 2.1563951972894448e-08  grad at x: [ 2.91000272e-04 -3.96818553e-05]  gradient norm: 0.0002936933909565855\n",
            "iter: 671  x: [-99.99985741  24.99998056]  f(x): 2.071001947434322e-08  grad at x: [ 2.85180266e-04 -3.88882182e-05]  gradient norm: 0.0002878195231345033\n",
            "iter: 672  x: [-99.99986026  24.99998094]  f(x): 1.9889902703794686e-08  grad at x: [ 2.79476661e-04 -3.81104538e-05]  gradient norm: 0.00028206313267631903\n",
            "iter: 673  x: [-99.99986306  24.99998133]  f(x): 1.9102262555626646e-08  grad at x: [ 2.73887128e-04 -3.73482447e-05]  gradient norm: 0.00027642187001484993\n",
            "iter: 674  x: [-99.9998658  24.9999817]  f(x): 1.8345812959888878e-08  grad at x: [ 2.68409385e-04 -3.66012798e-05]  gradient norm: 0.0002708934326253693\n",
            "iter: 675  x: [-99.99986848  24.99998207]  f(x): 1.7619318765687545e-08  grad at x: [ 2.63041198e-04 -3.58692542e-05]  gradient norm: 0.0002654755639654056\n",
            "iter: 676  x: [-99.99987111  24.99998242]  f(x): 1.692159374424893e-08  grad at x: [ 2.57780374e-04 -3.51518691e-05]  gradient norm: 0.0002601660526990324\n",
            "iter: 677  x: [-99.99987369  24.99998278]  f(x): 1.6251498631030216e-08  grad at x: [ 2.52624766e-04 -3.44488318e-05]  gradient norm: 0.0002549627316376275\n",
            "iter: 678  x: [-99.99987621  24.99998312]  f(x): 1.5607939286951754e-08  grad at x: [ 2.47572271e-04 -3.37598551e-05]  gradient norm: 0.0002498634770185651\n",
            "iter: 679  x: [-99.99987869  24.99998346]  f(x): 1.498986489116182e-08  grad at x: [ 2.42620826e-04 -3.30846580e-05]  gradient norm: 0.0002448662074779762\n",
            "iter: 680  x: [-99.99988112  24.99998379]  f(x): 1.439626624159468e-08  grad at x: [ 2.37768409e-04 -3.24229649e-05]  gradient norm: 0.0002399688833294407\n",
            "iter: 681  x: [-99.99988349  24.99998411]  f(x): 1.3826174099930416e-08  grad at x: [ 2.33013041e-04 -3.17745056e-05]  gradient norm: 0.00023516950567563317\n",
            "iter: 682  x: [-99.99988582  24.99998443]  f(x): 1.3278657604292837e-08  grad at x: [ 2.28352780e-04 -3.11390155e-05]  gradient norm: 0.00023046611555100968\n",
            "iter: 683  x: [-99.99988811  24.99998474]  f(x): 1.2752822761775123e-08  grad at x: [ 2.23785724e-04 -3.05162351e-05]  gradient norm: 0.00022585679322770103\n",
            "iter: 684  x: [-99.99989034  24.99998505]  f(x): 1.2247810980073796e-08  grad at x: [ 2.19310010e-04 -2.99059104e-05]  gradient norm: 0.00022133965736011968\n",
            "iter: 685  x: [-99.99989254  24.99998535]  f(x): 1.1762797664761005e-08  grad at x: [ 2.14923810e-04 -2.93077922e-05]  gradient norm: 0.0002169128642082899\n",
            "iter: 686  x: [-99.99989469  24.99998564]  f(x): 1.1296990876736474e-08  grad at x: [ 2.10625333e-04 -2.87216364e-05]  gradient norm: 0.00021257460691941993\n",
            "iter: 687  x: [-99.99989679  24.99998593]  f(x): 1.0849630038347038e-08  grad at x: [ 2.06412827e-04 -2.81472037e-05]  gradient norm: 0.00020832311478419325\n",
            "iter: 688  x: [-99.99989886  24.99998621]  f(x): 1.0419984688999014e-08  grad at x: [ 2.02284570e-04 -2.75842596e-05]  gradient norm: 0.00020415665249017985\n",
            "iter: 689  x: [-99.99990088  24.99998648]  f(x): 1.0007353294721772e-08  grad at x: [ 1.98238879e-04 -2.70325744e-05]  gradient norm: 0.00020007351943444963\n",
            "iter: 690  x: [-99.99990286  24.99998675]  f(x): 9.611062102935645e-09  grad at x: [ 1.94274101e-04 -2.64919229e-05]  gradient norm: 0.00019607204903234572\n",
            "iter: 691  x: [-99.99990481  24.99998702]  f(x): 9.230464044566378e-09  grad at x: [ 1.90388619e-04 -2.59620845e-05]  gradient norm: 0.00019215060806113915\n",
            "iter: 692  x: [-99.99990671  24.99998728]  f(x): 8.864937669584468e-09  grad at x: [ 1.86580847e-04 -2.54428428e-05]  gradient norm: 0.00018830759591248006\n",
            "iter: 693  x: [-99.99990858  24.99998753]  f(x): 8.513886137431912e-09  grad at x: [ 1.82849230e-04 -2.49339859e-05]  gradient norm: 0.00018454144398949427\n",
            "iter: 694  x: [-99.9999104   24.99998778]  f(x): 8.176736246036576e-09  grad at x: [ 1.79192245e-04 -2.44353062e-05]  gradient norm: 0.00018085061510580026\n",
            "iter: 695  x: [-99.9999122   24.99998803]  f(x): 7.852937490030506e-09  grad at x: [ 1.75608400e-04 -2.39466001e-05]  gradient norm: 0.00017723360279620233\n",
            "iter: 696  x: [-99.99991395  24.99998827]  f(x): 7.541961165570369e-09  grad at x: [ 1.72096232e-04 -2.34676681e-05]  gradient norm: 0.00017368893074194876\n",
            "iter: 697  x: [-99.99991567  24.9999885 ]  f(x): 7.243299502330899e-09  grad at x: [ 1.68654308e-04 -2.29983147e-05]  gradient norm: 0.0001702151521143861\n",
            "iter: 698  x: [-99.99991736  24.99998873]  f(x): 6.956464842929534e-09  grad at x: [ 1.65281222e-04 -2.25383484e-05]  gradient norm: 0.0001668108490827804\n",
            "iter: 699  x: [-99.99991901  24.99998896]  f(x): 6.6809888351924225e-09  grad at x: [ 1.61975597e-04 -2.20875814e-05]  gradient norm: 0.0001634746321016496\n",
            "iter: 700  x: [-99.99992063  24.99998918]  f(x): 6.416421676441613e-09  grad at x: [ 1.58736085e-04 -2.16458298e-05]  gradient norm: 0.0001602051394486658\n",
            "iter: 701  x: [-99.99992222  24.99998939]  f(x): 6.162331377512914e-09  grad at x: [ 1.55561364e-04 -2.12129132e-05]  gradient norm: 0.000157001036652793\n",
            "iter: 702  x: [-99.99992377  24.99998961]  f(x): 5.9183030548009185e-09  grad at x: [ 1.52450136e-04 -2.07886550e-05]  gradient norm: 0.00015386101591762506\n",
            "iter: 703  x: [-99.9999253   24.99998981]  f(x): 5.683938254615873e-09  grad at x: [ 1.49401134e-04 -2.03728819e-05]  gradient norm: 0.00015078379560968575\n",
            "iter: 704  x: [-99.99992679  24.99999002]  f(x): 5.4588543007228146e-09  grad at x: [ 1.46413111e-04 -1.99654242e-05]  gradient norm: 0.00014776811971088777\n",
            "iter: 705  x: [-99.99992826  24.99999022]  f(x): 5.242683671323882e-09  grad at x: [ 1.43484849e-04 -1.95661157e-05]  gradient norm: 0.0001448127573292337\n",
            "iter: 706  x: [-99.99992969  24.99999041]  f(x): 5.035073398120208e-09  grad at x: [ 1.40615152e-04 -1.91747934e-05]  gradient norm: 0.00014191650218519633\n",
            "iter: 707  x: [-99.9999311  24.9999906]  f(x): 4.835684492320164e-09  grad at x: [ 1.37802849e-04 -1.87912976e-05]  gradient norm: 0.00013907817215250083\n",
            "iter: 708  x: [-99.99993248  24.99999079]  f(x): 4.644191386566911e-09  grad at x: [ 1.35046792e-04 -1.84154716e-05]  gradient norm: 0.00013629660871154368\n",
            "iter: 709  x: [-99.99993383  24.99999098]  f(x): 4.460281408400474e-09  grad at x: [ 1.32345856e-04 -1.80471622e-05]  gradient norm: 0.00013357067654841723\n",
            "iter: 710  x: [-99.99993515  24.99999116]  f(x): 4.28365426415656e-09  grad at x: [ 1.29698939e-04 -1.76862189e-05]  gradient norm: 0.0001308992630102486\n",
            "iter: 711  x: [-99.99993645  24.99999133]  f(x): 4.114021556031195e-09  grad at x: [ 1.27104960e-04 -1.73324946e-05]  gradient norm: 0.00012828127776150648\n",
            "iter: 712  x: [-99.99993772  24.99999151]  f(x): 3.95110630266018e-09  grad at x: [ 1.24562861e-04 -1.69858447e-05]  gradient norm: 0.00012571565221021892\n",
            "iter: 713  x: [-99.99993896  24.99999168]  f(x): 3.794642492849714e-09  grad at x: [ 1.22071604e-04 -1.66461278e-05]  gradient norm: 0.00012320133916235998\n",
            "iter: 714  x: [-99.99994018  24.99999184]  f(x): 3.644374650895955e-09  grad at x: [ 1.19630172e-04 -1.63132052e-05]  gradient norm: 0.00012073731239175327\n",
            "iter: 715  x: [-99.99994138  24.99999201]  f(x): 3.5000574152963968e-09  grad at x: [ 1.17237568e-04 -1.59869411e-05]  gradient norm: 0.00011832256615365298\n",
            "iter: 716  x: [-99.99994255  24.99999217]  f(x): 3.3614551422577365e-09  grad at x: [ 1.14892817e-04 -1.56672023e-05]  gradient norm: 0.00011595611484105073\n",
            "iter: 717  x: [-99.9999437   24.99999232]  f(x): 3.2283415191032603e-09  grad at x: [ 1.12594960e-04 -1.53538582e-05]  gradient norm: 0.00011363699255265884\n",
            "iter: 718  x: [-99.99994483  24.99999248]  f(x): 3.100499194408282e-09  grad at x: [ 1.10343061e-04 -1.50467811e-05]  gradient norm: 0.0001113642526919349\n",
            "iter: 719  x: [-99.99994593  24.99999263]  f(x): 2.9777194269914536e-09  grad at x: [ 1.08136200e-04 -1.47458454e-05]  gradient norm: 0.00010913696765058948\n",
            "iter: 720  x: [-99.99994701  24.99999277]  f(x): 2.859801737594287e-09  grad at x: [ 1.05973476e-04 -1.44509285e-05]  gradient norm: 0.00010695422829592643\n",
            "iter: 721  x: [-99.99994807  24.99999292]  f(x): 2.74655358909716e-09  grad at x: [ 1.03854006e-04 -1.41619100e-05]  gradient norm: 0.00010481514373595373\n",
            "iter: 722  x: [-99.99994911  24.99999306]  f(x): 2.637790067160552e-09  grad at x: [ 1.01776926e-04 -1.38786718e-05]  gradient norm: 0.000102718840864966\n",
            "iter: 723  x: [-99.99995013  24.9999932 ]  f(x): 2.533333580421748e-09  grad at x: [ 9.97413878e-05 -1.36010983e-05]  gradient norm: 0.00010066446404609221\n",
            "iter: 724  x: [-99.99995113  24.99999334]  f(x): 2.433013570709657e-09  grad at x: [ 9.77465601e-05 -1.33290764e-05]  gradient norm: 9.865117476664243e-05\n",
            "iter: 725  x: [-99.9999521   24.99999347]  f(x): 2.3366662330187356e-09  grad at x: [ 9.57916289e-05 -1.30624948e-05]  gradient norm: 9.667815126529335e-05\n",
            "iter: 726  x: [-99.99995306  24.9999936 ]  f(x): 2.2441342505516917e-09  grad at x: [ 9.38757963e-05 -1.28012449e-05]  gradient norm: 9.474458824759737e-05\n",
            "iter: 727  x: [-99.999954    24.99999373]  f(x): 2.155266534161207e-09  grad at x: [ 9.19982804e-05 -1.25452200e-05]  gradient norm: 9.284969648116696e-05\n",
            "iter: 728  x: [-99.99995492  24.99999385]  f(x): 2.0699179788534196e-09  grad at x: [ 9.01583147e-05 -1.22943156e-05]  gradient norm: 9.099270253934476e-05\n",
            "iter: 729  x: [-99.99995582  24.99999398]  f(x): 1.9879492267193202e-09  grad at x: [ 8.83551484e-05 -1.20484293e-05]  gradient norm: 8.917284848471132e-05\n",
            "iter: 730  x: [-99.99995671  24.9999941 ]  f(x): 1.909226437794559e-09  grad at x: [ 8.65880455e-05 -1.18074607e-05]  gradient norm: 8.738939152539189e-05\n",
            "iter: 731  x: [-99.99995757  24.99999421]  f(x): 1.8336210711388097e-09  grad at x: [ 8.48562846e-05 -1.15713115e-05]  gradient norm: 8.56416037014443e-05\n",
            "iter: 732  x: [-99.99995842  24.99999433]  f(x): 1.7610096765267219e-09  grad at x: [ 8.31591589e-05 -1.13398853e-05]  gradient norm: 8.392877162276884e-05\n",
            "iter: 733  x: [-99.99995925  24.99999444]  f(x): 1.6912736930625247e-09  grad at x: [ 8.14959757e-05 -1.11130876e-05]  gradient norm: 8.22501961836572e-05\n",
            "iter: 734  x: [-99.99996007  24.99999455]  f(x): 1.6242992544205268e-09  grad at x: [ 7.98660562e-05 -1.08908258e-05]  gradient norm: 8.060519225014048e-05\n",
            "iter: 735  x: [-99.99996087  24.99999466]  f(x): 1.5599770036974894e-09  grad at x: [ 7.82687351e-05 -1.06730093e-05]  gradient norm: 7.899308839885903e-05\n",
            "iter: 736  x: [-99.99996165  24.99999477]  f(x): 1.4982019142678308e-09  grad at x: [ 7.67033603e-05 -1.04595491e-05]  gradient norm: 7.741322662873136e-05\n",
            "iter: 737  x: [-99.99996242  24.99999487]  f(x): 1.4388731189296852e-09  grad at x: [ 7.51692932e-05 -1.02503582e-05]  gradient norm: 7.586496210846441e-05\n",
            "iter: 738  x: [-99.99996317  24.99999498]  f(x): 1.3818937430098929e-09  grad at x: [ 7.36659073e-05 -1.00453510e-05]  gradient norm: 7.43476628552611e-05\n",
            "iter: 739  x: [-99.9999639   24.99999508]  f(x): 1.3271707506358443e-09  grad at x: [ 7.21925891e-05 -9.84444398e-06]  gradient norm: 7.286070959401491e-05\n",
            "iter: 740  x: [-99.99996463  24.99999518]  f(x): 1.2746147892863187e-09  grad at x: [ 7.07487374e-05 -9.64755510e-06]  gradient norm: 7.140349541265662e-05\n",
            "iter: 741  x: [-99.99996533  24.99999527]  f(x): 1.2241400436467035e-09  grad at x: [ 6.93337626e-05 -9.45460400e-06]  gradient norm: 6.99754255048643e-05\n",
            "iter: 742  x: [-99.99996603  24.99999537]  f(x): 1.175664097970743e-09  grad at x: [ 6.79470874e-05 -9.26551193e-06]  gradient norm: 6.857591699629669e-05\n",
            "iter: 743  x: [-99.99996671  24.99999546]  f(x): 1.1291077997519643e-09  grad at x: [ 6.65881456e-05 -9.08020169e-06]  gradient norm: 6.720439865818202e-05\n",
            "iter: 744  x: [-99.99996737  24.99999555]  f(x): 1.0843951307201326e-09  grad at x: [ 6.52563827e-05 -8.89859765e-06]  gradient norm: 6.586031068010939e-05\n",
            "iter: 745  x: [-99.99996802  24.99999564]  f(x): 1.0414530835330816e-09  grad at x: [ 6.39512550e-05 -8.72062569e-06]  gradient norm: 6.454310446618079e-05\n",
            "iter: 746  x: [-99.99996866  24.99999573]  f(x): 1.0002115414828598e-09  grad at x: [ 6.26722299e-05 -8.54621318e-06]  gradient norm: 6.325224237868124e-05\n",
            "iter: 747  x: [-99.99996929  24.99999581]  f(x): 9.606031645666964e-10  grad at x: [ 6.14187854e-05 -8.37528891e-06]  gradient norm: 6.198719753519098e-05\n",
            "iter: 748  x: [-99.9999699  24.9999959]  f(x): 9.225632791845372e-10  grad at x: [ 6.01904096e-05 -8.20778313e-06]  gradient norm: 6.0747453582336675e-05\n",
            "iter: 749  x: [-99.99997051  24.99999598]  f(x): 8.860297736376485e-10  grad at x: [ 5.89866015e-05 -8.04362747e-06]  gradient norm: 5.953250452106474e-05\n",
            "iter: 750  x: [-99.9999711   24.99999606]  f(x): 8.5094299432584e-10  grad at x: [ 5.78068694e-05 -7.88275491e-06]  gradient norm: 5.834185442119028e-05\n",
            "iter: 751  x: [-99.99997167  24.99999614]  f(x): 8.172456517867642e-10  grad at x: [ 5.66507320e-05 -7.72509981e-06]  gradient norm: 5.7175017334033725e-05\n",
            "iter: 752  x: [-99.99997224  24.99999621]  f(x): 7.848827237366325e-10  grad at x: [ 5.55177174e-05 -7.57059782e-06]  gradient norm: 5.603151697880872e-05\n",
            "iter: 753  x: [-99.9999728   24.99999629]  f(x): 7.538013675171354e-10  grad at x: [ 5.44073630e-05 -7.41918586e-06]  gradient norm: 5.491088662613764e-05\n",
            "iter: 754  x: [-99.99997334  24.99999636]  f(x): 7.239508334309621e-10  grad at x: [ 5.33192158e-05 -7.27080214e-06]  gradient norm: 5.3812668896123785e-05\n",
            "iter: 755  x: [-99.99997387  24.99999644]  f(x): 6.952823800764063e-10  grad at x: [ 5.22528314e-05 -7.12538610e-06]  gradient norm: 5.2736415504901595e-05\n",
            "iter: 756  x: [-99.9999744   24.99999651]  f(x): 6.677491980256653e-10  grad at x: [ 5.12077748e-05 -6.98287838e-06]  gradient norm: 5.168168720255426e-05\n",
            "iter: 757  x: [-99.99997491  24.99999658]  f(x): 6.413063298334454e-10  grad at x: [ 5.01836193e-05 -6.84322081e-06]  gradient norm: 5.064805346046166e-05\n",
            "iter: 758  x: [-99.99997541  24.99999665]  f(x): 6.159105989666628e-10  grad at x: [ 4.91799469e-05 -6.70635639e-06]  gradient norm: 4.9635092382976894e-05\n",
            "iter: 759  x: [-99.9999759   24.99999671]  f(x): 5.915205392773144e-10  grad at x: [ 4.81963480e-05 -6.57222927e-06]  gradient norm: 4.8642390536539806e-05\n",
            "iter: 760  x: [-99.99997638  24.99999678]  f(x): 5.680963257965379e-10  grad at x: [ 4.72324210e-05 -6.44078468e-06]  gradient norm: 4.766954272054801e-05\n",
            "iter: 761  x: [-99.99997686  24.99999684]  f(x): 5.455997109832929e-10  grad at x: [ 4.62877726e-05 -6.31196899e-06]  gradient norm: 4.671615185279254e-05\n",
            "iter: 762  x: [-99.99997732  24.99999691]  f(x): 5.239939626141259e-10  grad at x: [ 4.53620171e-05 -6.18572961e-06]  gradient norm: 4.5781828823852196e-05\n",
            "iter: 763  x: [-99.99997777  24.99999697]  f(x): 5.032438015678278e-10  grad at x: [ 4.44547768e-05 -6.06201502e-06]  gradient norm: 4.4866192241723734e-05\n",
            "iter: 764  x: [-99.99997822  24.99999703]  f(x): 4.833153471323968e-10  grad at x: [ 4.35656813e-05 -5.94077472e-06]  gradient norm: 4.396886840174065e-05\n",
            "iter: 765  x: [-99.99997865  24.99999709]  f(x): 4.6417605926998697e-10  grad at x: [ 4.26943676e-05 -5.82195923e-06]  gradient norm: 4.308949102832323e-05\n",
            "iter: 766  x: [-99.99997908  24.99999715]  f(x): 4.457946875327584e-10  grad at x: [ 4.18404803e-05 -5.70552005e-06]  gradient norm: 4.222770121769635e-05\n",
            "iter: 767  x: [-99.9999795  24.9999972]  f(x): 4.281412176427907e-10  grad at x: [ 4.10036707e-05 -5.59140965e-06]  gradient norm: 4.138314718059953e-05\n",
            "iter: 768  x: [-99.99997991  24.99999726]  f(x): 4.1118682549266135e-10  grad at x: [ 4.01835973e-05 -5.47958145e-06]  gradient norm: 4.055548424036687e-05\n",
            "iter: 769  x: [-99.99998031  24.99999732]  f(x): 3.949038270615929e-10  grad at x: [ 3.93799253e-05 -5.36998982e-06]  gradient norm: 3.974437454843605e-05\n",
            "iter: 770  x: [-99.9999807   24.99999737]  f(x): 3.7926563544763227e-10  grad at x: [ 3.85923268e-05 -5.26259002e-06]  gradient norm: 3.8949487054267214e-05\n",
            "iter: 771  x: [-99.99998109  24.99999742]  f(x): 3.642467161341497e-10  grad at x: [ 3.78204803e-05 -5.15733822e-06]  gradient norm: 3.8170497305335166e-05\n",
            "iter: 772  x: [-99.99998147  24.99999747]  f(x): 3.498225461673367e-10  grad at x: [ 3.70640707e-05 -5.05419145e-06]  gradient norm: 3.7407087358806044e-05\n",
            "iter: 773  x: [-99.99998184  24.99999752]  f(x): 3.359695733739522e-10  grad at x: [ 3.63227892e-05 -4.95310763e-06]  gradient norm: 3.66589456135308e-05\n",
            "iter: 774  x: [-99.9999822   24.99999757]  f(x): 3.2266517807830343e-10  grad at x: [ 3.55963334e-05 -4.85404547e-06]  gradient norm: 3.592576669068057e-05\n",
            "iter: 775  x: [-99.99998256  24.99999762]  f(x): 3.0988763694325396e-10  grad at x: [ 3.48844068e-05 -4.75696456e-06]  gradient norm: 3.520725135214358e-05\n",
            "iter: 776  x: [-99.99998291  24.99999767]  f(x): 2.9761608649921196e-10  grad at x: [ 3.41867186e-05 -4.66182527e-06]  gradient norm: 3.4503106323878255e-05\n",
            "iter: 777  x: [-99.99998325  24.99999772]  f(x): 2.858304892673863e-10  grad at x: [ 3.35029843e-05 -4.56858876e-06]  gradient norm: 3.3813044185189026e-05\n",
            "iter: 778  x: [-99.99998358  24.99999776]  f(x): 2.745116020255863e-10  grad at x: [ 3.28329246e-05 -4.47721699e-06]  gradient norm: 3.313678330952395e-05\n",
            "iter: 779  x: [-99.99998391  24.99999781]  f(x): 2.636409424556797e-10  grad at x: [ 3.21762661e-05 -4.38767265e-06]  gradient norm: 3.2474047635345966e-05\n",
            "iter: 780  x: [-99.99998423  24.99999785]  f(x): 2.532007612893373e-10  grad at x: [ 3.15327408e-05 -4.29991920e-06]  gradient norm: 3.1824566692373825e-05\n",
            "iter: 781  x: [-99.99998455  24.99999789]  f(x): 2.43174010995166e-10  grad at x: [ 3.09020859e-05 -4.21392081e-06]  gradient norm: 3.118807534909238e-05\n",
            "iter: 782  x: [-99.99998486  24.99999794]  f(x): 2.335443200681098e-10  grad at x: [ 3.02840442e-05 -4.12964239e-06]  gradient norm: 3.0564313836113504e-05\n",
            "iter: 783  x: [-99.99998516  24.99999798]  f(x): 2.242959648241356e-10  grad at x: [ 2.96783633e-05 -4.04704954e-06]  gradient norm: 2.9953027548088396e-05\n",
            "iter: 784  x: [-99.99998546  24.99999802]  f(x): 2.154138444318574e-10  grad at x: [ 2.90847960e-05 -3.96610855e-06]  gradient norm: 2.9353966984505342e-05\n",
            "iter: 785  x: [-99.99998575  24.99999806]  f(x): 2.068834560805979e-10  grad at x: [ 2.85031001e-05 -3.88678638e-06]  gradient norm: 2.8766887637045333e-05\n",
            "iter: 786  x: [-99.99998603  24.9999981 ]  f(x): 1.9869087106183724e-10  grad at x: [ 2.79330381e-05 -3.80905065e-06]  gradient norm: 2.8191549873097594e-05\n",
            "iter: 787  x: [-99.99998631  24.99999813]  f(x): 1.9082271262030528e-10  grad at x: [ 2.73743773e-05 -3.73286964e-06]  gradient norm: 2.7627718879437388e-05\n",
            "iter: 788  x: [-99.99998659  24.99999817]  f(x): 1.832661330322788e-10  grad at x: [ 2.68268898e-05 -3.65821225e-06]  gradient norm: 2.707516448941936e-05\n",
            "iter: 789  x: [-99.99998685  24.99999821]  f(x): 1.7600879432960719e-10  grad at x: [ 2.62903520e-05 -3.58504801e-06]  gradient norm: 2.6533661212098656e-05\n",
            "iter: 790  x: [-99.99998712  24.99999824]  f(x): 1.6903884590623133e-10  grad at x: [ 2.57645449e-05 -3.51334705e-06]  gradient norm: 2.6002987974940983e-05\n",
            "iter: 791  x: [-99.99998738  24.99999828]  f(x): 1.623449075051857e-10  grad at x: [ 2.5249254e-05 -3.4430801e-06]  gradient norm: 2.548292820734585e-05\n",
            "iter: 792  x: [-99.99998763  24.99999831]  f(x): 1.5591604915998862e-10  grad at x: [ 2.4744269e-05 -3.3742185e-06]  gradient norm: 2.497326964255891e-05\n",
            "iter: 793  x: [-99.99998788  24.99999835]  f(x): 1.4974177372047204e-10  grad at x: [ 2.42493836e-05 -3.30673413e-06]  gradient norm: 2.447380425846967e-05\n",
            "iter: 794  x: [-99.99998812  24.99999838]  f(x): 1.4381199938120917e-10  grad at x: [ 2.37643959e-05 -3.24059945e-06]  gradient norm: 2.3984328164967154e-05\n",
            "iter: 795  x: [-99.99998836  24.99999841]  f(x): 1.3811704418728497e-10  grad at x: [ 2.32891080e-05 -3.17578746e-06]  gradient norm: 2.3504641600099754e-05\n",
            "iter: 796  x: [-99.99998859  24.99999844]  f(x): 1.3264760917908746e-10  grad at x: [ 2.28233258e-05 -3.11227171e-06]  gradient norm: 2.3034548763028762e-05\n",
            "iter: 797  x: [-99.99998882  24.99999847]  f(x): 1.2739476377996122e-10  grad at x: [ 2.23668593e-05 -3.05002627e-06]  gradient norm: 2.2573857781067126e-05\n",
            "iter: 798  x: [-99.99998904  24.99999851]  f(x): 1.223499311315138e-10  grad at x: [ 2.19195221e-05 -2.98902575e-06]  gradient norm: 2.2122380625196176e-05\n",
            "iter: 799  x: [-99.99998926  24.99999854]  f(x): 1.1750487398802728e-10  grad at x: [ 2.14811317e-05 -2.92924523e-06]  gradient norm: 2.1679933024622312e-05\n",
            "iter: 800  x: [-99.99998947  24.99999856]  f(x): 1.12851680861701e-10  grad at x: [ 2.10515090e-05 -2.87066032e-06]  gradient norm: 2.1246334353172644e-05\n",
            "iter: 801  x: [-99.99998968  24.99999859]  f(x): 1.0838275416951377e-10  grad at x: [ 2.06304788e-05 -2.81324712e-06]  gradient norm: 2.0821407653615907e-05\n",
            "iter: 802  x: [-99.99998989  24.99999862]  f(x): 1.0409079706678912e-10  grad at x: [ 2.02178693e-05 -2.75698218e-06]  gradient norm: 2.0404979496857046e-05\n",
            "iter: 803  x: [-99.99999009  24.99999865]  f(x): 9.996880141789825e-11  grad at x: [ 1.98135119e-05 -2.70184253e-06]  gradient norm: 1.9996879898413977e-05\n",
            "iter: 804  x: [-99.99999029  24.99999868]  f(x): 9.601003701081209e-11  grad at x: [ 1.94172416e-05 -2.64780568e-06]  gradient norm: 1.9596942313617408e-05\n",
            "iter: 805  x: [-99.99999049  24.9999987 ]  f(x): 9.22080395760975e-11  grad at x: [ 1.90288968e-05 -2.59484957e-06]  gradient norm: 1.920500347056438e-05\n",
            "iter: 806  x: [-99.99999068  24.99999873]  f(x): 8.855660113666921e-11  grad at x: [ 1.86483189e-05 -2.54295258e-06]  gradient norm: 1.88209033934792e-05\n",
            "iter: 807  x: [-99.99999086  24.99999875]  f(x): 8.50497596813091e-11  grad at x: [ 1.82753525e-05 -2.49209353e-06]  gradient norm: 1.8444485320150205e-05\n",
            "iter: 808  x: [-99.99999105  24.99999878]  f(x): 8.168178920533331e-11  grad at x: [ 1.79098454e-05 -2.44225166e-06]  gradient norm: 1.8075595614566432e-05\n",
            "iter: 809  x: [-99.99999122  24.9999988 ]  f(x): 7.844719043499862e-11  grad at x: [ 1.75516485e-05 -2.39340662e-06]  gradient norm: 1.7714083711555462e-05\n",
            "iter: 810  x: [-99.9999914   24.99999883]  f(x): 7.534068160849804e-11  grad at x: [ 1.72006156e-05 -2.34553849e-06]  gradient norm: 1.7359802027499973e-05\n",
            "iter: 811  x: [-99.99999157  24.99999885]  f(x): 7.235719065447568e-11  grad at x: [ 1.68566033e-05 -2.29862772e-06]  gradient norm: 1.7012605991378943e-05\n",
            "iter: 812  x: [-99.99999174  24.99999887]  f(x): 6.949184590797308e-11  grad at x: [ 1.65194712e-05 -2.25265516e-06]  gradient norm: 1.6672353871960982e-05\n",
            "iter: 813  x: [-99.99999191  24.9999989 ]  f(x): 6.673996890350564e-11  grad at x: [ 1.61890818e-05 -2.20760206e-06]  gradient norm: 1.6338906805965402e-05\n",
            "iter: 814  x: [-99.99999207  24.99999892]  f(x): 6.409706603413618e-11  grad at x: [ 1.58653001e-05 -2.16345002e-06]  gradient norm: 1.601212865725681e-05\n",
            "iter: 815  x: [-99.99999223  24.99999894]  f(x): 6.155882232413579e-11  grad at x: [ 1.55479941e-05 -2.12018102e-06]  gradient norm: 1.569188609748819e-05\n",
            "iter: 816  x: [-99.99999238  24.99999896]  f(x): 5.912109300803216e-11  grad at x: [ 1.52370343e-05 -2.07777740e-06]  gradient norm: 1.5378048381772266e-05\n",
            "iter: 817  x: [-99.99999253  24.99999898]  f(x): 5.677989765609355e-11  grad at x: [ 1.49322936e-05 -2.03622186e-06]  gradient norm: 1.5070487405003668e-05\n",
            "iter: 818  x: [-99.99999268  24.999999  ]  f(x): 5.45314138045723e-11  grad at x: [ 1.46336477e-05 -1.99549742e-06]  gradient norm: 1.4769077669857695e-05\n",
            "iter: 819  x: [-99.99999283  24.99999902]  f(x): 5.237196984167217e-11  grad at x: [ 1.43409748e-05 -1.95558747e-06]  gradient norm: 1.4473696119743868e-05\n",
            "iter: 820  x: [-99.99999297  24.99999904]  f(x): 5.029803980657381e-11  grad at x: [ 1.40541553e-05 -1.91647572e-06]  gradient norm: 1.418422219320803e-05\n",
            "iter: 821  x: [-99.99999311  24.99999906]  f(x): 4.830623734144454e-11  grad at x: [ 1.37730721e-05 -1.87814621e-06]  gradient norm: 1.3900537736568976e-05\n",
            "iter: 822  x: [-99.99999325  24.99999908]  f(x): 4.6393310320926457e-11  grad at x: [ 1.34976107e-05 -1.84058328e-06]  gradient norm: 1.3622526978637474e-05\n",
            "iter: 823  x: [-99.99999339  24.9999991 ]  f(x): 4.455613526084144e-11  grad at x: [ 1.32276585e-05 -1.80377162e-06]  gradient norm: 1.335007644335289e-05\n",
            "iter: 824  x: [-99.99999352  24.99999912]  f(x): 4.279171233235397e-11  grad at x: [ 1.29631053e-05 -1.76769618e-06]  gradient norm: 1.3083074918741997e-05\n",
            "iter: 825  x: [-99.99999365  24.99999913]  f(x): 4.1097160602401294e-11  grad at x: [ 1.27038432e-05 -1.73234226e-06]  gradient norm: 1.2821413432598028e-05\n",
            "iter: 826  x: [-99.99999378  24.99999915]  f(x): 3.946971303181026e-11  grad at x: [ 1.24497664e-05 -1.69769542e-06]  gradient norm: 1.2564985162237202e-05\n",
            "iter: 827  x: [-99.9999939   24.99999917]  f(x): 3.790671243606221e-11  grad at x: [ 1.22007710e-05 -1.66374151e-06]  gradient norm: 1.231368546553991e-05\n",
            "iter: 828  x: [-99.99999402  24.99999918]  f(x): 3.640560668487966e-11  grad at x: [ 1.19567556e-05 -1.63046668e-06]  gradient norm: 1.2067411766386306e-05\n",
            "iter: 829  x: [-99.99999414  24.9999992 ]  f(x): 3.496394462182158e-11  grad at x: [ 1.17176205e-05 -1.59785734e-06]  gradient norm: 1.1826063524575128e-05\n",
            "iter: 830  x: [-99.99999426  24.99999922]  f(x): 3.357937247773572e-11  grad at x: [ 1.14832681e-05 -1.56590019e-06]  gradient norm: 1.1589542264944845e-05\n",
            "iter: 831  x: [-99.99999437  24.99999923]  f(x): 3.224962925834152e-11  grad at x: [ 1.12536027e-05 -1.53458219e-06]  gradient norm: 1.1357751407447077e-05\n",
            "iter: 832  x: [-99.99999449  24.99999925]  f(x): 3.0972543926425205e-11  grad at x: [ 1.10285307e-05 -1.50389054e-06]  gradient norm: 1.1130596376910844e-05\n",
            "iter: 833  x: [-99.9999946   24.99999926]  f(x): 2.9746031113983347e-11  grad at x: [ 1.08079601e-05 -1.47381273e-06]  gradient norm: 1.090798443599611e-05\n",
            "iter: 834  x: [-99.9999947   24.99999928]  f(x): 2.8568088245950406e-11  grad at x: [ 1.05918008e-05 -1.44433648e-06]  gradient norm: 1.0689824740555928e-05\n",
            "iter: 835  x: [-99.99999481  24.99999929]  f(x): 2.7436791985605846e-11  grad at x: [ 1.03799648e-05 -1.41544974e-06]  gradient norm: 1.0476028252273062e-05\n",
            "iter: 836  x: [-99.99999491  24.99999931]  f(x): 2.6350295007731554e-11  grad at x: [ 1.01723655e-05 -1.38714075e-06]  gradient norm: 1.0266507684257885e-05\n",
            "iter: 837  x: [-99.99999502  24.99999932]  f(x): 2.530682330890832e-11  grad at x: [ 9.96891822e-06 -1.35939793e-06]  gradient norm: 1.0061177527289402e-05\n",
            "iter: 838  x: [-99.99999512  24.99999933]  f(x): 2.4304673044883465e-11  grad at x: [ 9.76953984e-06 -1.33220998e-06]  gradient norm: 9.859953964371935e-06\n",
            "Optimizer: [-99.99999512  24.99999933]\n",
            "for step length = 0.01, the minimum value of function is 2.4304673044883465e-11 and number of iterations are= 838\n",
            "\n",
            "For step length = 0.1\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-12.  13.]  f(x): 7888.0  grad at x: [176. -24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-43.68  17.32]  f(x): 3230.9247999999993  grad at x: [112.64 -15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-63.9552  20.0848]  f(x): 1323.3867980799996  grad at x: [72.0896 -9.8304]  gradient norm: 72.75676733005665\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507711995  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404531\n",
            "iter: 7  x: [-76.931328  21.854272]  f(x): 542.0592324935676  grad at x: [46.137344 -6.291456]  gradient norm: 46.56433109123625\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958833  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989\n",
            "iter: 9  x: [-85.23604992  22.98673408]  f(x): 222.0274616293655  grad at x: [29.52790016 -4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-90.55107195  23.71150981]  f(x): 90.94244828338809  grad at x: [18.8978561  -2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-93.95268605  24.17536628]  f(x): 37.25002681687585  grad at x: [12.09462791 -1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-96.12971907  24.47223442]  f(x): 15.25761098419235  grad at x: [ 7.74056186 -1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-97.5230202   24.66223003]  f(x): 6.249517459125205  grad at x: [ 4.95395959 -0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-98.41473293  24.78382722]  f(x): 2.559802351257707  grad at x: [ 3.17053414 -0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-98.98542908  24.86164942]  f(x): 1.0484950430751447  grad at x: [ 2.02914185 -0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-99.35067461  24.91145563]  f(x): 0.42946356964356897  grad at x: [ 1.29865078 -0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-99.58443175  24.9433316 ]  f(x): 0.1759082781260078  grad at x: [ 0.8311365 -0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-99.73403632  24.96373223]  f(x): 0.07205203072040883  grad at x: [ 0.53192736 -0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-99.82978324  24.97678862]  f(x): 0.02951251178308121  grad at x: [ 0.34043351 -0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-99.89106128  24.98514472]  f(x): 0.01208832482635049  grad at x: [ 0.21787745 -0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-99.93027922  24.99049262]  f(x): 0.004951377848873557  grad at x: [ 0.13944157 -0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-99.9553787   24.99391528]  f(x): 0.0020280843668995063  grad at x: [ 0.0892426  -0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-99.97144237  24.99610578]  f(x): 0.0008307033566815811  grad at x: [ 0.05711527 -0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-99.98172312  24.9975077 ]  f(x): 0.000340256094896659  grad at x: [ 0.03655377 -0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-99.98830279  24.99840493]  f(x): 0.0001393688964697853  grad at x: [ 0.02339441 -0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-99.99251379  24.99897915]  f(x): 5.7085499994068934e-05  grad at x: [ 0.01497242 -0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-99.99520882  24.99934666]  f(x): 2.3382220797474075e-05  grad at x: [ 0.00958235 -0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-99.99693365  24.99958186]  f(x): 9.577357638678578e-06  grad at x: [ 0.0061327  -0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-99.99803753  24.99973239]  f(x): 3.922885688770902e-06  grad at x: [ 0.00392493 -0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-99.99874402  24.99982873]  f(x): 1.6068139781229793e-06  grad at x: [ 0.00251196 -0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-99.99919617  24.99989039]  f(x): 6.581510054452161e-07  grad at x: [ 0.00160765 -0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-99.99948555  24.99992985]  f(x): 2.6957865182860594e-07  grad at x: [ 0.0010289 -0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-99.99967075  24.9999551 ]  f(x): 1.104194157885206e-07  grad at x: [ 6.58494178e-04 -8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-99.99978928  24.99997127]  f(x): 4.5227792707696715e-08  grad at x: [ 4.21436274e-04 -5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-99.99986514  24.99998161]  f(x): 1.8525303893532528e-08  grad at x: [ 2.69719215e-04 -3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-99.99991369  24.99998823]  f(x): 7.587964476523691e-09  grad at x: [ 1.72620298e-04 -2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-99.99994476  24.99999247]  f(x): 3.1080302495512774e-09  grad at x: [ 1.10476991e-04 -1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-99.99996465  24.99999518]  f(x): 1.2730491896964565e-09  grad at x: [ 7.07052739e-05 -9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-99.99997737  24.99999691]  f(x): 5.214409481891125e-10  grad at x: [ 4.52513753e-05 -6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-99.99998552  24.99999803]  f(x): 2.1358221267738897e-10  grad at x: [ 2.89608802e-05 -3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-99.99999073  24.99999874]  f(x): 8.74832742970941e-11  grad at x: [ 1.85349633e-05 -2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-99.99999407  24.99999919]  f(x): 3.583314913446605e-11  grad at x: [ 1.18623765e-05 -1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "Optimizer: [-99.99999526  24.99999935]\n",
            "for step length = 0.1, the minimum value of function is 2.293321544605827e-11 and number of iterations are= 76\n",
            "\n",
            "For step length = 0.2\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-34.  16.]  f(x): 4437.0  grad at x: [132. -18.]  gradient norm: 133.2216198670471\n",
            "iter: 2  x: [-60.4  19.6]  f(x): 1597.3199999999997  grad at x: [ 79.2 -10.8]  gradient norm: 79.93297192022825\n",
            "iter: 3  x: [-76.24  21.76]  f(x): 575.0351999999996  grad at x: [47.52 -6.48]  gradient norm: 47.95978315213694\n",
            "iter: 4  x: [-85.744  23.056]  f(x): 207.012672  grad at x: [28.512 -3.888]  gradient norm: 28.775869891282177\n",
            "iter: 5  x: [-91.4464  23.8336]  f(x): 74.52456192000005  grad at x: [17.1072 -2.3328]  gradient norm: 17.26552193476931\n",
            "iter: 6  x: [-94.86784  24.30016]  f(x): 26.828842291199987  grad at x: [10.26432 -1.39968]  gradient norm: 10.35931316086158\n",
            "iter: 7  x: [-96.920704  24.580096]  f(x): 9.658383224831995  grad at x: [ 6.158592 -0.839808]  gradient norm: 6.215587896516948\n",
            "iter: 8  x: [-98.1524224  24.7480576]  f(x): 3.4770179609394978  grad at x: [ 3.6951552 -0.5038848]  gradient norm: 3.729352737910158\n",
            "iter: 9  x: [-98.89145344  24.84883456]  f(x): 1.2517264659382128  grad at x: [ 2.21709312 -0.30233088]  gradient norm: 2.237611642746089\n",
            "iter: 10  x: [-99.33487206  24.90930074]  f(x): 0.4506215277377493  grad at x: [ 1.33025587 -0.18139853]  gradient norm: 1.3425669856476425\n",
            "iter: 11  x: [-99.60092324  24.94558044]  f(x): 0.16222374998559413  grad at x: [ 0.79815352 -0.10883912]  gradient norm: 0.8055401913885964\n",
            "iter: 12  x: [-99.76055394  24.96734826]  f(x): 0.05840054999481652  grad at x: [ 0.47889211 -0.06530347]  gradient norm: 0.4833241148331687\n",
            "iter: 13  x: [-99.85633237  24.98040896]  f(x): 0.02102419799813237  grad at x: [ 0.28733527 -0.03918208]  gradient norm: 0.2899944688998904\n",
            "iter: 14  x: [-99.91379942  24.98824538]  f(x): 0.007568711279328648  grad at x: [ 0.17240116 -0.02350925]  gradient norm: 0.17399668133994567\n",
            "iter: 15  x: [-99.94827965  24.99294723]  f(x): 0.002724736060558334  grad at x: [ 0.1034407  -0.01410555]  gradient norm: 0.1043980088039678\n",
            "iter: 16  x: [-99.96896779  24.99576834]  f(x): 0.0009809049818006353  grad at x: [ 0.06206442 -0.00846333]  gradient norm: 0.06263880528236902\n",
            "iter: 17  x: [-99.98138067  24.997461  ]  f(x): 0.0003531257934483273  grad at x: [ 0.03723865 -0.005078  ]  gradient norm: 0.03758328316942666\n",
            "iter: 18  x: [-99.9888284  24.9984766]  f(x): 0.0001271252856412665  grad at x: [ 0.02234319 -0.0030468 ]  gradient norm: 0.02254996990164435\n",
            "iter: 19  x: [-99.99329704  24.99908596]  f(x): 4.5765102830777144e-05  grad at x: [ 0.01340591 -0.00182808]  gradient norm: 0.013529981940974961\n",
            "iter: 20  x: [-99.99597823  24.99945158]  f(x): 1.6475437019127054e-05  grad at x: [ 0.00804355 -0.00109685]  gradient norm: 0.008117989164596625\n",
            "iter: 21  x: [-99.99758694  24.99967095]  f(x): 5.931157326913172e-06  grad at x: [ 0.00482613 -0.00065811]  gradient norm: 0.0048707934987692394\n",
            "iter: 22  x: [-99.99855216  24.99980257]  f(x): 2.1352166376799506e-06  grad at x: [ 0.00289568 -0.00039487]  gradient norm: 0.002922476099255527\n",
            "iter: 23  x: [-99.9991313   24.99988154]  f(x): 7.686779895647823e-07  grad at x: [ 0.00173741 -0.00023692]  gradient norm: 0.0017534856595533164\n",
            "iter: 24  x: [-99.99947878  24.99992892]  f(x): 2.76724076237598e-07  grad at x: [ 0.00104244 -0.00014215]  gradient norm: 0.0010520913957211093\n",
            "iter: 25  x: [-99.99968727  24.99995735]  f(x): 9.962066744737358e-08  grad at x: [ 6.25466337e-04 -8.52908641e-05]  gradient norm: 0.0006312548374384899\n",
            "iter: 26  x: [-99.99981236  24.99997441]  f(x): 3.586344028208473e-08  grad at x: [ 3.75279802e-04 -5.11745185e-05]  gradient norm: 0.00037875290246853414\n",
            "iter: 27  x: [-99.99988742  24.99998465]  f(x): 1.2910838502830435e-08  grad at x: [ 2.25167881e-04 -3.07047111e-05]  gradient norm: 0.00022725174149238492\n",
            "iter: 28  x: [-99.99993245  24.99999079]  f(x): 4.647901861005866e-09  grad at x: [ 1.35100729e-04 -1.84228266e-05]  gradient norm: 0.00013635104489523894\n",
            "iter: 29  x: [-99.99995947  24.99999447]  f(x): 1.6732446695013367e-09  grad at x: [ 8.10604372e-05 -1.10536960e-05]  gradient norm: 8.181062692587893e-05\n",
            "iter: 30  x: [-99.99997568  24.99999668]  f(x): 6.023680810204812e-10  grad at x: [ 4.86362623e-05 -6.63221759e-06]  gradient norm: 4.908637615552736e-05\n",
            "iter: 31  x: [-99.99998541  24.99999801]  f(x): 2.1685250933890727e-10  grad at x: [ 2.91817574e-05 -3.97933056e-06]  gradient norm: 2.9451825704964864e-05\n",
            "iter: 32  x: [-99.99999125  24.99999881]  f(x): 7.806690346323056e-11  grad at x: [ 1.75090545e-05 -2.38759834e-06]  gradient norm: 1.767109543443536e-05\n",
            "iter: 33  x: [-99.99999475  24.99999928]  f(x): 2.8104085217922663e-11  grad at x: [ 1.05054327e-05 -1.43255900e-06]  gradient norm: 1.0602657255221007e-05\n",
            "iter: 34  x: [-99.99999685  24.99999957]  f(x): 1.011747066175869e-11  grad at x: [ 6.30325960e-06 -8.59535405e-07]  gradient norm: 6.361594347884402e-06\n",
            "Optimizer: [-99.99999685  24.99999957]\n",
            "for step length = 0.2, the minimum value of function is 1.011747066175869e-11 and number of iterations are= 34\n",
            "\n",
            "For step length = 0.3\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-56.  19.]  f(x): 1972.0  grad at x: [ 88. -12.]  gradient norm: 88.81441324469807\n",
            "iter: 2  x: [-82.4  22.6]  f(x): 315.5199999999998  grad at x: [35.2 -4.8]  gradient norm: 35.52576529787922\n",
            "iter: 3  x: [-92.96  24.04]  f(x): 50.48319999999989  grad at x: [14.08 -1.92]  gradient norm: 14.210306119151676\n",
            "iter: 4  x: [-97.184  24.616]  f(x): 8.077312000000015  grad at x: [ 5.632 -0.768]  gradient norm: 5.684122447660682\n",
            "iter: 5  x: [-98.8736  24.8464]  f(x): 1.292369920000009  grad at x: [ 2.2528 -0.3072]  gradient norm: 2.2736489790642787\n",
            "iter: 6  x: [-99.54944  24.93856]  f(x): 0.20677918719999638  grad at x: [ 0.90112 -0.12288]  gradient norm: 0.9094595916257003\n",
            "iter: 7  x: [-99.819776  24.975424]  f(x): 0.03308466995199836  grad at x: [ 0.360448 -0.049152]  gradient norm: 0.36378383665027425\n",
            "iter: 8  x: [-99.9279104  24.9901696]  f(x): 0.00529354719231971  grad at x: [ 0.1441792 -0.0196608]  gradient norm: 0.14551353466010933\n",
            "iter: 9  x: [-99.97116416  24.99606784]  f(x): 0.0008469675507711425  grad at x: [ 0.05767168 -0.00786432]  gradient norm: 0.05820541386404335\n",
            "iter: 10  x: [-99.98846566  24.99842714]  f(x): 0.00013551480812331945  grad at x: [ 0.02306867 -0.00314573]  gradient norm: 0.0232821655456119\n",
            "iter: 11  x: [-99.99538627  24.99937085]  f(x): 2.1682369299706677e-05  grad at x: [ 0.00922747 -0.00125829]  gradient norm: 0.009312866218239511\n",
            "iter: 12  x: [-99.99815451  24.99974834]  f(x): 3.4691790879632008e-06  grad at x: [ 0.00369099 -0.00050332]  gradient norm: 0.003725146487301245\n",
            "iter: 13  x: [-99.9992618   24.99989934]  f(x): 5.550686540823615e-07  grad at x: [ 0.0014764  -0.00020133]  gradient norm: 0.0014900585949315704\n",
            "iter: 14  x: [-99.99970472  24.99995973]  f(x): 8.881098465491351e-08  grad at x: [ 5.90558003e-04 -8.05306368e-05]  gradient norm: 0.0005960234379784523\n",
            "iter: 15  x: [-99.99988189  24.99998389]  f(x): 1.4209757544831938e-08  grad at x: [ 2.36223201e-04 -3.22122547e-05]  gradient norm: 0.00023840937519176497\n",
            "iter: 16  x: [-99.99995276  24.99999356]  f(x): 2.2735612071914207e-09  grad at x: [ 9.44892805e-05 -1.28849019e-05]  gradient norm: 9.536375007709e-05\n",
            "iter: 17  x: [-99.9999811   24.99999742]  f(x): 3.637697933654711e-10  grad at x: [ 3.77957122e-05 -5.15396076e-06]  gradient norm: 3.8145500042100434e-05\n",
            "iter: 18  x: [-99.99999244  24.99999897]  f(x): 5.8203166982908965e-11  grad at x: [ 1.51182849e-05 -2.06158430e-06]  gradient norm: 1.5258200022664398e-05\n",
            "iter: 19  x: [-99.99999698  24.99999959]  f(x): 9.312506683476372e-12  grad at x: [ 6.04731395e-06 -8.24633723e-07]  gradient norm: 6.103279997993332e-06\n",
            "Optimizer: [-99.99999698  24.99999959]\n",
            "for step length = 0.3, the minimum value of function is 9.312506683476372e-12 and number of iterations are= 19\n",
            "\n",
            "For step length = 0.4\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-78.  22.]  f(x): 493.0  grad at x: [44. -6.]  gradient norm: 44.40720662234904\n",
            "iter: 2  x: [-95.6  24.4]  f(x): 19.720000000000052  grad at x: [ 8.8 -1.2]  gradient norm: 8.881441324469819\n",
            "iter: 3  x: [-99.12  24.88]  f(x): 0.7887999999999922  grad at x: [ 1.76 -0.24]  gradient norm: 1.7762882648939526\n",
            "iter: 4  x: [-99.824  24.976]  f(x): 0.031552000000000725  grad at x: [ 0.352 -0.048]  gradient norm: 0.35525765297879636\n",
            "iter: 5  x: [-99.9648  24.9952]  f(x): 0.0012620800000002223  grad at x: [ 0.0704 -0.0096]  gradient norm: 0.07105153059576472\n",
            "iter: 6  x: [-99.99296  24.99904]  f(x): 5.048320000004754e-05  grad at x: [ 0.01408 -0.00192]  gradient norm: 0.014210306119158382\n",
            "iter: 7  x: [-99.998592  24.999808]  f(x): 2.0193279999933527e-06  grad at x: [ 0.002816 -0.000384]  gradient norm: 0.0028420612238256606\n",
            "iter: 8  x: [-99.9997184  24.9999616]  f(x): 8.077311999664182e-08  grad at x: [ 5.632e-04 -7.680e-05]  gradient norm: 0.0005684122447542516\n",
            "iter: 9  x: [-99.99994368  24.99999232]  f(x): 3.230924799854759e-09  grad at x: [ 1.1264e-04 -1.5360e-05]  gradient norm: 0.00011368244895065832\n",
            "iter: 10  x: [-99.99998874  24.99999846]  f(x): 1.2923699186395072e-10  grad at x: [ 2.2528e-05 -3.0720e-06]  gradient norm: 2.273648977867522e-05\n",
            "iter: 11  x: [-99.99999775  24.99999969]  f(x): 5.169479687800271e-12  grad at x: [ 4.50560000e-06 -6.14400001e-07]  gradient norm: 4.547297961559269e-06\n",
            "Optimizer: [-99.99999775  24.99999969]\n",
            "for step length = 0.4, the minimum value of function is 5.169479687800271e-12 and number of iterations are= 11\n",
            "\n",
            "For step length = 0.5\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "Optimizer: [-100.   25.]\n",
            "for step length = 0.5, the minimum value of function is 0.0 and number of iterations are= 1\n",
            "\n",
            "For step length = 0.6\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-122.   28.]  f(x): 493.0  grad at x: [-44.   6.]  gradient norm: 44.40720662234904\n",
            "iter: 2  x: [-95.6  24.4]  f(x): 19.720000000000052  grad at x: [ 8.8 -1.2]  gradient norm: 8.881441324469819\n",
            "iter: 3  x: [-100.88   25.12]  f(x): 0.7887999999999922  grad at x: [-1.76  0.24]  gradient norm: 1.7762882648939526\n",
            "iter: 4  x: [-99.824  24.976]  f(x): 0.031552000000000725  grad at x: [ 0.352 -0.048]  gradient norm: 0.35525765297879636\n",
            "iter: 5  x: [-100.0352   25.0048]  f(x): 0.0012620800000002223  grad at x: [-0.0704  0.0096]  gradient norm: 0.07105153059576472\n",
            "iter: 6  x: [-99.99296  24.99904]  f(x): 5.048320000004754e-05  grad at x: [ 0.01408 -0.00192]  gradient norm: 0.014210306119158382\n",
            "iter: 7  x: [-100.001408   25.000192]  f(x): 2.0193279999933527e-06  grad at x: [-0.002816  0.000384]  gradient norm: 0.0028420612238256606\n",
            "iter: 8  x: [-99.9997184  24.9999616]  f(x): 8.077311999664182e-08  grad at x: [ 5.632e-04 -7.680e-05]  gradient norm: 0.0005684122447542516\n",
            "iter: 9  x: [-100.00005632   25.00000768]  f(x): 3.230924799854759e-09  grad at x: [-1.1264e-04  1.5360e-05]  gradient norm: 0.00011368244895065832\n",
            "iter: 10  x: [-99.99998874  24.99999846]  f(x): 1.2923699186395072e-10  grad at x: [ 2.2528e-05 -3.0720e-06]  gradient norm: 2.273648977867522e-05\n",
            "iter: 11  x: [-100.00000225   25.00000031]  f(x): 5.169479687800271e-12  grad at x: [-4.50560000e-06  6.14400001e-07]  gradient norm: 4.547297961559269e-06\n",
            "Optimizer: [-100.00000225   25.00000031]\n",
            "for step length = 0.6, the minimum value of function is 5.169479687800271e-12 and number of iterations are= 11\n",
            "\n",
            "For step length = 0.7\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-144.   31.]  f(x): 1972.0  grad at x: [-88.  12.]  gradient norm: 88.81441324469807\n",
            "iter: 2  x: [-82.4  22.6]  f(x): 315.5199999999998  grad at x: [35.2 -4.8]  gradient norm: 35.52576529787922\n",
            "iter: 3  x: [-107.04   25.96]  f(x): 50.48319999999989  grad at x: [-14.08   1.92]  gradient norm: 14.210306119151676\n",
            "iter: 4  x: [-97.184  24.616]  f(x): 8.077312000000015  grad at x: [ 5.632 -0.768]  gradient norm: 5.684122447660682\n",
            "iter: 5  x: [-101.1264   25.1536]  f(x): 1.292369920000009  grad at x: [-2.2528  0.3072]  gradient norm: 2.2736489790642787\n",
            "iter: 6  x: [-99.54944  24.93856]  f(x): 0.20677918719999638  grad at x: [ 0.90112 -0.12288]  gradient norm: 0.9094595916257003\n",
            "iter: 7  x: [-100.180224   25.024576]  f(x): 0.03308466995199836  grad at x: [-0.360448  0.049152]  gradient norm: 0.36378383665027425\n",
            "iter: 8  x: [-99.9279104  24.9901696]  f(x): 0.00529354719231971  grad at x: [ 0.1441792 -0.0196608]  gradient norm: 0.14551353466010933\n",
            "iter: 9  x: [-100.02883584   25.00393216]  f(x): 0.0008469675507711425  grad at x: [-0.05767168  0.00786432]  gradient norm: 0.05820541386404335\n",
            "iter: 10  x: [-99.98846566  24.99842714]  f(x): 0.00013551480812331945  grad at x: [ 0.02306867 -0.00314573]  gradient norm: 0.0232821655456119\n",
            "iter: 11  x: [-100.00461373   25.00062915]  f(x): 2.1682369299706677e-05  grad at x: [-0.00922747  0.00125829]  gradient norm: 0.009312866218239511\n",
            "iter: 12  x: [-99.99815451  24.99974834]  f(x): 3.4691790879632008e-06  grad at x: [ 0.00369099 -0.00050332]  gradient norm: 0.003725146487301245\n",
            "iter: 13  x: [-100.0007382    25.00010066]  f(x): 5.550686540823615e-07  grad at x: [-0.0014764   0.00020133]  gradient norm: 0.0014900585949315704\n",
            "iter: 14  x: [-99.99970472  24.99995973]  f(x): 8.881098465491351e-08  grad at x: [ 5.90558003e-04 -8.05306368e-05]  gradient norm: 0.0005960234379784523\n",
            "iter: 15  x: [-100.00011811   25.00001611]  f(x): 1.4209757544831938e-08  grad at x: [-2.36223201e-04  3.22122547e-05]  gradient norm: 0.00023840937519176497\n",
            "iter: 16  x: [-99.99995276  24.99999356]  f(x): 2.2735612071914207e-09  grad at x: [ 9.44892805e-05 -1.28849019e-05]  gradient norm: 9.536375007709e-05\n",
            "iter: 17  x: [-100.0000189    25.00000258]  f(x): 3.637697933654711e-10  grad at x: [-3.77957122e-05  5.15396076e-06]  gradient norm: 3.8145500042100434e-05\n",
            "iter: 18  x: [-99.99999244  24.99999897]  f(x): 5.8203166982908965e-11  grad at x: [ 1.51182849e-05 -2.06158430e-06]  gradient norm: 1.5258200022664398e-05\n",
            "iter: 19  x: [-100.00000302   25.00000041]  f(x): 9.312506683476372e-12  grad at x: [-6.04731395e-06  8.24633723e-07]  gradient norm: 6.103279997993332e-06\n",
            "Optimizer: [-100.00000302   25.00000041]\n",
            "for step length = 0.7, the minimum value of function is 9.312506683476372e-12 and number of iterations are= 19\n",
            "\n",
            "For step length = 0.8\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-166.   34.]  f(x): 4437.0  grad at x: [-132.   18.]  gradient norm: 133.2216198670471\n",
            "iter: 2  x: [-60.4  19.6]  f(x): 1597.3200000000008  grad at x: [ 79.2 -10.8]  gradient norm: 79.93297192022828\n",
            "iter: 3  x: [-123.76   28.24]  f(x): 575.0352000000003  grad at x: [-47.52   6.48]  gradient norm: 47.95978315213697\n",
            "iter: 4  x: [-85.744  23.056]  f(x): 207.01267200000004  grad at x: [28.512 -3.888]  gradient norm: 28.775869891282177\n",
            "iter: 5  x: [-108.5536   26.1664]  f(x): 74.52456192000007  grad at x: [-17.1072   2.3328]  gradient norm: 17.265521934769314\n",
            "iter: 6  x: [-94.86784  24.30016]  f(x): 26.82884229119999  grad at x: [10.26432 -1.39968]  gradient norm: 10.35931316086158\n",
            "iter: 7  x: [-103.079296   25.419904]  f(x): 9.658383224831997  grad at x: [-6.158592  0.839808]  gradient norm: 6.215587896516949\n",
            "iter: 8  x: [-98.1524224  24.7480576]  f(x): 3.4770179609394978  grad at x: [ 3.6951552 -0.5038848]  gradient norm: 3.729352737910158\n",
            "iter: 9  x: [-101.10854656   25.15116544]  f(x): 1.2517264659382128  grad at x: [-2.21709312  0.30233088]  gradient norm: 2.237611642746089\n",
            "iter: 10  x: [-99.33487206  24.90930074]  f(x): 0.4506215277377493  grad at x: [ 1.33025587 -0.18139853]  gradient norm: 1.3425669856476425\n",
            "iter: 11  x: [-100.39907676   25.05441956]  f(x): 0.16222374998559413  grad at x: [-0.79815352  0.10883912]  gradient norm: 0.8055401913885964\n",
            "iter: 12  x: [-99.76055394  24.96734826]  f(x): 0.05840054999481652  grad at x: [ 0.47889211 -0.06530347]  gradient norm: 0.4833241148331687\n",
            "iter: 13  x: [-100.14366763   25.01959104]  f(x): 0.02102419799813237  grad at x: [-0.28733527  0.03918208]  gradient norm: 0.2899944688998904\n",
            "iter: 14  x: [-99.91379942  24.98824538]  f(x): 0.007568711279328648  grad at x: [ 0.17240116 -0.02350925]  gradient norm: 0.17399668133994567\n",
            "iter: 15  x: [-100.05172035   25.00705277]  f(x): 0.002724736060558334  grad at x: [-0.1034407   0.01410555]  gradient norm: 0.1043980088039678\n",
            "iter: 16  x: [-99.96896779  24.99576834]  f(x): 0.0009809049818006353  grad at x: [ 0.06206442 -0.00846333]  gradient norm: 0.06263880528236902\n",
            "iter: 17  x: [-100.01861933   25.002539  ]  f(x): 0.0003531257934483273  grad at x: [-0.03723865  0.005078  ]  gradient norm: 0.03758328316942666\n",
            "iter: 18  x: [-99.9888284  24.9984766]  f(x): 0.0001271252856412665  grad at x: [ 0.02234319 -0.0030468 ]  gradient norm: 0.02254996990164435\n",
            "iter: 19  x: [-100.00670296   25.00091404]  f(x): 4.5765102830777144e-05  grad at x: [-0.01340591  0.00182808]  gradient norm: 0.013529981940974961\n",
            "iter: 20  x: [-99.99597823  24.99945158]  f(x): 1.6475437019127054e-05  grad at x: [ 0.00804355 -0.00109685]  gradient norm: 0.008117989164596625\n",
            "iter: 21  x: [-100.00241306   25.00032905]  f(x): 5.931157326913172e-06  grad at x: [-0.00482613  0.00065811]  gradient norm: 0.0048707934987692394\n",
            "iter: 22  x: [-99.99855216  24.99980257]  f(x): 2.1352166376799506e-06  grad at x: [ 0.00289568 -0.00039487]  gradient norm: 0.002922476099255527\n",
            "iter: 23  x: [-100.0008687    25.00011846]  f(x): 7.686779895647823e-07  grad at x: [-0.00173741  0.00023692]  gradient norm: 0.0017534856595533164\n",
            "iter: 24  x: [-99.99947878  24.99992892]  f(x): 2.76724076237598e-07  grad at x: [ 0.00104244 -0.00014215]  gradient norm: 0.0010520913957211093\n",
            "iter: 25  x: [-100.00031273   25.00004265]  f(x): 9.962066744737358e-08  grad at x: [-6.25466337e-04  8.52908641e-05]  gradient norm: 0.0006312548374384899\n",
            "iter: 26  x: [-99.99981236  24.99997441]  f(x): 3.586344028208473e-08  grad at x: [ 3.75279802e-04 -5.11745185e-05]  gradient norm: 0.00037875290246853414\n",
            "iter: 27  x: [-100.00011258   25.00001535]  f(x): 1.2910838502830435e-08  grad at x: [-2.25167881e-04  3.07047111e-05]  gradient norm: 0.00022725174149238492\n",
            "iter: 28  x: [-99.99993245  24.99999079]  f(x): 4.647901861005866e-09  grad at x: [ 1.35100729e-04 -1.84228266e-05]  gradient norm: 0.00013635104489523894\n",
            "iter: 29  x: [-100.00004053   25.00000553]  f(x): 1.6732446695013367e-09  grad at x: [-8.10604372e-05  1.10536960e-05]  gradient norm: 8.181062692587893e-05\n",
            "iter: 30  x: [-99.99997568  24.99999668]  f(x): 6.023680810204812e-10  grad at x: [ 4.86362623e-05 -6.63221759e-06]  gradient norm: 4.908637615552736e-05\n",
            "iter: 31  x: [-100.00001459   25.00000199]  f(x): 2.1685250933890727e-10  grad at x: [-2.91817574e-05  3.97933056e-06]  gradient norm: 2.9451825704964864e-05\n",
            "iter: 32  x: [-99.99999125  24.99999881]  f(x): 7.806690346323056e-11  grad at x: [ 1.75090545e-05 -2.38759834e-06]  gradient norm: 1.767109543443536e-05\n",
            "iter: 33  x: [-100.00000525   25.00000072]  f(x): 2.8104085217922663e-11  grad at x: [-1.05054327e-05  1.43255900e-06]  gradient norm: 1.0602657255221007e-05\n",
            "iter: 34  x: [-99.99999685  24.99999957]  f(x): 1.011747066175869e-11  grad at x: [ 6.30325960e-06 -8.59535405e-07]  gradient norm: 6.361594347884402e-06\n",
            "Optimizer: [-99.99999685  24.99999957]\n",
            "for step length = 0.8, the minimum value of function is 1.011747066175869e-11 and number of iterations are= 34\n",
            "\n",
            "For step length = 0.9\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-188.   37.]  f(x): 7888.0  grad at x: [-176.   24.]  gradient norm: 177.62882648939615\n",
            "iter: 2  x: [-29.6  15.4]  f(x): 5048.320000000001  grad at x: [140.8 -19.2]  gradient norm: 142.10306119151693\n",
            "iter: 3  x: [-156.32   32.68]  f(x): 3230.9248  grad at x: [-112.64   15.36]  gradient norm: 113.68244895321352\n",
            "iter: 4  x: [-54.944  18.856]  f(x): 2067.791872  grad at x: [ 90.112 -12.288]  gradient norm: 90.94595916257082\n",
            "iter: 5  x: [-136.0448   29.9152]  f(x): 1323.3867980800007  grad at x: [-72.0896   9.8304]  gradient norm: 72.75676733005668\n",
            "iter: 6  x: [-71.16416  21.06784]  f(x): 846.9675507712003  grad at x: [57.67168 -7.86432]  gradient norm: 58.20541386404534\n",
            "iter: 7  x: [-123.068672   28.145728]  f(x): 542.0592324935683  grad at x: [-46.137344   6.291456]  gradient norm: 46.56433109123628\n",
            "iter: 8  x: [-81.5450624  22.4834176]  f(x): 346.9179087958838  grad at x: [36.9098752 -5.0331648]  gradient norm: 37.251464872989025\n",
            "iter: 9  x: [-114.76395008   27.01326592]  f(x): 222.0274616293655  grad at x: [-29.52790016   4.02653184]  gradient norm: 29.80117189839121\n",
            "iter: 10  x: [-88.18883994  23.38938726]  f(x): 142.09757544279404  grad at x: [23.62232013 -3.22122547]  gradient norm: 23.840937518712977\n",
            "iter: 11  x: [-109.44892805   26.28849019]  f(x): 90.94244828338809  grad at x: [-18.8978561    2.57698038]  gradient norm: 19.072750014970374\n",
            "iter: 12  x: [-92.44085756  23.96920785]  f(x): 58.20316690136841  grad at x: [15.11828488 -2.0615843 ]  gradient norm: 15.258200011976303\n",
            "iter: 13  x: [-106.04731395   25.82463372]  f(x): 37.25002681687585  grad at x: [-12.09462791   1.64926744]  gradient norm: 12.206560009581054\n",
            "iter: 14  x: [-95.16214884  24.34029302]  f(x): 23.840017162800546  grad at x: [ 9.67570232 -1.31941395]  gradient norm: 9.765248007664843\n",
            "iter: 15  x: [-103.87028093   25.52776558]  f(x): 15.25761098419235  grad at x: [-7.74056186  1.05553116]  gradient norm: 7.812198406131874\n",
            "iter: 16  x: [-96.90377526  24.57778753]  f(x): 9.764871029883087  grad at x: [ 6.19244949 -0.84442493]  gradient norm: 6.249758724905495\n",
            "iter: 17  x: [-102.4769798    25.33776997]  f(x): 6.249517459125205  grad at x: [-4.95395959  0.67553994]  gradient norm: 4.999806979924407\n",
            "iter: 18  x: [-98.01841616  24.72978402]  f(x): 3.9996911738401533  grad at x: [ 3.96316767 -0.54043196]  gradient norm: 3.999845583939537\n",
            "iter: 19  x: [-101.58526707   25.21617278]  f(x): 2.559802351257707  grad at x: [-3.17053414  0.43234556]  gradient norm: 3.199876467151635\n",
            "iter: 20  x: [-98.73178634  24.82706177]  f(x): 1.6382735048049322  grad at x: [ 2.53642731 -0.34587645]  gradient norm: 2.5599011737213075\n",
            "iter: 21  x: [-101.01457092   25.13835058]  f(x): 1.0484950430751447  grad at x: [-2.02914185  0.27670116]  gradient norm: 2.0479209389770348\n",
            "iter: 22  x: [-99.18834326  24.88931954]  f(x): 0.6710368275680882  grad at x: [ 1.62331348 -0.22136093]  gradient norm: 1.6383367511816223\n",
            "iter: 23  x: [-100.64932539   25.08854437]  f(x): 0.42946356964356897  grad at x: [-1.29865078  0.17708874]  gradient norm: 1.3106694009452864\n",
            "iter: 24  x: [-99.48053969  24.9291645 ]  f(x): 0.27485668457188733  grad at x: [ 1.03892063 -0.14167099]  gradient norm: 1.0485355207562352\n",
            "iter: 25  x: [-100.41556825   25.0566684 ]  f(x): 0.1759082781260078  grad at x: [-0.8311365  0.1133368]  gradient norm: 0.8388284166049879\n",
            "iter: 26  x: [-99.6675454   24.95466528]  f(x): 0.11258129800064115  grad at x: [ 0.6649092  -0.09066944]  gradient norm: 0.6710627332839789\n",
            "iter: 27  x: [-100.26596368   25.03626777]  f(x): 0.07205203072040883  grad at x: [-0.53192736  0.07253555]  gradient norm: 0.5368501866271775\n",
            "iter: 28  x: [-99.78722906  24.97098578]  f(x): 0.04611329966106277  grad at x: [ 0.42554189 -0.05802844]  gradient norm: 0.4294801493017472\n",
            "iter: 29  x: [-100.17021676   25.02321138]  f(x): 0.02951251178308121  grad at x: [-0.34043351  0.04642275]  gradient norm: 0.3435841194414038\n",
            "iter: 30  x: [-99.8638266  24.9814309]  f(x): 0.018888007541173576  grad at x: [ 0.27234681 -0.0371382 ]  gradient norm: 0.2748672955531347\n",
            "iter: 31  x: [-100.10893872   25.01485528]  f(x): 0.01208832482635049  grad at x: [-0.21787745  0.02971056]  gradient norm: 0.21989383644250232\n",
            "iter: 32  x: [-99.91284902  24.98811578]  f(x): 0.007736527888864313  grad at x: [ 0.17430196 -0.02376845]  gradient norm: 0.17591506915400185\n",
            "iter: 33  x: [-100.06972078   25.00950738]  f(x): 0.004951377848873557  grad at x: [-0.13944157  0.01901476]  gradient norm: 0.14073205532320712\n",
            "iter: 34  x: [-99.94422337  24.9923941 ]  f(x): 0.0031688818232796997  grad at x: [ 0.11155325 -0.01521181]  gradient norm: 0.11258564425857677\n",
            "iter: 35  x: [-100.0446213    25.00608472]  f(x): 0.0020280843668995063  grad at x: [-0.0892426   0.01216945]  gradient norm: 0.09006851540687248\n",
            "iter: 36  x: [-99.96430296  24.99513222]  f(x): 0.0012979739948154952  grad at x: [ 0.07139408 -0.00973556]  gradient norm: 0.07205481232549274\n",
            "iter: 37  x: [-100.02855763   25.00389422]  f(x): 0.0008307033566815811  grad at x: [-0.05711527  0.00778845]  gradient norm: 0.05764384986038254\n",
            "iter: 38  x: [-99.97715389  24.99688462]  f(x): 0.000531650148276203  grad at x: [ 0.04569221 -0.00623076]  gradient norm: 0.04611507988830565\n",
            "iter: 39  x: [-100.01827688   25.0024923 ]  f(x): 0.000340256094896659  grad at x: [-0.03655377  0.0049846 ]  gradient norm: 0.036892063910638505\n",
            "iter: 40  x: [-99.98537849  24.99800616]  f(x): 0.00021776390073393917  grad at x: [ 0.02924302 -0.00398768]  gradient norm: 0.02951365112851605\n",
            "iter: 41  x: [-100.01169721   25.00159507]  f(x): 0.0001393688964697853  grad at x: [-0.02339441  0.00319015]  gradient norm: 0.02361092090281828\n",
            "iter: 42  x: [-99.99064223  24.99872394]  f(x): 8.919609374066622e-05  grad at x: [ 0.01871553 -0.00255212]  gradient norm: 0.01888873672225501\n",
            "iter: 43  x: [-100.00748621   25.00102085]  f(x): 5.7085499994068934e-05  grad at x: [-0.01497242  0.00204169]  gradient norm: 0.01511098937780964\n",
            "iter: 44  x: [-99.99401103  24.99918332]  f(x): 3.653471999613835e-05  grad at x: [ 0.01197794 -0.00163336]  gradient norm: 0.01208879150223683\n",
            "iter: 45  x: [-100.00479118   25.00065334]  f(x): 2.3382220797474075e-05  grad at x: [-0.00958235  0.00130668]  gradient norm: 0.0096710332017782\n",
            "iter: 46  x: [-99.99616706  24.99947733]  f(x): 1.4964621310382665e-05  grad at x: [ 0.00766588 -0.00104535]  gradient norm: 0.007736826561422368\n",
            "iter: 47  x: [-100.00306635   25.00041814]  f(x): 9.577357638678578e-06  grad at x: [-0.0061327   0.00083628]  gradient norm: 0.006189461249148775\n",
            "iter: 48  x: [-99.99754692  24.99966549]  f(x): 6.129508888739396e-06  grad at x: [ 0.00490616 -0.00066902]  gradient norm: 0.004951568999313004\n",
            "iter: 49  x: [-100.00196247   25.00026761]  f(x): 3.922885688770902e-06  grad at x: [-0.00392493  0.00053522]  gradient norm: 0.003961255199439138\n",
            "iter: 50  x: [-99.99843003  24.99978591]  f(x): 2.5106468407952244e-06  grad at x: [ 0.00313994 -0.00042817]  gradient norm: 0.003169004159539854\n",
            "iter: 51  x: [-100.00125598   25.00017127]  f(x): 1.6068139781229793e-06  grad at x: [-0.00251196  0.00034254]  gradient norm: 0.002535203327642956\n",
            "iter: 52  x: [-99.99899522  24.99986298]  f(x): 1.0283609459933846e-06  grad at x: [ 0.00200956 -0.00027403]  gradient norm: 0.0020281626621091167\n",
            "iter: 53  x: [-100.00080383   25.00010961]  f(x): 6.581510054452161e-07  grad at x: [-0.00160765  0.00021923]  gradient norm: 0.0016225301296989418\n",
            "iter: 54  x: [-99.99935694  24.99991231]  f(x): 4.212166434776276e-07  grad at x: [ 0.00128612 -0.00017538]  gradient norm: 0.001298024103747889\n",
            "iter: 55  x: [-100.00051445   25.00007015]  f(x): 2.6957865182860594e-07  grad at x: [-0.0010289  0.0001403]  gradient norm: 0.0010384192830039433\n",
            "iter: 56  x: [-99.99958844  24.99994388]  f(x): 1.7253033717248775e-07  grad at x: [ 0.00082312 -0.00011224]  gradient norm: 0.0008307354264084029\n",
            "iter: 57  x: [-100.00032925   25.0000449 ]  f(x): 1.104194157885206e-07  grad at x: [-6.58494178e-04  8.97946606e-05]  gradient norm: 0.0006645883411210901\n",
            "iter: 58  x: [-99.9997366   24.99996408]  f(x): 7.066842610764766e-08  grad at x: [ 5.26795342e-04 -7.18357285e-05]  gradient norm: 0.0005316706729081365\n",
            "iter: 59  x: [-100.00021072   25.00002873]  f(x): 4.5227792707696715e-08  grad at x: [-4.21436274e-04  5.74685828e-05]  gradient norm: 0.000425336538320877\n",
            "iter: 60  x: [-99.99983143  24.99997701]  f(x): 2.8945787334842366e-08  grad at x: [ 3.37149019e-04 -4.59748662e-05]  gradient norm: 0.00034026923066796604\n",
            "iter: 61  x: [-100.00013486   25.00001839]  f(x): 1.8525303893532528e-08  grad at x: [-2.69719215e-04  3.67798930e-05]  gradient norm: 0.00027221538452874063\n",
            "iter: 62  x: [-99.99989211  24.99998529]  f(x): 1.1856194493087359e-08  grad at x: [ 2.15775372e-04 -2.94239144e-05]  gradient norm: 0.0002177723076342569\n",
            "iter: 63  x: [-100.00008631   25.00001177]  f(x): 7.587964476523691e-09  grad at x: [-1.72620298e-04  2.35391315e-05]  gradient norm: 0.00017421784611828595\n",
            "iter: 64  x: [-99.99993095  24.99999058]  f(x): 4.856297265381036e-09  grad at x: [ 1.38096238e-04 -1.88313052e-05]  gradient norm: 0.00013937427690045298\n",
            "iter: 65  x: [-100.00005524   25.00000753]  f(x): 3.1080302495512774e-09  grad at x: [-1.10476991e-04  1.50650442e-05]  gradient norm: 0.00011149942151511419\n",
            "iter: 66  x: [-99.99995581  24.99999397]  f(x): 1.989139359193299e-09  grad at x: [ 8.83815924e-05 -1.20520353e-05]  gradient norm: 8.91995372004429e-05\n",
            "iter: 67  x: [-100.00003535   25.00000482]  f(x): 1.2730491896964565e-09  grad at x: [-7.07052739e-05  9.64162827e-06]  gradient norm: 7.135962975510611e-05\n",
            "iter: 68  x: [-99.99997172  24.99999614]  f(x): 8.147514817327432e-10  grad at x: [ 5.65642192e-05 -7.71330262e-06]  gradient norm: 5.7087703815541333e-05\n",
            "iter: 69  x: [-100.00002263   25.00000309]  f(x): 5.214409481891125e-10  grad at x: [-4.52513753e-05  6.17064210e-06]  gradient norm: 4.5670163047184866e-05\n",
            "iter: 70  x: [-99.9999819   24.99999753]  f(x): 3.3372220704681144e-10  grad at x: [ 3.62011003e-05 -4.93651368e-06]  gradient norm: 3.653613044901233e-05\n",
            "iter: 71  x: [-100.00001448   25.00000197]  f(x): 2.1358221267738897e-10  grad at x: [-2.89608802e-05  3.94921094e-06]  gradient norm: 2.9228904370666305e-05\n",
            "iter: 72  x: [-99.99998842  24.99999842]  f(x): 1.3669261617713347e-10  grad at x: [ 2.31687042e-05 -3.15936875e-06]  gradient norm: 2.3383123501973252e-05\n",
            "iter: 73  x: [-100.00000927   25.00000126]  f(x): 8.74832742970941e-11  grad at x: [-1.85349633e-05  2.52749500e-06]  gradient norm: 1.870649879556237e-05\n",
            "iter: 74  x: [-99.99999259  24.99999899]  f(x): 5.598929563155405e-11  grad at x: [ 1.48279707e-05 -2.02199600e-06]  gradient norm: 1.4965199047330317e-05\n",
            "iter: 75  x: [-100.00000593   25.00000081]  f(x): 3.583314913446605e-11  grad at x: [-1.18623765e-05  1.61759679e-06]  gradient norm: 1.1972159226215803e-05\n",
            "iter: 76  x: [-99.99999526  24.99999935]  f(x): 2.293321544605827e-11  grad at x: [ 9.48990123e-06 -1.29407744e-06]  gradient norm: 9.577727380972643e-06\n",
            "Optimizer: [-99.99999526  24.99999935]\n",
            "for step length = 0.9, the minimum value of function is 2.293321544605827e-11 and number of iterations are= 76\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list_of_step_length,Iterations)\n",
        "plt.ylabel('Iterations')\n",
        "plt.xlabel('step_length')\n",
        "plt.ylim([0, 100])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "CTLBEPGLCbmY",
        "outputId": "15741096-7d50-4aab-f5cc-fdd0663f2806"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we obeserved that as we increase the value of step length from 0.0001 to 0.5 the numbers of iterations decreases but after 0.5 it again starts increasing and number of iterations is minimum (i.e. 1) at step length= 0.5"
      ],
      "metadata": {
        "id": "miCt-MWkImYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.5"
      ],
      "metadata": {
        "id": "52ge58HyC6Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_starting_points=[[10000,10000],[500,0],[0,1000],[1,1],[-500,-2]]"
      ],
      "metadata": {
        "id": "U3NkdUE78C8n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_iterations=[]\n",
        "final_optimizer=[]\n",
        "final_minimum_value=[]\n",
        "for x in list_of_starting_points:\n",
        "  my_start_x = np.array(x)\n",
        "  my_steplength = 0.1\n",
        "  my_tol= 1e-5 \n",
        "  print(f\"For starting point= {x}\")\n",
        "  opt_x, fvals_ret,iterations= find_minimizer(my_start_x, my_tol, my_steplength)\n",
        "  print('\\nOptimizer:',opt_x,)\n",
        "  print(f\"for starting point = {x}, the minimum value of function is {evalf(opt_x)} and number of iterations are= {iterations}\\n\")\n",
        "  no_of_iterations.append(iterations)\n",
        "  final_optimizer.append(opt_x)\n",
        "  final_minimum_value.append(evalf(opt_x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWd38zV8SQd",
        "outputId": "67660eea-b277-4758-92ee-5ca119e42428"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For starting point= [10000, 10000]\n",
            "iter: 0  x: [10000 10000]  f(x): 201510625  grad at x: [20200 19950]  gradient norm: 28390.887622615817\n",
            "iter: 1  x: [7980. 8005.]  f(x): 128966800.0  grad at x: [16160. 15960.]  gradient norm: 22712.710098092655\n",
            "iter: 2  x: [6364. 6409.]  f(x): 82538752.0  grad at x: [12928. 12768.]  gradient norm: 18170.168078474122\n",
            "iter: 3  x: [5071.2 5132.2]  f(x): 52824801.28  grad at x: [10342.4 10214.4]  gradient norm: 14536.134462779297\n",
            "iter: 4  x: [4036.96 4110.76]  f(x): 33807872.8192  grad at x: [8273.92 8171.52]  gradient norm: 11628.907570223439\n",
            "iter: 5  x: [3209.568 3293.608]  f(x): 21637038.604288004  grad at x: [6619.136 6537.216]  gradient norm: 9303.126056178751\n",
            "iter: 6  x: [2547.6544 2639.8864]  f(x): 13847704.706744324  grad at x: [5295.3088 5229.7728]  gradient norm: 7442.5008449430015\n",
            "iter: 7  x: [2018.12352 2116.90912]  f(x): 8862531.012316367  grad at x: [4236.24704 4183.81824]  gradient norm: 5954.000675954401\n",
            "iter: 8  x: [1594.498816 1698.527296]  f(x): 5672019.847882474  grad at x: [3388.997632 3347.054592]  gradient norm: 4763.200540763521\n",
            "iter: 9  x: [1255.5990528 1363.8218368]  f(x): 3630092.7026447835  grad at x: [2711.1981056 2677.6436736]  gradient norm: 3810.560432610817\n",
            "iter: 10  x: [ 984.47924224 1096.05746944]  f(x): 2323259.3296926618  grad at x: [2168.95848448 2142.11493888]  gradient norm: 3048.4483460886536\n",
            "iter: 11  x: [767.58339379 881.84597555]  f(x): 1486885.9710033033  grad at x: [1735.16678758 1713.6919511 ]  gradient norm: 2438.7586768709225\n",
            "iter: 12  x: [594.06671503 710.47678044]  f(x): 951607.0214421141  grad at x: [1388.13343007 1370.95356088]  gradient norm: 1951.0069414967381\n",
            "iter: 13  x: [455.25337203 573.38142435]  f(x): 609028.4937229531  grad at x: [1110.50674405 1096.76284871]  gradient norm: 1560.8055531973905\n",
            "iter: 14  x: [344.20269762 463.70513948]  f(x): 389778.2359826899  grad at x: [888.40539524 877.41027897]  gradient norm: 1248.6444425579125\n",
            "iter: 15  x: [255.3621581  375.96411159]  f(x): 249458.07102892155  grad at x: [710.72431619 701.92822317]  gradient norm: 998.9155540463299\n",
            "iter: 16  x: [184.28972648 305.77128927]  f(x): 159653.16545850981  grad at x: [568.57945296 561.54257854]  gradient norm: 799.1324432370639\n",
            "iter: 17  x: [127.43178118 249.61703142]  f(x): 102178.02589344625  grad at x: [454.86356236 449.23406283]  gradient norm: 639.305954589651\n",
            "iter: 18  x: [ 81.94542495 204.69362513]  f(x): 65393.93657180561  grad at x: [363.89084989 359.38725026]  gradient norm: 511.44476367172086\n",
            "iter: 19  x: [ 45.55633996 168.75490011]  f(x): 41852.119405955586  grad at x: [291.11267991 287.50980021]  gradient norm: 409.1558109373767\n",
            "iter: 20  x: [ 16.44507197 140.00392008]  f(x): 26785.35641981157  grad at x: [232.89014393 230.00784017]  gradient norm: 327.3246487499013\n",
            "iter: 21  x: [ -6.84394243 117.00313607]  f(x): 17142.628108679404  grad at x: [186.31211514 184.00627214]  gradient norm: 261.85971899992103\n",
            "iter: 22  x: [-25.47515394  98.60250885]  f(x): 10971.281989554818  grad at x: [149.04969212 147.20501771]  gradient norm: 209.48777519993683\n",
            "iter: 23  x: [-40.38012315  83.88200708]  f(x): 7021.620473315084  grad at x: [119.23975369 117.76401417]  gradient norm: 167.5902201599495\n",
            "iter: 24  x: [-52.30409852  72.10560567]  f(x): 4493.837102921654  grad at x: [95.39180295 94.21121133]  gradient norm: 134.07217612795958\n",
            "iter: 25  x: [-61.84327882  62.68448453]  f(x): 2876.0557458698577  grad at x: [76.31344236 75.36896907]  gradient norm: 107.25774090236766\n",
            "iter: 26  x: [-69.47462305  55.14758763]  f(x): 1840.6756773567088  grad at x: [61.05075389 60.29517525]  gradient norm: 85.80619272189412\n",
            "iter: 27  x: [-75.57969844  49.1180701 ]  f(x): 1178.0324335082935  grad at x: [48.84060311 48.2361402 ]  gradient norm: 68.64495417751529\n",
            "iter: 28  x: [-80.46375876  44.29445608]  f(x): 753.9407574453076  grad at x: [39.07248249 38.58891216]  gradient norm: 54.915963342012226\n",
            "iter: 29  x: [-84.371007    40.43556486]  f(x): 482.5220847649968  grad at x: [31.25798599 30.87112973]  gradient norm: 43.932770673609774\n",
            "iter: 30  x: [-87.4968056   37.34845189]  f(x): 308.81413424959817  grad at x: [25.00638879 24.69690378]  gradient norm: 35.14621653888783\n",
            "iter: 31  x: [-89.99744448  34.87876151]  f(x): 197.64104591974274  grad at x: [20.00511103 19.75752303]  gradient norm: 28.11697323111026\n",
            "iter: 32  x: [-91.99795559  32.90300921]  f(x): 126.49026938863543  grad at x: [16.00408883 15.80601842]  gradient norm: 22.493578584888215\n",
            "iter: 33  x: [-93.59836447  31.32240737]  f(x): 80.95377240872659  grad at x: [12.80327106 12.64481474]  gradient norm: 17.994862867910566\n",
            "iter: 34  x: [-94.87869158  30.05792589]  f(x): 51.81041434158496  grad at x: [10.24261685 10.11585179]  gradient norm: 14.395890294328442\n",
            "iter: 35  x: [-95.90295326  29.04634072]  f(x): 33.158665178614356  grad at x: [8.19409348 8.09268143]  gradient norm: 11.51671223546275\n",
            "iter: 36  x: [-96.72236261  28.23707257]  f(x): 21.22154571431315  grad at x: [6.55527478 6.47414515]  gradient norm: 9.213369788370192\n",
            "iter: 37  x: [-97.37789009  27.58965806]  f(x): 13.581789257160448  grad at x: [5.24421983 5.17931612]  gradient norm: 7.370695830696162\n",
            "iter: 38  x: [-97.90231207  27.07172645]  f(x): 8.692345124582692  grad at x: [4.19537586 4.14345289]  gradient norm: 5.896556664556932\n",
            "iter: 39  x: [-98.32184966  26.65738116]  f(x): 5.563100879732932  grad at x: [3.35630069 3.31476231]  gradient norm: 4.717245331645549\n",
            "iter: 40  x: [-98.65747972  26.32590493]  f(x): 3.56038456302909  grad at x: [2.68504055 2.65180985]  gradient norm: 3.7737962653164465\n",
            "iter: 41  x: [-98.92598378  26.06072394]  f(x): 2.2786461203386166  grad at x: [2.14803244 2.12144788]  gradient norm: 3.0190370122531562\n",
            "iter: 42  x: [-99.14078702  25.84857915]  f(x): 1.4583335170167193  grad at x: [1.71842595 1.69715831]  gradient norm: 2.415229609802529\n",
            "iter: 43  x: [-99.31262962  25.67886332]  f(x): 0.9333334508906994  grad at x: [1.37474076 1.35772664]  gradient norm: 1.9321836878420222\n",
            "iter: 44  x: [-99.4501037   25.54309066]  f(x): 0.5973334085700476  grad at x: [1.09979261 1.08618132]  gradient norm: 1.5457469502736179\n",
            "iter: 45  x: [-99.56008296  25.43447253]  f(x): 0.38229338148482916  grad at x: [0.87983409 0.86894505]  gradient norm: 1.2365975602188921\n",
            "iter: 46  x: [-99.64806636  25.34757802]  f(x): 0.2446677641502917  grad at x: [0.70386727 0.69515604]  gradient norm: 0.9892780481751159\n",
            "iter: 47  x: [-99.71845309  25.27806242]  f(x): 0.1565873690561827  grad at x: [0.56309382 0.55612483]  gradient norm: 0.7914224385400825\n",
            "iter: 48  x: [-99.77476247  25.22244993]  f(x): 0.10021591619595627  grad at x: [0.45047505 0.44489987]  gradient norm: 0.633137950832064\n",
            "iter: 49  x: [-99.81980998  25.17795995]  f(x): 0.06413818636541355  grad at x: [0.36038004 0.35591989]  gradient norm: 0.5065103606656572\n",
            "iter: 50  x: [-99.85584798  25.14236796]  f(x): 0.041048439273865686  grad at x: [0.28830403 0.28473591]  gradient norm: 0.4052082885325308\n",
            "iter: 51  x: [-99.88467839  25.11389437]  f(x): 0.026271001135273885  grad at x: [0.23064323 0.22778873]  gradient norm: 0.3241666308260237\n",
            "iter: 52  x: [-99.90774271  25.09111549]  f(x): 0.016813440726574494  grad at x: [0.18451458 0.18223099]  gradient norm: 0.25933330466081284\n",
            "iter: 53  x: [-99.92619417  25.07289239]  f(x): 0.010760602065007993  grad at x: [0.14761167 0.14578479]  gradient norm: 0.20746664372865334\n",
            "iter: 54  x: [-99.94095533  25.05831392]  f(x): 0.006886785321605032  grad at x: [0.11808933 0.11662783]  gradient norm: 0.16597331498292167\n",
            "iter: 55  x: [-99.95276427  25.04665113]  f(x): 0.004407542605827423  grad at x: [0.09447147 0.09330226]  gradient norm: 0.13277865198634037\n",
            "iter: 56  x: [-99.96221141  25.03732091]  f(x): 0.00282082726772923  grad at x: [0.07557717 0.07464181]  gradient norm: 0.10622292158906625\n",
            "iter: 57  x: [-99.96976913  25.02985672]  f(x): 0.001805329451346794  grad at x: [0.06046174 0.05971345]  gradient norm: 0.08497833727125506\n",
            "iter: 58  x: [-99.9758153   25.02388538]  f(x): 0.0011554108488619481  grad at x: [0.04836939 0.04777076]  gradient norm: 0.06798266981700404\n",
            "iter: 59  x: [-99.98065224  25.0191083 ]  f(x): 0.0007394629432715368  grad at x: [0.03869551 0.03821661]  gradient norm: 0.054386135853599187\n",
            "iter: 60  x: [-99.9845218   25.01528664]  f(x): 0.00047325628369389327  grad at x: [0.03095641 0.03057329]  gradient norm: 0.043508908682884394\n",
            "iter: 61  x: [-99.98761744  25.01222931]  f(x): 0.0003028840215639857  grad at x: [0.02476513 0.02445863]  gradient norm: 0.03480712694630143\n",
            "iter: 62  x: [-99.99009395  25.00978345]  f(x): 0.00019384577380109126  grad at x: [0.0198121 0.0195669]  gradient norm: 0.027845701557051225\n",
            "iter: 63  x: [-99.99207516  25.00782676]  f(x): 0.00012406129523274346  grad at x: [0.01584968 0.01565352]  gradient norm: 0.022276561245645025\n",
            "iter: 64  x: [-99.99366013  25.00626141]  f(x): 7.939922894900075e-05  grad at x: [0.01267975 0.01252282]  gradient norm: 0.017821248996521065\n",
            "iter: 65  x: [-99.9949281   25.00500913]  f(x): 5.081550652743238e-05  grad at x: [0.0101438  0.01001825]  gradient norm: 0.014256999197226937\n",
            "iter: 66  x: [-99.99594248  25.0040073 ]  f(x): 3.252192417757409e-05  grad at x: [0.00811504 0.0080146 ]  gradient norm: 0.011405599357784594\n",
            "iter: 67  x: [-99.99675399  25.00320584]  f(x): 2.0814031473628968e-05  grad at x: [0.00649203 0.00641168]  gradient norm: 0.009124479486223632\n",
            "iter: 68  x: [-99.99740319  25.00256467]  f(x): 1.3320980143118893e-05  grad at x: [0.00519362 0.00512935]  gradient norm: 0.007299583588977907\n",
            "iter: 69  x: [-99.99792255  25.00205174]  f(x): 8.525427291569559e-06  grad at x: [0.0041549  0.00410348]  gradient norm: 0.005839666871173238\n",
            "iter: 70  x: [-99.99833804  25.00164139]  f(x): 5.456273466625745e-06  grad at x: [0.00332392 0.00328278]  gradient norm: 0.004671733496947677\n",
            "iter: 71  x: [-99.99867043  25.00131311]  f(x): 3.4920150186442086e-06  grad at x: [0.00265914 0.00262623]  gradient norm: 0.0037373867975601396\n",
            "iter: 72  x: [-99.99893635  25.00105049]  f(x): 2.23488961192774e-06  grad at x: [0.00212731 0.00210098]  gradient norm: 0.0029899094380450657\n",
            "iter: 73  x: [-99.99914908  25.00084039]  f(x): 1.4303293516264683e-06  grad at x: [0.00170185 0.00168078]  gradient norm: 0.002391927550429961\n",
            "iter: 74  x: [-99.99931926  25.00067231]  f(x): 9.1541078504672e-07  grad at x: [0.00136148 0.00134463]  gradient norm: 0.0019135420403500103\n",
            "iter: 75  x: [-99.99945541  25.00053785]  f(x): 5.858629024268052e-07  grad at x: [0.00108918 0.0010757 ]  gradient norm: 0.001530833632275964\n",
            "iter: 76  x: [-99.99956433  25.00043028]  f(x): 3.749522575494559e-07  grad at x: [0.00087135 0.00086056]  gradient norm: 0.0012246669058147296\n",
            "iter: 77  x: [-99.99965146  25.00034422]  f(x): 2.399694448316518e-07  grad at x: [0.00069708 0.00068845]  gradient norm: 0.0009797335246517836\n",
            "iter: 78  x: [-99.99972117  25.00027538]  f(x): 1.5358044469067217e-07  grad at x: [0.00055766 0.00055076]  gradient norm: 0.0007837868197173825\n",
            "iter: 79  x: [-99.99977694  25.0002203 ]  f(x): 9.829148460329816e-08  grad at x: [0.00044613 0.00044061]  gradient norm: 0.0006270294557779504\n",
            "iter: 80  x: [-99.99982155  25.00017624]  f(x): 6.290655014509644e-08  grad at x: [0.0003569  0.00035249]  gradient norm: 0.000501623564618316\n",
            "iter: 81  x: [-99.99985724  25.00014099]  f(x): 4.026019209245095e-08  grad at x: [0.00028552 0.00028199]  gradient norm: 0.0004012988516926055\n",
            "iter: 82  x: [-99.99988579  25.0001128 ]  f(x): 2.57665229393289e-08  grad at x: [0.00022842 0.00022559]  gradient norm: 0.000321039081355083\n",
            "iter: 83  x: [-99.99990863  25.00009024]  f(x): 1.6490574680914032e-08  grad at x: [0.00018273 0.00018047]  gradient norm: 0.00025683126508206926\n",
            "iter: 84  x: [-99.99992691  25.00007219]  f(x): 1.0553967795784978e-08  grad at x: [0.00014619 0.00014438]  gradient norm: 0.00020546501206565539\n",
            "iter: 85  x: [-99.99994152  25.00005775]  f(x): 6.754539389220317e-09  grad at x: [0.00011695 0.0001155 ]  gradient norm: 0.00016437200965152573\n",
            "iter: 86  x: [-99.99995322  25.0000462 ]  f(x): 4.3229052096984855e-09  grad at x: [9.35600081e-05 9.24020872e-05]  gradient norm: 0.00013149760773030795\n",
            "iter: 87  x: [-99.99996258  25.00003696]  f(x): 2.76665933367652e-09  grad at x: [7.48480065e-05 7.39216698e-05]  gradient norm: 0.00010519808617416041\n",
            "iter: 88  x: [-99.99997006  25.00002957]  f(x): 1.7706619734689337e-09  grad at x: [5.98784052e-05 5.91373358e-05]  gradient norm: 8.415846893733116e-05\n",
            "iter: 89  x: [-99.99997605  25.00002365]  f(x): 1.1332236627478221e-09  grad at x: [4.79027241e-05 4.73098686e-05]  gradient norm: 6.732677514177616e-05\n",
            "iter: 90  x: [-99.99998084  25.00001892]  f(x): 7.25263144104821e-10  grad at x: [3.83221793e-05 3.78478949e-05]  gradient norm: 5.386142011142376e-05\n",
            "iter: 91  x: [-99.99998467  25.00001514]  f(x): 4.6416841231422e-10  grad at x: [3.06577435e-05 3.02783159e-05]  gradient norm: 4.3089136093183395e-05\n",
            "iter: 92  x: [-99.99998774  25.00001211]  f(x): 2.9706778375889676e-10  grad at x: [2.45261947e-05 2.42226527e-05]  gradient norm: 3.447130886745653e-05\n",
            "iter: 93  x: [-99.99999019  25.00000969]  f(x): 1.9012338170345716e-10  grad at x: [1.96209558e-05 1.93781222e-05]  gradient norm: 2.757704710105541e-05\n",
            "iter: 94  x: [-99.99999215  25.00000775]  f(x): 1.2167896431224296e-10  grad at x: [1.56967646e-05 1.55024978e-05]  gradient norm: 2.2061637682841496e-05\n",
            "iter: 95  x: [-99.99999372  25.0000062 ]  f(x): 7.787453707083057e-11  grad at x: [1.25574117e-05 1.24019982e-05]  gradient norm: 1.764931013618726e-05\n",
            "iter: 96  x: [-99.99999498  25.00000496]  f(x): 4.9839703739784375e-11  grad at x: [1.00459294e-05 9.92159856e-06]  gradient norm: 1.4119448110997027e-05\n",
            "iter: 97  x: [-99.99999598  25.00000397]  f(x): 3.189741045042515e-11  grad at x: [8.03674351e-06 7.93727885e-06]  gradient norm: 1.1295558498883559e-05\n",
            "iter: 98  x: [-99.99999679  25.00000317]  f(x): 2.0414342669998657e-11  grad at x: [6.42939480e-06 6.34982308e-06]  gradient norm: 9.036446795062461e-06\n",
            "\n",
            "Optimizer: [-99.99999679  25.00000317]\n",
            "for starting point = [10000, 10000], the minimum value of function is 2.0414342669998657e-11 and number of iterations are= 98\n",
            "\n",
            "For starting point= [500, 0]\n",
            "iter: 0  x: [500   0]  f(x): 360625  grad at x: [1200  -50]  gradient norm: 1201.0412149464314\n",
            "iter: 1  x: [380.   5.]  f(x): 230800.0  grad at x: [960. -40.]  gradient norm: 960.8329719571451\n",
            "iter: 2  x: [284.   9.]  f(x): 147712.0  grad at x: [768. -32.]  gradient norm: 768.6663775657161\n",
            "iter: 3  x: [207.2  12.2]  f(x): 94535.68  grad at x: [614.4 -25.6]  gradient norm: 614.9331020525728\n",
            "iter: 4  x: [145.76  14.76]  f(x): 60502.8352  grad at x: [491.52 -20.48]  gradient norm: 491.9464816420583\n",
            "iter: 5  x: [96.608 16.808]  f(x): 38721.814528  grad at x: [393.216 -16.384]  gradient norm: 393.55718531364664\n",
            "iter: 6  x: [57.2864 18.4464]  f(x): 24781.961297919996  grad at x: [314.5728 -13.1072]  gradient norm: 314.84574825091727\n",
            "iter: 7  x: [25.82912 19.75712]  f(x): 15860.455230668798  grad at x: [251.65824 -10.48576]  gradient norm: 251.87659860073384\n",
            "iter: 8  x: [ 0.663296 20.805696]  f(x): 10150.69134762803  grad at x: [201.326592  -8.388608]  gradient norm: 201.50127888058705\n",
            "iter: 9  x: [-19.4693632  21.6445568]  f(x): 6496.44246248194  grad at x: [161.0612736  -6.7108864]  gradient norm: 161.20102310446967\n",
            "iter: 10  x: [-35.57549056  22.31564544]  f(x): 4157.723175988441  grad at x: [128.84901888  -5.36870912]  gradient norm: 128.9608184835757\n",
            "iter: 11  x: [-48.46039245  22.85251635]  f(x): 2660.942832632602  grad at x: [103.0792151  -4.2949673]  gradient norm: 103.16865478686057\n",
            "iter: 12  x: [-58.76831396  23.28201308]  f(x): 1703.0034128848652  grad at x: [82.46337208 -3.43597384]  gradient norm: 82.53492382948845\n",
            "iter: 13  x: [-67.01465117  23.62561047]  f(x): 1089.9221842463141  grad at x: [65.97069767 -2.74877907]  gradient norm: 66.02793906359078\n",
            "iter: 14  x: [-73.61172093  23.90048837]  f(x): 697.5501979176407  grad at x: [52.77655813 -2.19902326]  gradient norm: 52.82235125087261\n",
            "iter: 15  x: [-78.88937675  24.1203907 ]  f(x): 446.4321266672901  grad at x: [42.22124651 -1.7592186 ]  gradient norm: 42.257881000698085\n",
            "iter: 16  x: [-83.1115014   24.29631256]  f(x): 285.71656106706575  grad at x: [33.77699721 -1.40737488]  gradient norm: 33.80630480055847\n",
            "iter: 17  x: [-86.48920112  24.43705005]  f(x): 182.85859908292207  grad at x: [27.02159776 -1.12589991]  gradient norm: 27.04504384044678\n",
            "iter: 18  x: [-89.19136089  24.54964004]  f(x): 117.02950341307026  grad at x: [21.61727821 -0.90071993]  gradient norm: 21.636035072357437\n",
            "iter: 19  x: [-91.35308872  24.63971203]  f(x): 74.89888218436487  grad at x: [17.29382257 -0.72057594]  gradient norm: 17.308828057885936\n",
            "iter: 20  x: [-93.08247097  24.71176962]  f(x): 47.93528459799351  grad at x: [13.83505806 -0.57646075]  gradient norm: 13.84706244630875\n",
            "iter: 21  x: [-94.46597678  24.7694157 ]  f(x): 30.678582142715847  grad at x: [11.06804644 -0.4611686 ]  gradient norm: 11.077649957046999\n",
            "iter: 22  x: [-95.57278142  24.81553256]  f(x): 19.634292571338193  grad at x: [ 8.85443716 -0.36893488]  gradient norm: 8.86211996563761\n",
            "iter: 23  x: [-96.45822514  24.85242605]  f(x): 12.565947245656442  grad at x: [ 7.08354972 -0.29514791]  gradient norm: 7.089695972510088\n",
            "iter: 24  x: [-97.16658011  24.88194084]  f(x): 8.042206237220155  grad at x: [ 5.66683978 -0.23611832]  gradient norm: 5.671756778008082\n",
            "iter: 25  x: [-97.73326409  24.90555267]  f(x): 5.147011991820873  grad at x: [ 4.53347182 -0.18889466]  gradient norm: 4.537405422406454\n",
            "iter: 26  x: [-98.18661127  24.92444214]  f(x): 3.294087674765359  grad at x: [ 3.62677746 -0.15111573]  gradient norm: 3.6299243379251633\n",
            "iter: 27  x: [-98.54928902  24.93955371]  f(x): 2.108216111849846  grad at x: [ 2.90142197 -0.12089258]  gradient norm: 2.9039394703401418\n",
            "iter: 28  x: [-98.83943121  24.95164297]  f(x): 1.3492583115839147  grad at x: [ 2.32113757 -0.09671407]  gradient norm: 2.323151576272125\n",
            "iter: 29  x: [-99.07154497  24.96131437]  f(x): 0.8635253194137108  grad at x: [ 1.85691006 -0.07737125]  gradient norm: 1.8585212610177058\n",
            "iter: 30  x: [-99.25723598  24.9690515 ]  f(x): 0.5526562044247791  grad at x: [ 1.48552805 -0.061897  ]  gradient norm: 1.4868170088141703\n",
            "iter: 31  x: [-99.40578878  24.9752412 ]  f(x): 0.35369997083186544  grad at x: [ 1.18842244 -0.0495176 ]  gradient norm: 1.1894536070513477\n",
            "iter: 32  x: [-99.52463102  24.98019296]  f(x): 0.22636798133239663  grad at x: [ 0.95073795 -0.03961408]  gradient norm: 0.9515628856410839\n",
            "iter: 33  x: [-99.61970482  24.98415437]  f(x): 0.1448755080527381  grad at x: [ 0.76059036 -0.03169127]  gradient norm: 0.7612503085128783\n",
            "iter: 34  x: [-99.69576386  24.98732349]  f(x): 0.09272032515374895  grad at x: [ 0.60847229 -0.02535301]  gradient norm: 0.6090002468102914\n",
            "iter: 35  x: [-99.75661108  24.9898588 ]  f(x): 0.059341008098399324  grad at x: [ 0.48677783 -0.02028241]  gradient norm: 0.48720019744823306\n",
            "iter: 36  x: [-99.80528887  24.99188704]  f(x): 0.03797824518297781  grad at x: [ 0.38942226 -0.01622593]  gradient norm: 0.389760157958598\n",
            "iter: 37  x: [-99.84423109  24.99350963]  f(x): 0.024306076917104028  grad at x: [ 0.31153781 -0.01298074]  gradient norm: 0.311808126366867\n",
            "iter: 38  x: [-99.87538488  24.9948077 ]  f(x): 0.015555889226947986  grad at x: [ 0.24923025 -0.01038459]  gradient norm: 0.2494465010935049\n",
            "iter: 39  x: [-99.9003079   24.99584616]  f(x): 0.009955769105247283  grad at x: [ 0.1993842  -0.00830767]  gradient norm: 0.19955720087480966\n",
            "iter: 40  x: [-99.92024632  24.99667693]  f(x): 0.006371692227357809  grad at x: [ 0.15950736 -0.00664614]  gradient norm: 0.15964576069984207\n",
            "iter: 41  x: [-99.93619706  24.99734154]  f(x): 0.004077883025509368  grad at x: [ 0.12760589 -0.00531691]  gradient norm: 0.12771660855987946\n",
            "iter: 42  x: [-99.94895764  24.99787324]  f(x): 0.0026098451363262823  grad at x: [ 0.10208471 -0.00425353]  gradient norm: 0.10217328684790918\n",
            "iter: 43  x: [-99.95916612  24.99829859]  f(x): 0.0016703008872488232  grad at x: [ 0.08166777 -0.00340282]  gradient norm: 0.0817386294783274\n",
            "iter: 44  x: [-99.96733289  24.99863887]  f(x): 0.001068992567839063  grad at x: [ 0.06533421 -0.00272226]  gradient norm: 0.0653909035826563\n",
            "iter: 45  x: [-99.97386631  24.9989111 ]  f(x): 0.0006841552434168549  grad at x: [ 0.05226737 -0.00217781]  gradient norm: 0.05231272286611948\n",
            "iter: 46  x: [-99.97909305  24.99912888]  f(x): 0.0004378593557866671  grad at x: [ 0.0418139  -0.00174225]  gradient norm: 0.041850178292889846\n",
            "iter: 47  x: [-99.98327444  24.9993031 ]  f(x): 0.000280229987703561  grad at x: [ 0.03345112 -0.0013938 ]  gradient norm: 0.03348014263431749\n",
            "iter: 48  x: [-99.98661955  24.99944248]  f(x): 0.00017934719213027981  grad at x: [ 0.02676089 -0.00111504]  gradient norm: 0.026784114107454054\n",
            "iter: 49  x: [-99.98929564  24.99955399]  f(x): 0.00011478220296325739  grad at x: [ 0.02140872 -0.00089203]  gradient norm: 0.021427291285951885\n",
            "iter: 50  x: [-99.99143651  24.99964319]  f(x): 7.346060989653238e-05  grad at x: [ 0.01712697 -0.00071362]  gradient norm: 0.017141833028767067\n",
            "iter: 51  x: [-99.99314921  24.99971455]  f(x): 4.701479033385902e-05  grad at x: [ 0.01370158 -0.0005709 ]  gradient norm: 0.013713466423025072\n",
            "iter: 52  x: [-99.99451937  24.99977164]  f(x): 3.0089465813606816e-05  grad at x: [ 0.01096126 -0.00045672]  gradient norm: 0.010970773138408581\n",
            "iter: 53  x: [-99.9956155   24.99981731]  f(x): 1.925725812075821e-05  grad at x: [ 0.00876901 -0.00036538]  gradient norm: 0.008776618510738224\n",
            "iter: 54  x: [-99.9964924   24.99985385]  f(x): 1.2324645197245585e-05  grad at x: [ 0.00701521 -0.0002923 ]  gradient norm: 0.007021294808579279\n",
            "iter: 55  x: [-99.99719392  24.99988308]  f(x): 7.887772926204942e-06  grad at x: [ 0.00561217 -0.00023384]  gradient norm: 0.0056170358468519465\n",
            "iter: 56  x: [-99.99775513  24.99990646]  f(x): 5.048174672745773e-06  grad at x: [ 0.00448973 -0.00018707]  gradient norm: 0.004493628677470257\n",
            "iter: 57  x: [-99.99820411  24.99992517]  f(x): 3.2308317905779246e-06  grad at x: [ 0.00359179 -0.00014966]  gradient norm: 0.003594902941987683\n",
            "iter: 58  x: [-99.99856329  24.99994014]  f(x): 2.0677323459861205e-06  grad at x: [ 0.00287343 -0.00011973]  gradient norm: 0.0028759223536014462\n",
            "iter: 59  x: [-99.99885063  24.99995211]  f(x): 1.32334870144432e-06  grad at x: [ 2.29874331e-03 -9.57809713e-05]  gradient norm: 0.002300737882892634\n",
            "iter: 60  x: [-99.9990805   24.99996169]  f(x): 8.469431689347637e-07  grad at x: [ 1.83899465e-03 -7.66247770e-05]  gradient norm: 0.001840590306325407\n",
            "iter: 61  x: [-99.9992644   24.99996935]  f(x): 5.420436281266987e-07  grad at x: [ 1.47119572e-03 -6.12998216e-05]  gradient norm: 0.0014724722450718025\n",
            "iter: 62  x: [-99.99941152  24.99997548]  f(x): 3.469079220077426e-07  grad at x: [ 1.17695658e-03 -4.90398573e-05]  gradient norm: 0.001177977796068742\n",
            "iter: 63  x: [-99.99952922  24.99998038]  f(x): 2.2202107009036318e-07  grad at x: [ 9.41565260e-04 -3.92318859e-05]  gradient norm: 0.0009423822368664706\n",
            "iter: 64  x: [-99.99962337  24.99998431]  f(x): 1.420934848599287e-07  grad at x: [ 7.53252208e-04 -3.13855087e-05]  gradient norm: 0.0007539057894987376\n",
            "iter: 65  x: [-99.9996987   24.99998745]  f(x): 9.093983030696466e-08  grad at x: [ 6.02601767e-04 -2.51084069e-05]  gradient norm: 0.0006031246315877496\n",
            "iter: 66  x: [-99.99975896  24.99998996]  f(x): 5.820149139371707e-08  grad at x: [ 4.82081413e-04 -2.00867256e-05]  gradient norm: 0.0004824997052588408\n",
            "iter: 67  x: [-99.99980717  24.99999197]  f(x): 3.724895449415975e-08  grad at x: [ 3.85665131e-04 -1.60693804e-05]  gradient norm: 0.00038599976421837233\n",
            "iter: 68  x: [-99.99984573  24.99999357]  f(x): 2.3839330874490174e-08  grad at x: [ 3.08532104e-04 -1.28555044e-05]  gradient norm: 0.0003087998113632207\n",
            "iter: 69  x: [-99.99987659  24.99999486]  f(x): 1.5257171760367925e-08  grad at x: [ 2.46825684e-04 -1.02844035e-05]  gradient norm: 0.00024703984909619684\n",
            "iter: 70  x: [-99.99990127  24.99999589]  f(x): 9.764589925513038e-09  grad at x: [ 1.97460547e-04 -8.22752278e-06]  gradient norm: 0.00019763187926559862\n",
            "iter: 71  x: [-99.99992102  24.99999671]  f(x): 6.249337551888725e-09  grad at x: [ 1.57968437e-04 -6.58201823e-06]  gradient norm: 0.00015810550340691781\n",
            "iter: 72  x: [-99.99993681  24.99999737]  f(x): 3.9995760324866855e-09  grad at x: [ 1.26374750e-04 -5.26561458e-06]  gradient norm: 0.00012648440271411626\n",
            "iter: 73  x: [-99.99994945  24.99999789]  f(x): 2.559728661081814e-09  grad at x: [ 1.01099800e-04 -4.21249167e-06]  gradient norm: 0.00010118752217703157\n",
            "iter: 74  x: [-99.99995956  24.99999832]  f(x): 1.6382263433270247e-09  grad at x: [ 8.08798400e-05 -3.36999334e-06]  gradient norm: 8.0950017747423e-05\n",
            "iter: 75  x: [-99.99996765  24.99999865]  f(x): 1.0484648597254646e-09  grad at x: [ 6.47038720e-05 -2.69599467e-06]  gradient norm: 6.476001419782008e-05\n",
            "iter: 76  x: [-99.99997412  24.99999892]  f(x): 6.710175099269931e-10  grad at x: [ 5.17630976e-05 -2.15679573e-06]  gradient norm: 5.180801134677891e-05\n",
            "iter: 77  x: [-99.99997929  24.99999914]  f(x): 4.294512064697453e-10  grad at x: [ 4.14104781e-05 -1.72543658e-06]  gradient norm: 4.144640908304338e-05\n",
            "iter: 78  x: [-99.99998344  24.99999931]  f(x): 2.7484877204746127e-10  grad at x: [ 3.31283825e-05 -1.38034927e-06]  gradient norm: 3.315712726081446e-05\n",
            "iter: 79  x: [-99.99998675  24.99999945]  f(x): 1.7590321410959056e-10  grad at x: [ 2.65027060e-05 -1.10427941e-06]  gradient norm: 2.6525701808592402e-05\n",
            "iter: 80  x: [-99.9999894   24.99999956]  f(x): 1.1257805691024533e-10  grad at x: [ 2.12021648e-05 -8.83423532e-07]  gradient norm: 2.1220561435574257e-05\n",
            "iter: 81  x: [-99.99999152  24.99999965]  f(x): 7.20499563738467e-11  grad at x: [ 1.69617318e-05 -7.06738824e-07]  gradient norm: 1.697644914272083e-05\n",
            "iter: 82  x: [-99.99999322  24.99999972]  f(x): 4.611197200293233e-11  grad at x: [ 1.35693854e-05 -5.65391062e-07]  gradient norm: 1.3581159302936156e-05\n",
            "iter: 83  x: [-99.99999457  24.99999977]  f(x): 2.951166211208713e-11  grad at x: [ 1.08555084e-05 -4.52312847e-07]  gradient norm: 1.0864927447910018e-05\n",
            "iter: 84  x: [-99.99999566  24.99999982]  f(x): 1.888746375147865e-11  grad at x: [ 8.68440668e-06 -3.61850276e-07]  gradient norm: 8.691941958268854e-06\n",
            "\n",
            "Optimizer: [-99.99999566  24.99999982]\n",
            "for starting point = [500, 0], the minimum value of function is 1.888746375147865e-11 and number of iterations are= 84\n",
            "\n",
            "For starting point= [0, 1000]\n",
            "iter: 0  x: [   0 1000]  f(x): 960625  grad at x: [ 200 1950]  gradient norm: 1960.229578391266\n",
            "iter: 1  x: [-20. 805.]  f(x): 614800.0  grad at x: [ 160. 1560.]  gradient norm: 1568.1836627130126\n",
            "iter: 2  x: [-36. 649.]  f(x): 393472.0  grad at x: [ 128. 1248.]  gradient norm: 1254.5469301704102\n",
            "iter: 3  x: [-48.8 524.2]  f(x): 251822.08000000005  grad at x: [102.4 998.4]  gradient norm: 1003.6375441363282\n",
            "iter: 4  x: [-59.04 424.36]  f(x): 161166.1312  grad at x: [ 81.92 798.72]  gradient norm: 802.9100353090626\n",
            "iter: 5  x: [-67.232 344.488]  f(x): 103146.323968  grad at x: [ 65.536 638.976]  gradient norm: 642.32802824725\n",
            "iter: 6  x: [-73.7856 280.5904]  f(x): 66013.64733951999  grad at x: [ 52.4288 511.1808]  gradient norm: 513.8624225978\n",
            "iter: 7  x: [-79.02848 229.47232]  f(x): 42248.734297292795  grad at x: [ 41.94304 408.94464]  gradient norm: 411.08993807824004\n",
            "iter: 8  x: [-83.222784 188.577856]  f(x): 27039.18995026739  grad at x: [ 33.554432 327.155712]  gradient norm: 328.871950462592\n",
            "iter: 9  x: [-86.5782272 155.8622848]  f(x): 17305.08156817113  grad at x: [ 26.8435456 261.7245696]  gradient norm: 263.0975603700736\n",
            "iter: 10  x: [-89.26258176 129.68982784]  f(x): 11075.252203629521  grad at x: [ 21.47483648 209.37965568]  gradient norm: 210.47804829605886\n",
            "iter: 11  x: [-91.41006541 108.75186227]  f(x): 7088.161410322895  grad at x: [ 17.17986918 167.50372454]  gradient norm: 168.3824386368471\n",
            "iter: 12  x: [-93.12805233  92.00148982]  f(x): 4536.423302606653  grad at x: [ 13.74389535 134.00297964]  gradient norm: 134.70595090947768\n",
            "iter: 13  x: [-94.50244186  78.60119185]  f(x): 2903.310913668258  grad at x: [ 10.99511628 107.20238371]  gradient norm: 107.76476072758214\n",
            "iter: 14  x: [-95.60195349  67.88095348]  f(x): 1858.1189847476846  grad at x: [ 8.79609302 85.76190697]  gradient norm: 86.2118085820657\n",
            "iter: 15  x: [-96.48156279  59.30476279]  f(x): 1189.196150238518  grad at x: [ 7.03687442 68.60952557]  gradient norm: 68.96944686565256\n",
            "iter: 16  x: [-97.18525023  52.44381023]  f(x): 761.0855361526516  grad at x: [ 5.62949953 54.88762046]  gradient norm: 55.17555749252205\n",
            "iter: 17  x: [-97.74820019  46.95504818]  f(x): 487.09474313769715  grad at x: [ 4.50359963 43.91009637]  gradient norm: 44.14044599401765\n",
            "iter: 18  x: [-98.19856015  42.56403855]  f(x): 311.74063560812607  grad at x: [ 3.6028797  35.12807709]  gradient norm: 35.312356795214114\n",
            "iter: 19  x: [-98.55884812  39.05123084]  f(x): 199.51400678920078  grad at x: [ 2.88230376 28.10246167]  gradient norm: 28.249885436171297\n",
            "iter: 20  x: [-98.8470785   36.24098467]  f(x): 127.68896434508841  grad at x: [ 2.30584301 22.48196934]  gradient norm: 22.599908348937028\n",
            "iter: 21  x: [-99.0776628   33.99278774]  f(x): 81.72093718085658  grad at x: [ 1.84467441 17.98557547]  gradient norm: 18.079926679149626\n",
            "iter: 22  x: [-99.26213024  32.19423019]  f(x): 52.301399795748196  grad at x: [ 1.47573953 14.38846038]  gradient norm: 14.463941343319696\n",
            "iter: 23  x: [-99.40970419  30.75538415]  f(x): 33.47289586927886  grad at x: [ 1.18059162 11.5107683 ]  gradient norm: 11.57115307465576\n",
            "iter: 24  x: [-99.52776335  29.60430732]  f(x): 21.42265335633847  grad at x: [0.9444733  9.20861464]  gradient norm: 9.256922459724608\n",
            "iter: 25  x: [-99.62221068  28.68344586]  f(x): 13.710498148056615  grad at x: [0.75557864 7.36689171]  gradient norm: 7.405537967779684\n",
            "iter: 26  x: [-99.69776855  27.94675669]  f(x): 8.774718814756225  grad at x: [0.60446291 5.89351337]  gradient norm: 5.9244303742237445\n",
            "iter: 27  x: [-99.75821484  27.35740535]  f(x): 5.615820041443988  grad at x: [0.48357033 4.7148107 ]  gradient norm: 4.739544299378998\n",
            "iter: 28  x: [-99.80657187  26.88592428]  f(x): 3.5941248265241534  grad at x: [0.38685626 3.77184856]  gradient norm: 3.7916354395031986\n",
            "iter: 29  x: [-99.8452575   26.50873942]  f(x): 2.300239888975458  grad at x: [0.30948501 3.01747885]  gradient norm: 3.0333083516025585\n",
            "iter: 30  x: [-99.876206    26.20699154]  f(x): 1.4721535289442946  grad at x: [0.24758801 2.41398308]  gradient norm: 2.4266466812820484\n",
            "iter: 31  x: [-99.9009648   25.96559323]  f(x): 0.9421782585243492  grad at x: [0.19807041 1.93118646]  gradient norm: 1.9413173450256394\n",
            "iter: 32  x: [-99.92077184  25.77247458]  f(x): 0.6029940854555861  grad at x: [0.15845633 1.54494917]  gradient norm: 1.5530538760205148\n",
            "iter: 33  x: [-99.93661747  25.61797967]  f(x): 0.38591621469157356  grad at x: [0.12676506 1.23595934]  gradient norm: 1.2424431008164092\n",
            "iter: 34  x: [-99.94929398  25.49438373]  f(x): 0.24698637740260662  grad at x: [0.10141205 0.98876747]  gradient norm: 0.9939544806531266\n",
            "iter: 35  x: [-99.95943518  25.39550699]  f(x): 0.1580712815376688  grad at x: [0.08112964 0.79101397]  gradient norm: 0.7951635845225027\n",
            "iter: 36  x: [-99.96754814  25.31640559]  f(x): 0.1011656201841084  grad at x: [0.06490371 0.63281118]  gradient norm: 0.6361308676180033\n",
            "iter: 37  x: [-99.97403852  25.25312447]  f(x): 0.0647459969178301  grad at x: [0.05192297 0.50624894]  gradient norm: 0.5089046940944055\n",
            "iter: 38  x: [-99.97923081  25.20249958]  f(x): 0.041437438027411726  grad at x: [0.04153837 0.40499915]  gradient norm: 0.4071237552755266\n",
            "iter: 39  x: [-99.98338465  25.16199966]  f(x): 0.02651996033754383  grad at x: [0.0332307  0.32399932]  gradient norm: 0.3256990042204233\n",
            "iter: 40  x: [-99.98670772  25.12959973]  f(x): 0.016972774616028048  grad at x: [0.02658456 0.25919946]  gradient norm: 0.26055920337633864\n",
            "iter: 41  x: [-99.98936618  25.10367978]  f(x): 0.010862575754258072  grad at x: [0.02126765 0.20735957]  gradient norm: 0.20844736270107206\n",
            "iter: 42  x: [-99.99149294  25.08294383]  f(x): 0.006952048482725  grad at x: [0.01701412 0.16588765]  gradient norm: 0.16675789016085565\n",
            "iter: 43  x: [-99.99319435  25.06635506]  f(x): 0.0044493110289439445  grad at x: [0.01361129 0.13271012]  gradient norm: 0.1334063121286837\n",
            "iter: 44  x: [-99.99455548  25.05308405]  f(x): 0.002847559058524004  grad at x: [0.01088904 0.1061681 ]  gradient norm: 0.1067250497029447\n",
            "iter: 45  x: [-99.99564439  25.04246724]  f(x): 0.0018224377974555331  grad at x: [0.00871123 0.08493448]  gradient norm: 0.08538003976235975\n",
            "iter: 46  x: [-99.99651551  25.03397379]  f(x): 0.0011663601903714731  grad at x: [0.00696898 0.06794758]  gradient norm: 0.06830403180988581\n",
            "iter: 47  x: [-99.99721241  25.02717903]  f(x): 0.0007464705218376496  grad at x: [0.00557519 0.05435807]  gradient norm: 0.05464322544790524\n",
            "iter: 48  x: [-99.99776993  25.02174323]  f(x): 0.0004777411339761703  grad at x: [0.00446015 0.04348645]  gradient norm: 0.0437145803583276\n",
            "iter: 49  x: [-99.99821594  25.01739458]  f(x): 0.0003057543257447984  grad at x: [0.00356812 0.03478916]  gradient norm: 0.03497166428666491\n",
            "iter: 50  x: [-99.99857275  25.01391567]  f(x): 0.0001956827684766943  grad at x: [0.0028545  0.02783133]  gradient norm: 0.027977331429333594\n",
            "iter: 51  x: [-99.9988582   25.01113253]  f(x): 0.0001252369718251225  grad at x: [0.0022836  0.02226506]  gradient norm: 0.02238186514347028\n",
            "iter: 52  x: [-99.99908656  25.00890603]  f(x): 8.015166196808066e-05  grad at x: [0.00182688 0.01781205]  gradient norm: 0.01790549211477648\n",
            "iter: 53  x: [-99.99926925  25.00712482]  f(x): 5.1297063659571626e-05  grad at x: [0.0014615  0.01424964]  gradient norm: 0.014324393691821183\n",
            "iter: 54  x: [-99.9994154   25.00569986]  f(x): 3.2830120742137265e-05  grad at x: [0.0011692  0.01139971]  gradient norm: 0.01145951495345894\n",
            "iter: 55  x: [-99.99953232  25.00455989]  f(x): 2.1011277274956053e-05  grad at x: [0.00093536 0.00911977]  gradient norm: 0.009167611962764579\n",
            "iter: 56  x: [-99.99962586  25.00364791]  f(x): 1.3447217455979185e-05  grad at x: [0.00074829 0.00729582]  gradient norm: 0.007334089570213657\n",
            "iter: 57  x: [-99.99970068  25.00291833]  f(x): 8.606219171825933e-06  grad at x: [0.00059863 0.00583665]  gradient norm: 0.005867271656170671\n",
            "iter: 58  x: [-99.99976055  25.00233466]  f(x): 5.507980269967236e-06  grad at x: [0.0004789  0.00466932]  gradient norm: 0.004693817324935957\n",
            "iter: 59  x: [-99.99980844  25.00186773]  f(x): 3.5251073727865175e-06  grad at x: [0.00038312 0.00373546]  gradient norm: 0.003755053859952753\n",
            "iter: 60  x: [-99.99984675  25.00149418]  f(x): 2.2560687185824996e-06  grad at x: [0.0003065  0.00298837]  gradient norm: 0.0030040430879616224\n",
            "iter: 61  x: [-99.9998774   25.00119535]  f(x): 1.4438839798907962e-06  grad at x: [0.0002452  0.00239069]  gradient norm: 0.0024032344703676304\n",
            "iter: 62  x: [-99.99990192  25.00095628]  f(x): 9.240857471309111e-07  grad at x: [0.00019616 0.00191255]  gradient norm: 0.0019225875762949381\n",
            "iter: 63  x: [-99.99992154  25.00076502]  f(x): 5.914148781646752e-07  grad at x: [0.00015693 0.00153004]  gradient norm: 0.0015380700610371106\n",
            "iter: 64  x: [-99.99993723  25.00061202]  f(x): 3.7850552202467844e-07  grad at x: [0.00012554 0.00122403]  gradient norm: 0.0012304560488285285\n",
            "iter: 65  x: [-99.99994978  25.00048961]  f(x): 2.422435340970609e-07  grad at x: [0.00010043 0.00097923]  gradient norm: 0.0009843648390653963\n",
            "iter: 66  x: [-99.99995983  25.00039169]  f(x): 1.5503586182100573e-07  grad at x: [8.03469022e-05 7.83382297e-04]  gradient norm: 0.0007874918712494897\n",
            "iter: 67  x: [-99.99996786  25.00031335]  f(x): 9.922295156615158e-08  grad at x: [6.42775218e-05 6.26705837e-04]  gradient norm: 0.0006299934970018391\n",
            "iter: 68  x: [-99.99997429  25.00025068]  f(x): 6.350268900168847e-08  grad at x: [5.14220174e-05 5.01364670e-04]  gradient norm: 0.0005039947975988977\n",
            "iter: 69  x: [-99.99997943  25.00020055]  f(x): 4.064172096067871e-08  grad at x: [4.11376139e-05 4.01091736e-04]  gradient norm: 0.00040319583807712454\n",
            "iter: 70  x: [-99.99998354  25.00016044]  f(x): 2.6010701414378385e-08  grad at x: [3.29100911e-05 3.20873389e-04]  gradient norm: 0.0003225566704588723\n",
            "iter: 71  x: [-99.99998684  25.00012835]  f(x): 1.6646848905716612e-08  grad at x: [2.63280729e-05 2.56698711e-04]  gradient norm: 0.0002580453363710851\n",
            "iter: 72  x: [-99.99998947  25.00010268]  f(x): 1.0653983299890604e-08  grad at x: [2.10624583e-05 2.05358969e-04]  gradient norm: 0.00020643626909911544\n",
            "iter: 73  x: [-99.99999158  25.00008214]  f(x): 6.818549311861143e-09  grad at x: [1.68499667e-05 1.64287175e-04]  gradient norm: 0.00016514901527845866\n",
            "iter: 74  x: [-99.99999326  25.00006571]  f(x): 4.3638715595743706e-09  grad at x: [1.34799733e-05 1.31429740e-04]  gradient norm: 0.0001321192122225132\n",
            "iter: 75  x: [-99.99999461  25.00005257]  f(x): 2.792877797916879e-09  grad at x: [1.07839787e-05 1.05143792e-04]  gradient norm: 0.00010569536977402327\n",
            "iter: 76  x: [-99.99999569  25.00004206]  f(x): 1.7874417907372972e-09  grad at x: [8.62718292e-06 8.41150336e-05]  gradient norm: 8.455629582088604e-05\n",
            "iter: 77  x: [-99.99999655  25.00003365]  f(x): 1.1439627461478818e-09  grad at x: [6.90174633e-06 6.72920269e-05]  gradient norm: 6.76450366589562e-05\n",
            "iter: 78  x: [-99.99999724  25.00002692]  f(x): 7.321361575503372e-10  grad at x: [5.52139707e-06 5.38336215e-05]  gradient norm: 5.4116029327744926e-05\n",
            "iter: 79  x: [-99.99999779  25.00002153]  f(x): 4.685671408196616e-10  grad at x: [4.41711765e-06 4.30668972e-05]  gradient norm: 4.329282346161597e-05\n",
            "iter: 80  x: [-99.99999823  25.00001723]  f(x): 2.9988297011014613e-10  grad at x: [3.53369413e-06 3.44535178e-05]  gradient norm: 3.4634258768459074e-05\n",
            "iter: 81  x: [-99.99999859  25.00001378]  f(x): 1.919251008624588e-10  grad at x: [2.82695530e-06 2.75628142e-05]  gradient norm: 2.7707407014187295e-05\n",
            "iter: 82  x: [-99.99999887  25.00001103]  f(x): 1.228320645491615e-10  grad at x: [2.26156425e-06 2.20502514e-05]  gradient norm: 2.21659256110961e-05\n",
            "iter: 83  x: [-99.9999991   25.00000882]  f(x): 7.861252130117896e-11  grad at x: [1.80925139e-06 1.76402011e-05]  gradient norm: 1.7732740487716946e-05\n",
            "iter: 84  x: [-99.99999928  25.00000706]  f(x): 5.031201363866807e-11  grad at x: [1.44740110e-06 1.41121609e-05]  gradient norm: 1.4186192391007259e-05\n",
            "iter: 85  x: [-99.99999942  25.00000564]  f(x): 3.219968874150022e-11  grad at x: [1.15792088e-06 1.12897287e-05]  gradient norm: 1.1348953915053178e-05\n",
            "iter: 86  x: [-99.99999954  25.00000452]  f(x): 2.060780080476227e-11  grad at x: [9.26336696e-07 9.03178297e-06]  gradient norm: 9.079163134289916e-06\n",
            "\n",
            "Optimizer: [-99.99999954  25.00000452]\n",
            "for starting point = [0, 1000], the minimum value of function is 2.060780080476227e-11 and number of iterations are= 86\n",
            "\n",
            "For starting point= [1, 1]\n",
            "iter: 0  x: [1 1]  f(x): 10777  grad at x: [202 -48]  gradient norm: 207.62466134830902\n",
            "iter: 1  x: [-19.2   5.8]  f(x): 6897.28  grad at x: [161.6 -38.4]  gradient norm: 166.09972907864721\n",
            "iter: 2  x: [-35.36   9.64]  f(x): 4414.2592  grad at x: [129.28 -30.72]  gradient norm: 132.87978326291778\n",
            "iter: 3  x: [-48.288  12.712]  f(x): 2825.1258880000005  grad at x: [103.424 -24.576]  gradient norm: 106.30382661033423\n",
            "iter: 4  x: [-58.6304  15.1696]  f(x): 1808.0805683200003  grad at x: [ 82.7392 -19.6608]  gradient norm: 85.04306128826738\n",
            "iter: 5  x: [-66.90432  17.13568]  f(x): 1157.1715637248003  grad at x: [ 66.19136 -15.72864]  gradient norm: 68.0344490306139\n",
            "iter: 6  x: [-73.523456  18.708544]  f(x): 740.5898007838722  grad at x: [ 52.953088 -12.582912]  gradient norm: 54.42755922449113\n",
            "iter: 7  x: [-78.8187648  19.9668352]  f(x): 473.9774725016782  grad at x: [ 42.3624704 -10.0663296]  gradient norm: 43.5420473795929\n",
            "iter: 8  x: [-83.05501184  20.97346816]  f(x): 303.3455824010743  grad at x: [33.88997632 -8.05306368]  gradient norm: 34.83363790367433\n",
            "iter: 9  x: [-86.44400947  21.77877453]  f(x): 194.1411727366876  grad at x: [27.11198106 -6.44245094]  gradient norm: 27.866910322939468\n",
            "iter: 10  x: [-89.15520758  22.42301962]  f(x): 124.25035055148001  grad at x: [21.68958484 -5.15396076]  gradient norm: 22.29352825835157\n",
            "iter: 11  x: [-91.32416606  22.9384157 ]  f(x): 79.5202243529472  grad at x: [17.35166788 -4.1231686 ]  gradient norm: 17.834822606681257\n",
            "iter: 12  x: [-93.05933285  23.35073256]  f(x): 50.89294358588613  grad at x: [13.8813343  -3.29853488]  gradient norm: 14.267858085344994\n",
            "iter: 13  x: [-94.44746628  23.68058605]  f(x): 32.57148389496712  grad at x: [11.10506744 -2.63882791]  gradient norm: 11.414286468275995\n",
            "iter: 14  x: [-95.55797302  23.94446884]  f(x): 20.845749692778906  grad at x: [ 8.88405395 -2.11106233]  gradient norm: 9.131429174620784\n",
            "iter: 15  x: [-96.44637842  24.15557507]  f(x): 13.34127980337846  grad at x: [ 7.10724316 -1.68884986]  gradient norm: 7.305143339696617\n",
            "iter: 16  x: [-97.15710274  24.32446006]  f(x): 8.53841907416223  grad at x: [ 5.68579453 -1.35107989]  gradient norm: 5.844114671757299\n",
            "iter: 17  x: [-97.72568219  24.45956804]  f(x): 5.464588207463851  grad at x: [ 4.54863562 -1.08086391]  gradient norm: 4.67529173740585\n",
            "iter: 18  x: [-98.18054575  24.56765444]  f(x): 3.4973364527768562  grad at x: [ 3.6389085  -0.86469113]  gradient norm: 3.7402333899246747\n",
            "iter: 19  x: [-98.5444366   24.65412355]  f(x): 2.238295329777196  grad at x: [ 2.9111268 -0.6917529]  gradient norm: 2.992186711939745\n",
            "iter: 20  x: [-98.83554928  24.72329884]  f(x): 1.4325090110574112  grad at x: [ 2.32890144 -0.55340232]  gradient norm: 2.393749369551801\n",
            "iter: 21  x: [-99.06843942  24.77863907]  f(x): 0.9168057670767333  grad at x: [ 1.86312115 -0.44272186]  gradient norm: 1.9149994956414305\n",
            "iter: 22  x: [-99.25475154  24.82291126]  f(x): 0.5867556909291173  grad at x: [ 1.49049692 -0.35417749]  gradient norm: 1.5319995965131548\n",
            "iter: 23  x: [-99.40380123  24.85832901]  f(x): 0.3755236421946416  grad at x: [ 1.19239754 -0.28334199]  gradient norm: 1.2255996772105346\n",
            "iter: 24  x: [-99.52304099  24.8866632 ]  f(x): 0.2403351310045764  grad at x: [ 0.95391803 -0.22667359]  gradient norm: 0.9804797417684394\n",
            "iter: 25  x: [-99.61843279  24.90933056]  f(x): 0.15381448384292862  grad at x: [ 0.76313442 -0.18133887]  gradient norm: 0.7843837934147508\n",
            "iter: 26  x: [-99.69474623  24.92746445]  f(x): 0.09844126965947606  grad at x: [ 0.61050754 -0.1450711 ]  gradient norm: 0.6275070347318061\n",
            "iter: 27  x: [-99.75579698  24.94197156]  f(x): 0.06300241258206477  grad at x: [ 0.48840603 -0.11605688]  gradient norm: 0.5020056277854453\n",
            "iter: 28  x: [-99.80463759  24.95357725]  f(x): 0.040321544052519226  grad at x: [ 0.39072482 -0.0928455 ]  gradient norm: 0.40160450222834515\n",
            "iter: 29  x: [-99.84371007  24.9628618 ]  f(x): 0.02580578819361314  grad at x: [ 0.31257986 -0.0742764 ]  gradient norm: 0.32128360178268134\n",
            "iter: 30  x: [-99.87496806  24.97028944]  f(x): 0.01651570444391245  grad at x: [ 0.25006389 -0.05942112]  gradient norm: 0.2570268814261454\n",
            "iter: 31  x: [-99.89997444  24.97623155]  f(x): 0.010570050844102799  grad at x: [ 0.20005111 -0.0475369 ]  gradient norm: 0.20562150514090494\n",
            "iter: 32  x: [-99.91997956  24.98098524]  f(x): 0.006764832540226219  grad at x: [ 0.16004089 -0.03802952]  gradient norm: 0.16449720411272914\n",
            "iter: 33  x: [-99.93598364  24.98478819]  f(x): 0.00432949282574446  grad at x: [ 0.12803271 -0.03042361]  gradient norm: 0.13159776329017844\n",
            "iter: 34  x: [-99.94878692  24.98783055]  f(x): 0.002770875408475837  grad at x: [ 0.10242617 -0.02433889]  gradient norm: 0.10527821063213103\n",
            "iter: 35  x: [-99.95902953  24.99026444]  f(x): 0.0017733602614249879  grad at x: [ 0.08194093 -0.01947111]  gradient norm: 0.08422256850571556\n",
            "iter: 36  x: [-99.96722363  24.99221155]  f(x): 0.0011349505673121786  grad at x: [ 0.06555275 -0.01557689]  gradient norm: 0.06737805480457798\n",
            "iter: 37  x: [-99.9737789   24.99376924]  f(x): 0.000726368363079505  grad at x: [ 0.0524422  -0.01246151]  gradient norm: 0.05390244384365165\n",
            "iter: 38  x: [-99.97902312  24.9950154 ]  f(x): 0.00046487575237089736  grad at x: [ 0.04195376 -0.00996921]  gradient norm: 0.04312195507492198\n",
            "iter: 39  x: [-99.9832185   24.99601232]  f(x): 0.0002975204815172676  grad at x: [ 0.03356301 -0.00797537]  gradient norm: 0.034497564059931395\n",
            "iter: 40  x: [-99.9865748   24.99680985]  f(x): 0.00019041310817089862  grad at x: [ 0.02685041 -0.00638029]  gradient norm: 0.027598051247934057\n",
            "iter: 41  x: [-99.98925984  24.99744788]  f(x): 0.00012186438922937875  grad at x: [ 0.02148032 -0.00510424]  gradient norm: 0.022078440998347573\n",
            "iter: 42  x: [-99.99140787  24.99795831]  f(x): 7.799320910685413e-05  grad at x: [ 0.01718426 -0.00408339]  gradient norm: 0.017662752798683918\n",
            "iter: 43  x: [-99.9931263   24.99836664]  f(x): 4.99156538284648e-05  grad at x: [ 0.01374741 -0.00326671]  gradient norm: 0.014130202238958195\n",
            "iter: 44  x: [-99.99450104  24.99869332]  f(x): 3.194601845024872e-05  grad at x: [ 0.01099793 -0.00261337]  gradient norm: 0.011304161791172086\n",
            "iter: 45  x: [-99.99560083  24.99895465]  f(x): 2.0445451808131206e-05  grad at x: [ 0.00879834 -0.00209069]  gradient norm: 0.009043329432931482\n",
            "iter: 46  x: [-99.99648066  24.99916372]  f(x): 1.308508915720516e-05  grad at x: [ 0.00703867 -0.00167256]  gradient norm: 0.007234663546345514\n",
            "iter: 47  x: [-99.99718453  24.99933098]  f(x): 8.3744570605972e-06  grad at x: [ 0.00563094 -0.00133804]  gradient norm: 0.0057877308370715376\n",
            "iter: 48  x: [-99.99774762  24.99946478]  f(x): 5.359652518758123e-06  grad at x: [ 0.00450475 -0.00107044]  gradient norm: 0.004630184669646827\n",
            "iter: 49  x: [-99.9981981   24.99957183]  f(x): 3.4301776119847135e-06  grad at x: [ 0.0036038  -0.00085635]  gradient norm: 0.0037041477357064004\n",
            "iter: 50  x: [-99.99855848  24.99965746]  f(x): 2.195313671662023e-06  grad at x: [ 0.00288304 -0.00068508]  gradient norm: 0.00296331818855959\n",
            "iter: 51  x: [-99.99884678  24.99972597]  f(x): 1.4050007498509733e-06  grad at x: [ 0.00230643 -0.00054806]  gradient norm: 0.0023706545508369397\n",
            "iter: 52  x: [-99.99907743  24.99978077]  f(x): 8.992004798993787e-07  grad at x: [ 0.00184515 -0.00043845]  gradient norm: 0.0018965236406640216\n",
            "iter: 53  x: [-99.99926194  24.99982462]  f(x): 5.754883071439931e-07  grad at x: [ 0.00147612 -0.00035076]  gradient norm: 0.001517218912542278\n",
            "iter: 54  x: [-99.99940955  24.9998597 ]  f(x): 3.683125165721556e-07  grad at x: [ 0.00118089 -0.00028061]  gradient norm: 0.0012137751300338223\n",
            "iter: 55  x: [-99.99952764  24.99988776]  f(x): 2.3572001061170917e-07  grad at x: [ 0.00094471 -0.00022449]  gradient norm: 0.0009710201040384471\n",
            "iter: 56  x: [-99.99962211  24.99991021]  f(x): 1.508608067873254e-07  grad at x: [ 0.00075577 -0.00017959]  gradient norm: 0.0007768160832200255\n",
            "iter: 57  x: [-99.99969769  24.99992816]  f(x): 9.655091634570877e-08  grad at x: [ 0.00060462 -0.00014367]  gradient norm: 0.0006214528665818793\n",
            "iter: 58  x: [-99.99975815  24.99994253]  f(x): 6.179258646408477e-08  grad at x: [ 0.00048369 -0.00011494]  gradient norm: 0.0004971622932768927\n",
            "iter: 59  x: [-99.99980652  24.99995403]  f(x): 3.954725533597979e-08  grad at x: [ 3.86955124e-04 -9.19497325e-05]  gradient norm: 0.00039772983461631235\n",
            "iter: 60  x: [-99.99984522  24.99996322]  f(x): 2.5310243416839e-08  grad at x: [ 3.09564099e-04 -7.35597860e-05]  gradient norm: 0.0003181838677044391\n",
            "iter: 61  x: [-99.99987617  24.99997058]  f(x): 1.6198555786114906e-08  grad at x: [ 2.47651279e-04 -5.88478288e-05]  gradient norm: 0.00025454709415834945\n",
            "iter: 62  x: [-99.99990094  24.99997646]  f(x): 1.0367075702054256e-08  grad at x: [ 1.98121024e-04 -4.70782630e-05]  gradient norm: 0.00020363767531627594\n",
            "iter: 63  x: [-99.99992075  24.99998117]  f(x): 6.634928449287962e-09  grad at x: [ 1.58496819e-04 -3.76626104e-05]  gradient norm: 0.00016291014025269222\n",
            "iter: 64  x: [-99.9999366   24.99998493]  f(x): 4.246354206823536e-09  grad at x: [ 1.26797455e-04 -3.01300883e-05]  gradient norm: 0.0001303281121910931\n",
            "iter: 65  x: [-99.99994928  24.99998795]  f(x): 2.717666691807582e-09  grad at x: [ 1.01437964e-04 -2.41040707e-05]  gradient norm: 0.0001042624897421423\n",
            "iter: 66  x: [-99.99995942  24.99999036]  f(x): 1.7393066827568525e-09  grad at x: [ 8.11503712e-05 -1.92832565e-05]  gradient norm: 8.340999179371383e-05\n",
            "iter: 67  x: [-99.99996754  24.99999229]  f(x): 1.1131562771708228e-09  grad at x: [ 6.49202970e-05 -1.54266052e-05]  gradient norm: 6.672799344115849e-05\n",
            "iter: 68  x: [-99.99997403  24.99999383]  f(x): 7.124200173805575e-10  grad at x: [ 5.19362376e-05 -1.23412842e-05]  gradient norm: 5.3382394752598256e-05\n",
            "iter: 69  x: [-99.99997923  24.99999506]  f(x): 4.559488108803629e-10  grad at x: [ 4.15489901e-05 -9.87302735e-06]  gradient norm: 4.270591579068937e-05\n",
            "iter: 70  x: [-99.99998338  24.99999605]  f(x): 2.918072390635159e-10  grad at x: [ 3.32391920e-05 -7.89842188e-06]  gradient norm: 3.416473263841038e-05\n",
            "iter: 71  x: [-99.9999867   24.99999684]  f(x): 1.8675663308520681e-10  grad at x: [ 2.65913536e-05 -6.31873750e-06]  gradient norm: 2.7331786116915727e-05\n",
            "iter: 72  x: [-99.99998936  24.99999747]  f(x): 1.1952424523858588e-10  grad at x: [ 2.12730829e-05 -5.05499001e-06]  gradient norm: 2.1865428899391468e-05\n",
            "iter: 73  x: [-99.99999149  24.99999798]  f(x): 7.649551695844182e-11  grad at x: [ 1.70184663e-05 -4.04399201e-06]  gradient norm: 1.7492343120170244e-05\n",
            "iter: 74  x: [-99.99999319  24.99999838]  f(x): 4.8957130926196305e-11  grad at x: [ 1.36147731e-05 -3.23519360e-06]  gradient norm: 1.3993874506539825e-05\n",
            "iter: 75  x: [-99.99999455  24.99999871]  f(x): 3.1332563858356445e-11  grad at x: [ 1.08918185e-05 -2.58815489e-06]  gradient norm: 1.1195099616949632e-05\n",
            "iter: 76  x: [-99.99999564  24.99999896]  f(x): 2.0052840841640608e-11  grad at x: [ 8.71345478e-06 -2.07052391e-06]  gradient norm: 8.956079687372285e-06\n",
            "\n",
            "Optimizer: [-99.99999564  24.99999896]\n",
            "for starting point = [1, 1], the minimum value of function is 2.0052840841640608e-11 and number of iterations are= 76\n",
            "\n",
            "For starting point= [-500, -2]\n",
            "iter: 0  x: [-500   -2]  f(x): 160729  grad at x: [-800  -54]  gradient norm: 801.8204287744232\n",
            "iter: 1  x: [-420.     3.4]  f(x): 102866.56  grad at x: [-640.   -43.2]  gradient norm: 641.4563430195386\n",
            "iter: 2  x: [-356.      7.72]  f(x): 65834.5984  grad at x: [-512.    -34.56]  gradient norm: 513.1650744156309\n",
            "iter: 3  x: [-304.8     11.176]  f(x): 42134.14297600001  grad at x: [-409.6    -27.648]  gradient norm: 410.5320595325048\n",
            "iter: 4  x: [-263.84     13.9408]  f(x): 26965.851504640013  grad at x: [-327.68    -22.1184]  gradient norm: 328.42564762600387\n",
            "iter: 5  x: [-231.072     16.15264]  f(x): 17258.144962969607  grad at x: [-262.144    -17.69472]  gradient norm: 262.74051810080306\n",
            "iter: 6  x: [-204.8576     17.922112]  f(x): 11045.212776300548  grad at x: [-209.7152    -14.155776]  gradient norm: 210.19241448064247\n",
            "iter: 7  x: [-183.88608     19.3376896]  f(x): 7068.936176832352  grad at x: [-167.77216    -11.3246208]  gradient norm: 168.153931584514\n",
            "iter: 8  x: [-167.108864     20.47015168]  f(x): 4524.1191531727045  grad at x: [-134.217728     -9.05969664]  gradient norm: 134.52314526761117\n",
            "iter: 9  x: [-153.6870912    21.37612134]  f(x): 2895.436258030529  grad at x: [-107.3741824    -7.24775731]  gradient norm: 107.6185162140889\n",
            "iter: 10  x: [-142.94967296   22.10089708]  f(x): 1853.0792051395379  grad at x: [-85.89934592  -5.79820585]  gradient norm: 86.09481297127111\n",
            "iter: 11  x: [-134.35973837   22.68071766]  f(x): 1185.9706912893046  grad at x: [-68.71947674  -4.63856468]  gradient norm: 68.87585037701689\n",
            "iter: 12  x: [-127.48779069   23.14457413]  f(x): 759.0212424251547  grad at x: [-54.97558139  -3.71085174]  gradient norm: 55.10068030161351\n",
            "iter: 13  x: [-121.99023256   23.5156593 ]  f(x): 485.7735951520991  grad at x: [-43.98046511  -2.96868139]  gradient norm: 44.08054424129081\n",
            "iter: 14  x: [-117.59218604   23.81252744]  f(x): 310.8951008973434  grad at x: [-35.18437209  -2.37494512]  gradient norm: 35.26443539303265\n",
            "iter: 15  x: [-114.07374884   24.05002195]  f(x): 198.97286457429982  grad at x: [-28.14749767  -1.89995609]  gradient norm: 28.21154831442612\n",
            "iter: 16  x: [-111.25899907   24.24001756]  f(x): 127.34263332755181  grad at x: [-22.51799814  -1.51996487]  gradient norm: 22.56923865154089\n",
            "iter: 17  x: [-109.00719925   24.39201405]  f(x): 81.49928532963305  grad at x: [-18.01439851  -1.2159719 ]  gradient norm: 18.0553909212327\n",
            "iter: 18  x: [-107.2057594    24.51361124]  f(x): 52.15954261096512  grad at x: [-14.41151881  -0.97277752]  gradient norm: 14.444312736986156\n",
            "iter: 19  x: [-105.76460752   24.61088899]  f(x): 33.38210727101771  grad at x: [-11.52921505  -0.77822202]  gradient norm: 11.55545018958893\n",
            "iter: 20  x: [-104.61168602   24.68871119]  f(x): 21.36454865345128  grad at x: [-9.22337204 -0.62257761]  gradient norm: 9.244360151671133\n",
            "iter: 21  x: [-103.68934881   24.75096896]  f(x): 13.67331113820884  grad at x: [-7.37869763 -0.49806209]  gradient norm: 7.3954881213369115\n",
            "iter: 22  x: [-102.95147905   24.80077516]  f(x): 8.750919128453674  grad at x: [-5.9029581  -0.39844967]  gradient norm: 5.916390497069535\n",
            "iter: 23  x: [-102.36118324   24.84062013]  f(x): 5.600588242210325  grad at x: [-4.72236648 -0.31875974]  gradient norm: 4.733112397655616\n",
            "iter: 24  x: [-101.88894659   24.8724961 ]  f(x): 3.584376475014597  grad at x: [-3.77789319 -0.25500779]  gradient norm: 3.7864899181244875\n",
            "iter: 25  x: [-101.51115727   24.89799688]  f(x): 2.2940009440093507  grad at x: [-3.02231455 -0.20400623]  gradient norm: 3.029191934499596\n",
            "iter: 26  x: [-101.20892582   24.91839751]  f(x): 1.4681606041659705  grad at x: [-2.41785164 -0.16320499]  gradient norm: 2.423353547599665\n",
            "iter: 27  x: [-100.96714066   24.93471801]  f(x): 0.9396227866662153  grad at x: [-1.93428131 -0.13056399]  gradient norm: 1.938682838079726\n",
            "iter: 28  x: [-100.77371252   24.9477744 ]  f(x): 0.6013585834663866  grad at x: [-1.54742505 -0.10445119]  gradient norm: 1.5509462704637922\n",
            "iter: 29  x: [-100.61897002   24.95821952]  f(x): 0.3848694934184874  grad at x: [-1.23794004 -0.08356095]  gradient norm: 1.2407570163710337\n",
            "iter: 30  x: [-100.49517602   24.96657562]  f(x): 0.24631647578783206  grad at x: [-0.99035203 -0.06684876]  gradient norm: 0.9926056130968272\n",
            "iter: 31  x: [-100.39614081   24.9732605 ]  f(x): 0.15764254450421025  grad at x: [-0.79228163 -0.05347901]  gradient norm: 0.7940844904774561\n",
            "iter: 32  x: [-100.31691265   24.9786084 ]  f(x): 0.10089122848269093  grad at x: [-0.6338253  -0.04278321]  gradient norm: 0.6352675923819534\n",
            "iter: 33  x: [-100.25353012   24.98288672]  f(x): 0.06457038622892214  grad at x: [-0.50706024 -0.03422657]  gradient norm: 0.5082140739055625\n",
            "iter: 34  x: [-100.2028241    24.98630937]  f(x): 0.04132504718650906  grad at x: [-0.40564819 -0.02738125]  gradient norm: 0.4065712591244446\n",
            "iter: 35  x: [-100.16225928   24.9890475 ]  f(x): 0.026448030199364893  grad at x: [-0.32451855 -0.021905  ]  gradient norm: 0.3252570072995501\n",
            "iter: 36  x: [-100.12980742   24.991238  ]  f(x): 0.016926739327595006  grad at x: [-0.25961484 -0.017524  ]  gradient norm: 0.2602056058396514\n",
            "iter: 37  x: [-100.10384594   24.9929904 ]  f(x): 0.010833113169659632  grad at x: [-0.20769187 -0.0140192 ]  gradient norm: 0.20816448467170987\n",
            "iter: 38  x: [-100.08307675   24.99439232]  f(x): 0.006933192428581709  grad at x: [-0.1661535  -0.01121536]  gradient norm: 0.1665315877373624\n",
            "iter: 39  x: [-100.0664614    24.99551386]  f(x): 0.004437243154291525  grad at x: [-0.1329228  -0.00897229]  gradient norm: 0.1332252701898784\n",
            "iter: 40  x: [-100.05316912   24.99641108]  f(x): 0.002839835618745967  grad at x: [-0.10633824 -0.00717783]  gradient norm: 0.10658021615189128\n",
            "iter: 41  x: [-100.0425353    24.99712887]  f(x): 0.0018174947959976524  grad at x: [-0.08507059 -0.00574226]  gradient norm: 0.0852641729215185\n",
            "iter: 42  x: [-100.03402824   24.99770309]  f(x): 0.0011631966694381041  grad at x: [-0.06805647 -0.00459381]  gradient norm: 0.06821133833720328\n",
            "iter: 43  x: [-100.02722259   24.99816248]  f(x): 0.0007444458684407013  grad at x: [-0.05444518 -0.00367505]  gradient norm: 0.05456907066977415\n",
            "iter: 44  x: [-100.02177807   24.99852998]  f(x): 0.00047644535580230064  grad at x: [-0.04355614 -0.00294004]  gradient norm: 0.043655256535830855\n",
            "iter: 45  x: [-100.01742246   24.99882398]  f(x): 0.0003049250277135698  grad at x: [-0.03484491 -0.00235203]  gradient norm: 0.034924205228670256\n",
            "iter: 46  x: [-100.01393797   24.99905919]  f(x): 0.00019515201773652886  grad at x: [-0.02787593 -0.00188163]  gradient norm: 0.027939364182925055\n",
            "iter: 47  x: [-100.01115037   24.99924735]  f(x): 0.00012489729135131724  grad at x: [-0.02230075 -0.0015053 ]  gradient norm: 0.022351491346334566\n",
            "iter: 48  x: [-100.0089203    24.99939788]  f(x): 7.993426646474161e-05  grad at x: [-0.0178406  -0.00120424]  gradient norm: 0.01788119307705631\n",
            "iter: 49  x: [-100.00713624   24.9995183 ]  f(x): 5.115793053735419e-05  grad at x: [-0.01427248 -0.00096339]  gradient norm: 0.014304954461633799\n",
            "iter: 50  x: [-100.00570899   24.99961464]  f(x): 3.2741075543906685e-05  grad at x: [-0.01141798 -0.00077071]  gradient norm: 0.01144396356930704\n",
            "iter: 51  x: [-100.00456719   24.99969171]  f(x): 2.095428834807344e-05  grad at x: [-0.00913439 -0.00061657]  gradient norm: 0.009155170855439768\n",
            "iter: 52  x: [-100.00365375   24.99975337]  f(x): 1.3410744542787768e-05  grad at x: [-0.00730751 -0.00049326]  gradient norm: 0.007324136684357486\n",
            "iter: 53  x: [-100.002923    24.9998027]  f(x): 8.582876507401348e-06  grad at x: [-0.00584601 -0.00039461]  gradient norm: 0.005859309347491852\n",
            "iter: 54  x: [-100.0023384    24.99984216]  f(x): 5.4930409647368625e-06  grad at x: [-0.00467681 -0.00031568]  gradient norm: 0.004687447477993482\n",
            "iter: 55  x: [-100.00187072   24.99987373]  f(x): 3.5155462174525014e-06  grad at x: [-0.00374144 -0.00025255]  gradient norm: 0.003749957982405937\n",
            "iter: 56  x: [-100.00149658   24.99989898]  f(x): 2.2499495791778204e-06  grad at x: [-0.00299316 -0.00020204]  gradient norm: 0.0029999663859302294\n",
            "iter: 57  x: [-100.00119726   24.99991918]  f(x): 1.4399677306669997e-06  grad at x: [-0.00239452 -0.00016163]  gradient norm: 0.0023999731087385122\n",
            "iter: 58  x: [-100.00095781   24.99993535]  f(x): 9.215793476268797e-07  grad at x: [-0.00191562 -0.0001293 ]  gradient norm: 0.0019199784869908098\n",
            "iter: 59  x: [-100.00076625   24.99994828]  f(x): 5.898107824724918e-07  grad at x: [-0.0015325  -0.00010344]  gradient norm: 0.0015359827895813048\n",
            "iter: 60  x: [-100.000613     24.99995862]  f(x): 3.7747890077891027e-07  grad at x: [-1.22599643e-03 -8.27547592e-05]  gradient norm: 0.0012287862316593724\n",
            "iter: 61  x: [-100.0004904   24.9999669]  f(x): 2.4158649650407775e-07  grad at x: [-9.80797146e-04 -6.62038074e-05]  gradient norm: 0.0009830289853388408\n",
            "iter: 62  x: [-100.00039232   24.99997352]  f(x): 1.546153577603044e-07  grad at x: [-7.84637717e-04 -5.29630459e-05]  gradient norm: 0.0007864231882652098\n",
            "iter: 63  x: [-100.00031386   24.99997881]  f(x): 9.895382897013284e-08  grad at x: [-6.27710174e-04 -4.23704367e-05]  gradient norm: 0.000629138550623415\n",
            "iter: 64  x: [-100.00025108   24.99998305]  f(x): 6.333045054083685e-08  grad at x: [-5.02168139e-04 -3.38963494e-05]  gradient norm: 0.0005033108404985406\n",
            "iter: 65  x: [-100.00020087   24.99998644]  f(x): 4.053148834383272e-08  grad at x: [-4.01734511e-04 -2.71170795e-05]  gradient norm: 0.00040264867238739394\n",
            "iter: 66  x: [-100.00016069   24.99998915]  f(x): 2.594015253821065e-08  grad at x: [-3.21387609e-04 -2.16936636e-05]  gradient norm: 0.00032211893789847653\n",
            "iter: 67  x: [-100.00012856   24.99999132]  f(x): 1.6601697623748728e-08  grad at x: [-2.57110087e-04 -1.73549309e-05]  gradient norm: 0.0002576951503133012\n",
            "iter: 68  x: [-100.00010284   24.99999306]  f(x): 1.0625086479803517e-08  grad at x: [-2.05688070e-04 -1.38839447e-05]  gradient norm: 0.00020615612025650382\n",
            "iter: 69  x: [-100.00008228   24.99999445]  f(x): 6.800055348017504e-09  grad at x: [-1.64550456e-04 -1.11071558e-05]  gradient norm: 0.00016492489621664164\n",
            "iter: 70  x: [-100.00006582   24.99999556]  f(x): 4.3520354234794915e-09  grad at x: [-1.31640365e-04 -8.88572461e-06]  gradient norm: 0.00013193991698465617\n",
            "iter: 71  x: [-100.00005266   24.99999645]  f(x): 2.7853026710218236e-09  grad at x: [-1.05312292e-04 -7.10857969e-06]  gradient norm: 0.00010555193358762925\n",
            "iter: 72  x: [-100.00004212   24.99999716]  f(x): 1.7825937096974603e-09  grad at x: [-8.42498333e-05 -5.68686375e-06]  gradient norm: 8.444154687587053e-05\n",
            "iter: 73  x: [-100.0000337    24.99999773]  f(x): 1.1408599742096071e-09  grad at x: [-6.73998667e-05 -4.54949100e-06]  gradient norm: 6.755323750079213e-05\n",
            "iter: 74  x: [-100.00002696   24.99999818]  f(x): 7.301503831928217e-10  grad at x: [-5.39198933e-05 -3.63959280e-06]  gradient norm: 5.4042589989482245e-05\n",
            "iter: 75  x: [-100.00002157   24.99999854]  f(x): 4.672962452434058e-10  grad at x: [-4.31359147e-05 -2.91167424e-06]  gradient norm: 4.3234071991585795e-05\n",
            "iter: 76  x: [-100.00001725   24.99999884]  f(x): 2.9906959715193915e-10  grad at x: [-3.45087317e-05 -2.32933940e-06]  gradient norm: 3.458725760461151e-05\n",
            "iter: 77  x: [-100.0000138    24.99999907]  f(x): 1.914045420216376e-10  grad at x: [-2.76069854e-05 -1.86347152e-06]  gradient norm: 2.766980607244204e-05\n",
            "iter: 78  x: [-100.00001104   24.99999925]  f(x): 1.2249890695767835e-10  grad at x: [-2.20855883e-05 -1.49077722e-06]  gradient norm: 2.2135844863720773e-05\n",
            "iter: 79  x: [-100.00000883   24.9999994 ]  f(x): 7.839930055419517e-11  grad at x: [-1.76684707e-05 -1.19262177e-06]  gradient norm: 1.7708675902415197e-05\n",
            "iter: 80  x: [-100.00000707   24.99999952]  f(x): 5.017555239350251e-11  grad at x: [-1.41347765e-05 -9.54097416e-07]  gradient norm: 1.4166940727412184e-05\n",
            "iter: 81  x: [-100.00000565   24.99999962]  f(x): 3.211235359557679e-11  grad at x: [-1.13078212e-05 -7.63277932e-07]  gradient norm: 1.1333552593176914e-05\n",
            "iter: 82  x: [-100.00000452   24.99999969]  f(x): 2.055190630073527e-11  grad at x: [-9.04625699e-06 -6.10622344e-07]  gradient norm: 9.066842074445827e-06\n",
            "\n",
            "Optimizer: [-100.00000452   24.99999969]\n",
            "for starting point = [-500, -2], the minimum value of function is 2.055190630073527e-11 and number of iterations are= 82\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "h6ym02OE8tmJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head=['Starting Point',\"Final Optimizer\",\"No. of Iterations\",\"Optimum Value\"]"
      ],
      "metadata": {
        "id": "RXYLsBEEstJ4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data=[[list_of_starting_points[i],final_optimizer[i],no_of_iterations[i],final_minimum_value[i]] for i in range(len(list_of_starting_points))]"
      ],
      "metadata": {
        "id": "JbdUvyG4tOjL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate(my_data, headers=head, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS_NW6nctzyt",
        "outputId": "b9982e45-99b5-42ee-e91a-60f52290adb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------------------------+---------------------+-----------------+\n",
            "| Starting Point   | Final Optimizer               |   No. of Iterations |   Optimum Value |\n",
            "+==================+===============================+=====================+=================+\n",
            "| [10000, 10000]   | [-99.99999679  25.00000317]   |                  98 |     2.04143e-11 |\n",
            "+------------------+-------------------------------+---------------------+-----------------+\n",
            "| [500, 0]         | [-99.99999566  24.99999982]   |                  84 |     1.88875e-11 |\n",
            "+------------------+-------------------------------+---------------------+-----------------+\n",
            "| [0, 1000]        | [-99.99999954  25.00000452]   |                  86 |     2.06078e-11 |\n",
            "+------------------+-------------------------------+---------------------+-----------------+\n",
            "| [1, 1]           | [-99.99999564  24.99999896]   |                  76 |     2.00528e-11 |\n",
            "+------------------+-------------------------------+---------------------+-----------------+\n",
            "| [-500, -2]       | [-100.00000452   24.99999969] |                  82 |     2.05519e-11 |\n",
            "+------------------+-------------------------------+---------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we got highest number of iterations for starting point $[10000,10000]$ and minimum number of iterations for starting point $[1,1]$ and for every starting point the optimal value is approximately zero and the optimizer value for all starting points are approximately equal to $[-100,25].$"
      ],
      "metadata": {
        "id": "9JxOVBQOYbOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dJLULDYluKTV"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}