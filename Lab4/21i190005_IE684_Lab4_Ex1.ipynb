{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab4_Ex1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{Exercise-1}$$"
      ],
      "metadata": {
        "id": "lmyxWhC-sYUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given function is:$$f(x)=400x_1^2+0.004x_2^4\n",
        "$$\n",
        "then the hassian matrix of the function will be:\n",
        "$$\\nabla^2f(x)=\\begin{bmatrix}\n",
        "800 &0\\\\\n",
        "0& 0.048x_2^2\n",
        "\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "axeex7UGY0qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6TYZVl-YbkJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOH6JZvwYpSs"
      },
      "outputs": [],
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x)==2\n",
        "  return 400*(x[0]**2)+0.004*(x[1]**4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalh(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x)==2\n",
        "  return np.array([[800,0],[0,0.048*(x[1]**2)]])"
      ],
      "metadata": {
        "id": "50IwZYzqb93N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x)==2\n",
        "  return np.array([800*x[0],0.016*(x[1]**3)])"
      ],
      "metadata": {
        "id": "TnpBcrrzcaeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  if np.linalg.det(evalh(x)) == 0:\n",
        "    raise ValueError('Inverse does not exists. Please check!')\n",
        "  return np.linalg.inv(evalh(x))"
      ],
      "metadata": {
        "id": "V8k2axjmfOvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  alpha = alpha_start\n",
        "  p = - gradf\n",
        "  D_k = d_k(x)\n",
        "  r=rho\n",
        "  y=gamma\n",
        "  while evalf(x + alpha*np.matmul(D_k,p)) > evalf(x) + y*alpha* (np.matmul(np.matrix.transpose(gradf), np.matmul(D_k,p)) ):\n",
        "    alpha=alpha*r\n",
        "  return alpha\n",
        "\n",
        "  \n",
        "  \n",
        "  #Complete the code "
      ],
      "metadata": {
        "id": "O2Xbo4Alfem2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BACKTRACKING_LINE_SEARCH = 1\n",
        "CONSTANT_STEP_LENGTH = 2"
      ],
      "metadata": {
        "id": "ca5HI7VyfsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k = d_k(x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      \n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x, g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here  \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "  return x, k\n",
        "  #Complete the code  "
      ],
      "metadata": {
        "id": "tzk6FZbRfsps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.3\n"
      ],
      "metadata": {
        "id": "wwyADCMLkZbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "my_tol= 1e-9\n",
        "\n",
        "x_opt_bls, k = find_minimizer_gdscaling(my_start_x, my_tol,CONSTANT_STEP_LENGTH)\n",
        "#print(x_opt_bls)\n",
        "\n",
        "print('\\n\\nminimizer of the function by using constant step length with scaling=',x_opt_bls)\n",
        "print('the number of iterations using constant step length computation with scaling=',k)\n",
        "print('minimum value of the function by using constant step length with scaling=',evalf(x_opt_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asuReZbYfzco",
        "outputId": "e5632378-f1cc-4994-9a05-05c655f9dbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "minimizer of the function by using constant step length with scaling= [0.         0.00304488]\n",
            "the number of iterations using constant step length computation with scaling= 16\n",
            "minimum value of the function by using constant step length with scaling= 3.4382653805813626e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "my_tol= 1e-9\n",
        "\n",
        "x_opt_bls, k = find_minimizer_gdscaling(my_start_x, my_tol,BACKTRACKING_LINE_SEARCH,1.0,0.5,0.5)\n",
        "#print(x_opt_bls)\n",
        "\n",
        "print('\\n\\nminimizer of the function by using backtracking line search with scaling=',x_opt_bls)\n",
        "print('the number of iterations using backtracking line search computation with scaling=',k)\n",
        "print('minimum value of the function by using backtracking line search with scaling=',evalf(x_opt_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRoeeSBZg49I",
        "outputId": "bbdcfc48-4a4d-4410-ff44-7e1545a4278a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "minimizer of the function by using backtracking line search with scaling= [0.         0.00304488]\n",
            "the number of iterations using backtracking line search computation with scaling= 16\n",
            "minimum value of the function by using backtracking line search with scaling= 3.4382653805813626e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can observe that the number of iterations(16 iterations) are same for both methods (newton's method(with backtracking line search) and newton's method(with constant step length)).\n",
        "\n",
        "\n",
        "****\n",
        "$\\textbf{Constant step length line search in Newton's Method :}$\n",
        "\n",
        "The value of Minimizer: $(0. ,        0.00304488)$\n",
        "\n",
        "Minimum function value: $3.4382653805813626e-13 $\n",
        "\n",
        "No. of Iterations: $16$\n",
        "****\n",
        "$\\textbf{Backtracking line search with scaling in Newton's Method:}$\n",
        "\n",
        "Minimizer: $(0. ,       0.00304488)$\n",
        "\n",
        "Minimum function value: $3.4382653805813626e-13 $\n",
        "\n",
        "No. of Iterations: $16$\n",
        "****\n",
        "The value of minimizer and the minimum value of the function is exactly same for both methods."
      ],
      "metadata": {
        "id": "EOvtiYuzm1SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que.4"
      ],
      "metadata": {
        "id": "Sz652J7HkdUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  #Complete the code \n",
        "  alpha = alpha_start\n",
        "  p=rho\n",
        "  y=gamma\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x)-y*alpha*np.dot((gradf.T),gradf):\n",
        "    alpha=p*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha\n",
        "  "
      ],
      "metadata": {
        "id": "hzxixuk9jlFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_gd(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k  \n"
      ],
      "metadata": {
        "id": "9-DQ_5_zk7J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "my_tol= 1e-9\n",
        " \n",
        "x_opt_bls, k= find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\n",
        "#print(x_opt_bls)\n",
        " \n",
        "print('minimizer of the function by using backtracking line search with scaling=',x_opt_bls)\n",
        "print('the number of iterations using backtracking line search computation with scaling=',k)\n",
        "print('minimum value of the function by using backtracking line search with scaling=',evalf(x_opt_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhYt9mmblnPX",
        "outputId": "20bb0131-18fe-4db4-a015-78f98ce28a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimizer of the function by using backtracking line search with scaling= [0.         0.00304488]\n",
            "the number of iterations using backtracking line search computation with scaling= 16\n",
            "minimum value of the function by using backtracking line search with scaling= 3.4382653805813626e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.0,2.0])\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "my_tol= 1e-9\n",
        "\n",
        "x_opt_bls, k = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\n",
        "#print(x_opt_bls)\n",
        "\n",
        "print('minimizer of the function by using backtracking line search without scaling=',x_opt_bls)\n",
        "print('the number of iterations using backtracking line search computation without scaling=',k)\n",
        "print('minimum value of the function by using backtracking line search without scaling=',evalf(x_opt_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "xfRJYqM3lvwe",
        "outputId": "aa73b2d1-c572-4068-93c0-39542cbc72b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a3c8301eed80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_opt_bls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(x_opt_bls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a28f8c796f66>\u001b[0m in \u001b[0;36mfind_minimizer_gd\u001b[0;34m(start_x, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call the new function you wrote to compute the steplength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0;31m#raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCONSTANT_STEP_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#do a gradient descent with constant step length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-06ab37576ba3>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking\u001b[0;34m(x, gradf, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m#implement the backtracking line search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above method is taking very long times means it takes billions of iterations to terminate."
      ],
      "metadata": {
        "id": "TVBy21zfkj8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Backtracking line search with scaling:}$\n",
        "\n",
        "Minimizer: $[0.         ,0.00304488]$\n",
        "\n",
        "Minimum function value: $ 3.4382653805813626e-13 $\n",
        "\n",
        "No. of Iterations: $16$\n",
        "****\n",
        "gradient descent algorith with backtracking without scaling is taking very high number of iterations to terminate while backtraking with line search is taking only 16 iterations to terminate which is very low as compare to other.\n",
        "****\n",
        "$\\textbf{Constant step length line search in Newton's Method :}$\n",
        "\n",
        "The value of Minimizer: $(0. ,        0.00304488)$\n",
        "\n",
        "Minimum function value: $3.4382653805813626e-13 $\n",
        "\n",
        "No. of Iterations: $16$\n",
        "***\n",
        "here constant step length in newton's method is also taking 16 itertions to terminate which is very less as compared to backtracking without scaling\n"
      ],
      "metadata": {
        "id": "J6u5IiIqk2mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k96rbgiDk2H0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}