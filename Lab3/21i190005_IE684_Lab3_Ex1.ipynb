{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190005_IE684_Lab3_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 3.}$ $\\large\\textbf{Exercise 1.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "In the last lab, when we tried to solve certain problems of the form $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$ using gradient descent algorithm, we noticed that the algorithm needed a large number of iterations to find the minimizer. Today we will discuss some remedy measures for this issue.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Meohokl4xP"
      },
      "source": [
        "Consider the problem $\\min_{\\mathbf{x}} f(\\mathbf{x}) = 1500x_1^2 + 4x_1 x_2 +  x_2^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvWjvAgnXxS3"
      },
      "source": [
        "Note that the function $f(\\mathbf{x})$ is twice continuously differentiable. First let us investigate the Hessian $\\nabla^2 f(\\mathbf{x})$ of the function. \n",
        "\n",
        "Note that the Hessian $\\nabla^2 f(\\mathbf{x})$ of the function $f(\\mathbf{x})$ is positive definite. \n",
        "\n",
        "Due to the positive definite nature of the Hessian, we shall find the condition number of the Hessian given by $\\kappa\\left (\\nabla^2 f(\\mathbf{x}) \\right ) = \\frac{\\lambda_{\\max} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}{\\lambda_{\\min} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}$, where $\\lambda_{\\max}(\\mathbf{A})$ denotes the maximum eigen value of matrix $\\mathbf{A}$ and $\\lambda_{\\min}(\\mathbf{A})$ denotes the minimum eigen value of matrix $\\mathbf{A}$.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLfdgrmxANif"
      },
      "source": [
        "$\\textbf{Question:2}$ Write code to find the Hessian matrix of the function $f(\\mathbf{x}) = 1500x_1^2 + 4x_1 x_2 +  x_2^2$ and its condition number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qwiJ0CDtOX"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "#method to find Hessian matrix: Complete the code\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "  return np.array([[3000,4],[4,2]])\n",
        "\n",
        "#method to find the condition number of any square matrix: : Complete the code\n",
        "def find_condition_number(A):\n",
        "  assert type(A) is np.ndarray\n",
        "  assert A.shape[0] == A.shape[1]\n",
        "  eig_value=np.linalg.eigvals(A)\n",
        "  return max(eig_value)/min(eig_value)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=evalh(np.array([1,2]))"
      ],
      "metadata": {
        "id": "xo123tJKkn4E"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_condition_number(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKJJKmgXljZP",
        "outputId": "fa82b1ca-57d7-4a02-a848-edd80a53f43e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1504.0160463434236"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "The condition number of the Hessian plays a major role in the progress of the iterates of gradient descent towards the optimal solution point. Typically a large value of the condition number indicates that the problem is $\\textbf{ill-conditioned}$ and hence leads to slow progress of the iterates towards the optimal solution point. Now we shall discuss a method which would help in better $\\textbf{conditioning}$ of the problem and hence would help in speeding up the progress of the iterates towards the optimal solution point. \n",
        "\n",
        "Let us first illustrate an equivalent transformation of the problem $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. Consider the transformation $\\mathbf{x}=\\mathbf{My}$ where $\\mathbf{M}\\in {\\mathbb{R}}^{n \\times n}$ is an invertible matrix and $\\mathbf{y} \\in {\\mathbb{R}}^n$ and consider the equivalent problem $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y}) \\equiv \\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} f(\\mathbf{My})$. \n",
        "\n",
        "$\\textbf{Check:}$ Why are the two problems $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$ and $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y})$  equivalent? \n",
        "\n",
        "Note that the gradient $\\nabla_{\\mathbf{y}} g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{x}} f(\\mathbf{x})$ and the Hessian is $\\nabla^2_{\\mathbf{y}} g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla^2_{\\mathbf{x}} f(\\mathbf{x}) \\mathbf{M}$. \n",
        "\n",
        "Hence the gradient descent update to solve $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y})$ becomes: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\mathbf{y}}^{k+1} &= {\\mathbf{y}}^{k} - \\eta \\nabla_{\\mathbf{y}} g({\\mathbf{y}}^{k}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Pre-multiplying by $\\mathbf{M}$, we have:\n",
        "\\begin{align}\n",
        "{\\mathbf{M}\\mathbf{y}}^{k+1} &= {\\mathbf{M}\\mathbf{y}}^{k} -  \\eta \\mathbf{M} \\nabla_{\\mathbf{y}} g({\\mathbf{y}}^{k})  \\\\\n",
        "\\implies \\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{MM}^\\top \\nabla_{\\mathbf{x}} f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Letting $\\mathbf{D} = \\mathbf{MM}^\\top$, we see that the update is of the form:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{D} \\nabla f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "Note that the matrix $\\mathbf{D}$ is symmetric and positive definite and hence can be written as $\\mathbf{D} = \\mathbf{B}^2$, where $\\mathbf{B}$ is also symmetric and positive definite. Denoting $\\mathbf{B}= \\mathbf{D}^{\\frac{1}{2}}$, we see that a useful choice for the matrix $\\mathbf{M}$ is $\\mathbf{M} = \\mathbf{B} = \\mathbf{D}^{\\frac{1}{2}}$. \n",
        "\n",
        "The matrix $\\mathbf{D}$ is called a $\\textbf{scaling}$ matrix and helps in scaling the Hessian. We will consider $\\mathbf{D}$ to be a diagonal matrix. Thus it would be useful to find a suitable candidate of the scaling matrix at each iteration which could help in significant progress of the iterates towards the optimal solution. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgHGXcTYNXuw"
      },
      "source": [
        "This discussion leads to the following algorithm:\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\text{ Choose a suitable scaling matrix }\\mathbf{D}^k. \\\\ \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{D}^k \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{D}^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k  \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XvbzQ5LeJ_N"
      },
      "source": [
        "Note that the update step in the modified gradient descent scheme uses a scaled gradient. Thus it becomes important to set up some criteria for choosing the $\\mathbf{D}^k$ matrix in every iteration. In this exercise, we will assume $\\mathbf{D}^k$ to be a diagonal matrix. The following questions will help in designing a suitable $\\mathbf{D}^k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC5h9vGOfLcr"
      },
      "source": [
        "$\\textbf{Question:4}$ Based on our discussion on condition number and the derivation of the gradient descent scheme with scaling, can you identify and write down the matrix $\\mathbf{Q}$ whose condition number needs to be analyzed in the new gradient scheme with scaling? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL8skZZIf8yd"
      },
      "source": [
        "$\\textbf{Your Answer:}$ The matrix $\\mathbf{Q}$ whose condition number needs to be analyzed in the new gradient scheme with scaling can be written as:-\n",
        "\n",
        "$\\mathbf{Q}$  = $(\\mathbf{D}^k)^{\\frac{1}{2}} \\mathbf{H}^k (\\mathbf{D}^k)^{\\frac{1}{2}}\\ \\ \\ $\n",
        "Where, $\\mathbf{H}^k = (\\nabla^2f(\\mathbf{x})) $ is the hessian of a function for $k^{th}$ iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrbWd2XigB2N"
      },
      "source": [
        "$\\textbf{Question:5}$ Based on the matrix $\\mathbf{Q}$, can you come up with a useful choice for $\\mathbf{D}^k$ (assuming $\\mathbf{D}^k$ to be diagonal)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTIH3LnUgRh6"
      },
      "source": [
        "$\\textbf{Your Answer:}$\n",
        "\n",
        "$\\mathbf{D^k}$ is a diagonal matrix whose diagonal entries are the inverse of the second partial derivatives.Hence our $D^k$ will be:\n",
        "\n",
        "$$\\mathbf{D^k}=\\begin{bmatrix}\n",
        "\\frac{1}{3000} & 0 \\\\ 0 & \\frac{1}{2}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNH0zgxgaPR"
      },
      "source": [
        "Write code to find the matrix $\\mathbf{D}^k$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#The method defines a way to construct D_k matrix used in scaling the gradient in the modified gradient descent scheme\n",
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  a11=1/3000\n",
        "  a12=0\n",
        "  a21=0\n",
        "  a22=1/2\n",
        "  return np.array([[a11,a12],[a21,a22]])\n",
        "  "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_D_k(np.array([1,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCOatxD-pg6o",
        "outputId": "e9e6227c-c27c-4273-fa19-046b89c33d43"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.33333333e-04, 0.00000000e+00],\n",
              "       [0.00000000e+00, 5.00000000e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a Python function which will compute and return the objective function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it\n",
        "  return 1500*(x[0]**2)+4*x[0]*x[1]+x[1]**2 \n",
        "  \n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalf(np.array([1,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzGHrlZOo5vS",
        "outputId": "c1a91b77-8079-4e52-ebcf-b13f32d34e5e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1505"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a Python function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([3000*x[0]+4*x[1],4*x[0]+2*x[1]])\n",
        "  "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalg(np.array([1,1])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djuMp0w7pPrN",
        "outputId": "f5d4f5af-210d-4f8c-a1fa-0efeab02f847"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "  step_length=(np.dot(gradf.T,gradf)) / (np.matmul(np.matmul(gradf,2*A),gradf)) \n",
        "  \n",
        "  return step_length\n",
        "  "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  #Complete the code \n",
        "  alpha = alpha_start\n",
        "  p=rho\n",
        "  y=gamma\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*(-gradf)) > evalf(x)-y*alpha*np.dot((gradf.T),gradf):\n",
        "    alpha=p*alpha\n",
        "\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha\n",
        "  \n",
        "  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjdHM3eaHYf"
      },
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  alpha = alpha_start\n",
        "  p = - gradf\n",
        "  D_k = compute_D_k(x)\n",
        "  r=rho\n",
        "  y=gamma\n",
        "  while evalf(x + alpha*np.matmul(D_k,p)) > evalf(x) + y*alpha* (np.matmul(np.matrix.transpose(gradf), np.matmul(D_k,p)) ):\n",
        "    alpha=alpha*r\n",
        "  return alpha\n",
        "\n",
        "  \n",
        "  \n",
        "  #Complete the code "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLRu5s635ph"
      },
      "source": [
        "#line search type \n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdYW5nldqZU-"
      },
      "source": [
        "#complete the code for gradient descent to find the minimizer\n",
        "def find_minimizer_gd(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  A=np.array([[1500,2],[2,1]])\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k  \n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  A = np.array([[1500, 2], [2, 1]])\n",
        "  D_k = compute_D_k(x)\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x, g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here  \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "  return x, k\n",
        "  #Complete the code  \n",
        "  #Complete the code   "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question:7}$ \n",
        "\n"
      ],
      "metadata": {
        "id": "4wPTYkafwpQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([4.,4.])\n",
        "my_tol=1e-5\n",
        "#check gradient descent with exact line search \n",
        "x_opt,iterations = find_minimizer_gd(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(f'Value of optimizer for exact step length= {x_opt}\\nMinimum value is= {evalf(x_opt)}\\nNumber of iterations= {iterations}\\n ')\n",
        "#check gradient descent with backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "x_opt_bls,iterations = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho,gamma)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls} \\nMinimum value is= {evalf(x_opt_bls)}\\nNumber of iterations are= {iterations}\\n\")\n",
        "#check gradient descent with scaling and backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "x_opt_bls,iterations = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho,gamma)\n",
        "print(f\"Value of optimizer for Backtracking scaled line search ={x_opt_bls} \\nMinimum value is= {evalf(x_opt_bls)}\\nNumber of iterations are= {iterations}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10J3TEDdwyk2",
        "outputId": "a3beda25-7201-4096-c786-18b9a2d13049"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for exact step length= [-2.32108719e-09  1.73908326e-06]\n",
            "Minimum value is= 3.0163454977772667e-12\n",
            "Number of iterations= 5\n",
            " \n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[-7.41280132e-09  4.58747524e-06] \n",
            "Minimum value is= 2.0991329363516383e-11\n",
            "Number of iterations are= 8194\n",
            "\n",
            "Value of optimizer for Backtracking scaled line search =[ 7.57844280e-11 -3.79259259e-08] \n",
            "Minimum value is= 1.43549399821784e-15\n",
            "Number of iterations are= 9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question:8}$ "
      ],
      "metadata": {
        "id": "Q-lsTAhCxeIT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hoak3jnHkXd"
      },
      "source": [
        "my_start_x = np.array([1.,4000.])\n",
        "my_tol= 1e-12"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgu4yasdJEKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f561f124-ce0e-4aeb-8fc3-1282b6a89a52"
      },
      "source": [
        "#check gradient descent with exact line search \n",
        "x_opt,iterations = find_minimizer_gd(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(f'Value of optimizer for exact step length= {x_opt}\\nMinimum value is= {evalf(x_opt)}\\nNumber of iterations= {iterations} ')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for exact step length= [-7.44595231e-16  4.61544291e-13]\n",
            "Minimum value is= 2.12480110780353e-25\n",
            "Number of iterations= 14075 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0_iOLVoQFYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a29479d-f40c-40ff-b2e8-d3ac3bb60e77"
      },
      "source": [
        "#check gradient descent with backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "x_opt_bls,iterations = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho,gamma)\n",
        "print(f\"Value of optimizer for Backtracking line search ={x_opt_bls} \\nMinimum value is= {evalf(x_opt_bls)}\\nNumber of iterations are= {iterations}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer for Backtracking line search =[-4.78532202e-16  4.53575301e-13] \n",
            "Minimum value is= 2.0520584176089396e-25\n",
            "Number of iterations are= 21985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpABILpQxPKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6b9019-121e-4dc8-e5d4-964efabd05ec"
      },
      "source": [
        "#check gradient descent with scaling and backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "x_opt_bls,iterations = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho,gamma)\n",
        "print(f\"Value of optimizer for Backtracking scaled line search ={x_opt_bls} \\nMinimum value is= {evalf(x_opt_bls)}\\nNumber of iterations are= {iterations}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of optimizer for Backtracking scaled line search =[-2.31674405e-18  9.60515434e-16] \n",
            "Minimum value is= 9.217397790469283e-31\n",
            "Number of iterations are= 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can observe that the backtracking algorithm with scaling is taking only 16 iterations to terminate while backtracking without scaling is taking 21985 interations which is very high as compared to backtracking with scaling algorithm. and the exact step length algorithm is taking 14075 iterations to terminate which is also very high as compared to backtracking with scaling. hence in this case backtracking with scaling is a faster approach.\n",
        "In each algorithm the value of optimizer is approaching to $[0,0]$ and the optimum value is approaching to $0$."
      ],
      "metadata": {
        "id": "yvp1tZTg1AcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question:9}$ "
      ],
      "metadata": {
        "id": "4KqaXa7ZzaEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1.,4000.])\n",
        "my_tol=1e-12\n",
        "list_of_alpha=[1.,0.9,0.75,0.6,0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "opt_with_scaling=[]\n",
        "opt_without_scaling=[]\n",
        "iterations_scaling=[]\n",
        "iterations_without_scaling=[]\n",
        "for alpha in list_of_alpha:\n",
        "  print(f'For alpha={alpha}')\n",
        "  print(\"For backtracking with scaling process:\")\n",
        "  opt_bt_scale,k=find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha,0.5,0.5)\n",
        "  print(f\"Value of optimizer ={opt_bt_scale} \\nMinimum value is= {evalf(opt_bt_scale)}\\nNumber of iterations are= {k}\\n\")\n",
        "\n",
        "  opt_with_scaling.append(opt_bt_scale)\n",
        "  iterations_scaling.append(k)\n",
        "  print(\"For backtracking without scaling process:\")\n",
        "  opt_bt,k1=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,0.5,0.5)\n",
        "  print(f\"Value of optimizer ={opt_bt} \\nMinimum value is= {evalf(opt_bt)}\\nNumber of iterations are= {k1}\\n\")\n",
        "  opt_without_scaling.append(opt_bt)\n",
        "  iterations_without_scaling.append(k1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_qIUcDxlP5",
        "outputId": "5d892ceb-1c74-4041-81d9-ea75c68d55cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For alpha=1.0\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.31674405e-18  9.60515434e-16] \n",
            "Minimum value is= 9.217397790469283e-31\n",
            "Number of iterations are= 16\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-4.78532202e-16  4.53575301e-13] \n",
            "Minimum value is= 2.0520584176089396e-25\n",
            "Number of iterations are= 21985\n",
            "\n",
            "For alpha=0.9\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-1.54825431e-16  5.99636318e-15] \n",
            "Minimum value is= 6.819918471923904e-29\n",
            "Number of iterations are= 21\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.9 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-5.48799355e-16  4.71515450e-13] \n",
            "Minimum value is= 2.2174352131747877e-25\n",
            "Number of iterations are= 15941\n",
            "\n",
            "For alpha=0.75\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.78273450e-16  1.07784868e-14] \n",
            "Minimum value is= 2.203324807217381e-28\n",
            "Number of iterations are= 32\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.75 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-7.18643891e-16  4.89175378e-13] \n",
            "Minimum value is= 2.386610528547938e-25\n",
            "Number of iterations are= 6750\n",
            "\n",
            "For alpha=0.6\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.37385169e-16  1.30849588e-14] \n",
            "Minimum value is= 3.2430059206322154e-28\n",
            "Number of iterations are= 47\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.6 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-6.07554226e-16  4.94036636e-13] \n",
            "Minimum value is= 2.43425264283868e-25\n",
            "Number of iterations are= 6887\n",
            "\n",
            "For alpha=0.5\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.51144512e-16  9.75949896e-15] \n",
            "Minimum value is= 1.800539902805011e-28\n",
            "Number of iterations are= 62\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.5 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-4.78532202e-16  4.53575301e-13] \n",
            "Minimum value is= 2.0520584176089396e-25\n",
            "Number of iterations are= 21985\n",
            "\n",
            "For alpha=0.4\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.26530890e-16  1.27315971e-14] \n",
            "Minimum value is= 3.0539815808527928e-28\n",
            "Number of iterations are= 83\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.4 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-7.24091328e-16  4.84454819e-13] \n",
            "Minimum value is= 2.3407977615582728e-25\n",
            "Number of iterations are= 11362\n",
            "\n",
            "For alpha=0.25\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.68853026e-16  1.05480788e-14] \n",
            "Minimum value is= 2.0834135979657747e-28\n",
            "Number of iterations are= 147\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.25 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-4.78532202e-16  4.53575301e-13] \n",
            "Minimum value is= 2.0520584176089396e-25\n",
            "Number of iterations are= 21985\n",
            "\n",
            "For alpha=0.1\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.34815103e-16  1.32481337e-14] \n",
            "Minimum value is= 3.2592207464795465e-28\n",
            "Number of iterations are= 397\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.1 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-7.24091328e-16  4.84454819e-13] \n",
            "Minimum value is= 2.3407977615582728e-25\n",
            "Number of iterations are= 11362\n",
            "\n",
            "For alpha=0.01\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.51028779e-16  1.39670836e-14] \n",
            "Minimum value is= 3.602998362211868e-28\n",
            "Number of iterations are= 4146\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 0.01 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-5.80852788e-16  4.86459735e-13] \n",
            "Minimum value is= 2.3601891237044125e-25\n",
            "Number of iterations are= 5509\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "head=['alpha',\"optimizer with scaling\",\"optimizer without scaling\",\"No. of Iterations with scaling\",\"No. of Iterations without scaling\",\"Optimum Value with sacling\",\"Optimum value without scaling\"]"
      ],
      "metadata": {
        "id": "-dDfhd1v3aV2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data=[[list_of_alpha[i],opt_with_scaling[i],opt_without_scaling[i],iterations_scaling[i],iterations_without_scaling[i],evalf(opt_with_scaling[i]),evalf(opt_without_scaling[i])] for i in range(len(list_of_alpha))]"
      ],
      "metadata": {
        "id": "t3FknA2C333J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate(my_data, headers=head, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x4EYtSO4Yxi",
        "outputId": "37db65dc-9e33-46b8-de14-7cae37b7e7dc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   alpha | optimizer with scaling            | optimizer without scaling         |   No. of Iterations with scaling |   No. of Iterations without scaling |   Optimum Value with sacling |   Optimum value without scaling |\n",
            "+=========+===================================+===================================+==================================+=====================================+==============================+=================================+\n",
            "|    1    | [-2.31674405e-18  9.60515434e-16] | [-4.78532202e-16  4.53575301e-13] |                               16 |                               21985 |                  9.2174e-31  |                     2.05206e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.9  | [-1.54825431e-16  5.99636318e-15] | [-5.48799355e-16  4.71515450e-13] |                               21 |                               15941 |                  6.81992e-29 |                     2.21744e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.75 | [-2.78273450e-16  1.07784868e-14] | [-7.18643891e-16  4.89175378e-13] |                               32 |                                6750 |                  2.20332e-28 |                     2.38661e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.6  | [-3.37385169e-16  1.30849588e-14] | [-6.07554226e-16  4.94036636e-13] |                               47 |                                6887 |                  3.24301e-28 |                     2.43425e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.5  | [-2.51144512e-16  9.75949896e-15] | [-4.78532202e-16  4.53575301e-13] |                               62 |                               21985 |                  1.80054e-28 |                     2.05206e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.4  | [-3.26530890e-16  1.27315971e-14] | [-7.24091328e-16  4.84454819e-13] |                               83 |                               11362 |                  3.05398e-28 |                     2.3408e-25  |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.25 | [-2.68853026e-16  1.05480788e-14] | [-4.78532202e-16  4.53575301e-13] |                              147 |                               21985 |                  2.08341e-28 |                     2.05206e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.1  | [-3.34815103e-16  1.32481337e-14] | [-7.24091328e-16  4.84454819e-13] |                              397 |                               11362 |                  3.25922e-28 |                     2.3408e-25  |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|    0.01 | [-3.51028779e-16  1.39670836e-14] | [-5.80852788e-16  4.86459735e-13] |                             4146 |                                5509 |                  3.603e-28   |                     2.36019e-25 |\n",
            "+---------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(list_of_alpha,iterations_scaling,label=\"with scaling\")\n",
        "plt.plot(list_of_alpha,iterations_without_scaling,label=\"without scaling\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Iterations\")\n",
        "plt.legend(loc=\"upper right\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wWVzt5PK4czT",
        "outputId": "1de30ac6-9137-4958-9b3e-600980920b79"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fadc5598850>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot and the table, we can see that no. of iteration for backtracking with scaling algorithm is increasing as alpha decreases.If we decrease the value of alpha from 1 to 0.01 the no. of iteration increasea from 21 to 4146.\n",
        "\n",
        "and for backtracking without scaling algorithm we can't see any pattern as we decrease the value of alpha from 1 to 0.01. and we observe that for each value of alpha the no. of iterations taken by backtracking without scaling algorithm is very high as compared to backtracking with scaling algorithm.\n",
        "\n",
        "The value of optimizer approaching to $[0,0]$ for each alpha for both algorithms and the optimum value is also approaching to 0 for both algorithms\n"
      ],
      "metadata": {
        "id": "hi_47oJM3GZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question:10}$ "
      ],
      "metadata": {
        "id": "sHW3kY656B5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([1.,4000.])\n",
        "my_tol=1e-12\n",
        "list_of_rho=[0.9, 0.8, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "opt_with_scaling=[]\n",
        "opt_without_scaling=[]\n",
        "iterations_scaling=[]\n",
        "iterations_without_scaling=[]\n",
        "for r in list_of_rho:\n",
        "  print(f'For rho = {r}')\n",
        "  print(\"For backtracking with scaling process:\")\n",
        "  opt_bt_scale,k=find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.,r,0.5)\n",
        "  print(f\"Value of optimizer ={opt_bt_scale} \\nMinimum value is= {evalf(opt_bt_scale)}\\nNumber of iterations are= {k}\\n\")\n",
        "\n",
        "  opt_with_scaling.append(opt_bt_scale)\n",
        "  iterations_scaling.append(k)\n",
        "  print(\"For backtracking without scaling process:\")\n",
        "  opt_bt,k1=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,1.,r,0.5)\n",
        "  print(f\"Value of optimizer ={opt_bt} \\nMinimum value is= {evalf(opt_bt)}\\nNumber of iterations are= {k1}\\n\")\n",
        "  opt_without_scaling.append(opt_bt)\n",
        "  iterations_without_scaling.append(k1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kOGRgxP6BN8",
        "outputId": "12687bcb-f424-4952-9c3b-f74c7c92f138"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rho = 0.9\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-1.90920422e-16  3.38018326e-15] \n",
            "Minimum value is= 6.352016622002106e-29\n",
            "Number of iterations are= 14\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.9  gamma: 0.5\n",
            "Value of optimizer =[-6.44686128e-16  4.90962904e-13] \n",
            "Minimum value is= 2.404019351870065e-25\n",
            "Number of iterations are= 127\n",
            "\n",
            "For rho = 0.8\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.89959677e-18  7.65599523e-16] \n",
            "Minimum value is= 5.970107943515186e-31\n",
            "Number of iterations are= 15\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.8  gamma: 0.5\n",
            "Value of optimizer =[-6.99968590e-16  4.40149178e-13] \n",
            "Minimum value is= 1.932338703071826e-25\n",
            "Number of iterations are= 1471\n",
            "\n",
            "For rho = 0.75\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-3.59593964e-18  9.57478862e-16] \n",
            "Minimum value is= 9.223897999650174e-31\n",
            "Number of iterations are= 15\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.75  gamma: 0.5\n",
            "Value of optimizer =[-7.15454842e-16  4.83616543e-13] \n",
            "Minimum value is= 2.332687511412074e-25\n",
            "Number of iterations are= 3135\n",
            "\n",
            "For rho = 0.6\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.68496827e-18  1.53311688e-15] \n",
            "Minimum value is= 2.3447954703965872e-30\n",
            "Number of iterations are= 15\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.6  gamma: 0.5\n",
            "Value of optimizer =[-7.07691748e-16  4.70469643e-13] \n",
            "Minimum value is= 2.2076113616544364e-25\n",
            "Number of iterations are= 15834\n",
            "\n",
            "For rho = 0.5\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.31674405e-18  9.60515434e-16] \n",
            "Minimum value is= 9.217397790469283e-31\n",
            "Number of iterations are= 16\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "Value of optimizer =[-4.78532202e-16  4.53575301e-13] \n",
            "Minimum value is= 2.0520584176089396e-25\n",
            "Number of iterations are= 21985\n",
            "\n",
            "For rho = 0.4\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-2.10920879e-18  1.38155681e-15] \n",
            "Minimum value is= 1.903716407388883e-30\n",
            "Number of iterations are= 16\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.4  gamma: 0.5\n",
            "Value of optimizer =[-5.16461460e-16  4.67846629e-13] \n",
            "Minimum value is= 2.1831406770108246e-25\n",
            "Number of iterations are= 20049\n",
            "\n",
            "For rho = 0.25\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-6.9534609e-16  4.5561674e-13] \n",
            "Minimum value is= 2.0704462793405916e-25\n",
            "Number of iterations are= 16\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.25  gamma: 0.5\n",
            "Value of optimizer =[-5.67128954e-16  4.70177132e-13] \n",
            "Minimum value is= 2.204823842206756e-25\n",
            "Number of iterations are= 24570\n",
            "\n",
            "For rho = 0.1\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-6.22892805e-16  4.51919078e-13] \n",
            "Minimum value is= 2.0368685744853975e-25\n",
            "Number of iterations are= 23\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.1  gamma: 0.5\n",
            "Value of optimizer =[-4.35224428e-16  4.42644725e-13] \n",
            "Minimum value is= 1.9544788340231146e-25\n",
            "Number of iterations are= 45435\n",
            "\n",
            "For rho = 0.01\n",
            "For backtracking with scaling process:\n",
            "Value of optimizer =[-5.88632151e-16  4.39928337e-13] \n",
            "Minimum value is= 1.9302084997635662e-25\n",
            "Number of iterations are= 130\n",
            "\n",
            "For backtracking without scaling process:\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.01  gamma: 0.5\n",
            "Value of optimizer =[-6.03891924e-16  4.89972672e-13] \n",
            "Minimum value is= 2.394366854291179e-25\n",
            "Number of iterations are= 17463\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head=['rho ',\"optimizer with scaling\",\"optimizer without scaling\",\"No. of Iterations with scaling\",\"No. of Iterations without scaling\",\"Optimum Value with sacling\",\"Optimum value without scaling\"]"
      ],
      "metadata": {
        "id": "VtxnZ2Mw7MWc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_data=[[list_of_rho[i],opt_with_scaling[i],opt_without_scaling[i],iterations_scaling[i],iterations_without_scaling[i],evalf(opt_with_scaling[i]),evalf(opt_without_scaling[i])] for i in range(len(list_of_rho))]"
      ],
      "metadata": {
        "id": "qNjGWsG97TWH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate(my_data, headers=head, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxlfUcwh7c3X",
        "outputId": "ba6d80a3-1a54-4533-9077-c2d526885391"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   rho  | optimizer with scaling            | optimizer without scaling         |   No. of Iterations with scaling |   No. of Iterations without scaling |   Optimum Value with sacling |   Optimum value without scaling |\n",
            "+========+===================================+===================================+==================================+=====================================+==============================+=================================+\n",
            "|   0.9  | [-1.90920422e-16  3.38018326e-15] | [-6.44686128e-16  4.90962904e-13] |                               14 |                                 127 |                  6.35202e-29 |                     2.40402e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.8  | [-3.89959677e-18  7.65599523e-16] | [-6.99968590e-16  4.40149178e-13] |                               15 |                                1471 |                  5.97011e-31 |                     1.93234e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.75 | [-3.59593964e-18  9.57478862e-16] | [-7.15454842e-16  4.83616543e-13] |                               15 |                                3135 |                  9.2239e-31  |                     2.33269e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.6  | [-2.68496827e-18  1.53311688e-15] | [-7.07691748e-16  4.70469643e-13] |                               15 |                               15834 |                  2.3448e-30  |                     2.20761e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.5  | [-2.31674405e-18  9.60515434e-16] | [-4.78532202e-16  4.53575301e-13] |                               16 |                               21985 |                  9.2174e-31  |                     2.05206e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.4  | [-2.10920879e-18  1.38155681e-15] | [-5.16461460e-16  4.67846629e-13] |                               16 |                               20049 |                  1.90372e-30 |                     2.18314e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.25 | [-6.9534609e-16  4.5561674e-13]   | [-5.67128954e-16  4.70177132e-13] |                               16 |                               24570 |                  2.07045e-25 |                     2.20482e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.1  | [-6.22892805e-16  4.51919078e-13] | [-4.35224428e-16  4.42644725e-13] |                               23 |                               45435 |                  2.03687e-25 |                     1.95448e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n",
            "|   0.01 | [-5.88632151e-16  4.39928337e-13] | [-6.03891924e-16  4.89972672e-13] |                              130 |                               17463 |                  1.93021e-25 |                     2.39437e-25 |\n",
            "+--------+-----------------------------------+-----------------------------------+----------------------------------+-------------------------------------+------------------------------+---------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list_of_rho,iterations_scaling,label=\"with scaling\")\n",
        "plt.plot(list_of_rho,iterations_without_scaling,label=\"without scaling\")\n",
        "plt.xlabel(\"rho\")\n",
        "plt.ylabel(\"Iterations\")\n",
        "plt.legend(loc=\"upper right\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "1pucSf7Y7dP2",
        "outputId": "c58de9e7-039a-4620-b41f-06fccb2de620"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fadc556b890>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above table and plot, we can see that no. of iterations for backtracking with scaling algorithm are increasing very slowly as we decrease the value of rho. and no of iterations for backtracking without scaling increases as we decreaser the value of rho from 0.9 to 0.1 then it decreases when we decrease the value of rho from 0.1 to 0.01.\n",
        "\n",
        "The value of optimizer is $[0,0]$(approximately),and the optimum value is also 0(approximately) for both algorithms."
      ],
      "metadata": {
        "id": "W_OsUqFn6gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XqGQ3xKk98Ux"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}